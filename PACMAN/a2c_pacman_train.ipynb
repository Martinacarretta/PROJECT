{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\propietari\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Gymnasium and ALE\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "# Stable Baselines3\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n",
    "\n",
    "# Wandb Integration\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# Custom constants\n",
    "import a2c_pacman_constants as constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, render_mode=None):\n",
    "    def _env():\n",
    "        env = gym.make(env_id, render_mode=render_mode)\n",
    "        env = Monitor(env, allow_early_resets=True)  \n",
    "        return env\n",
    "    return _env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarionapla\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\propietari\\Downloads\\curs 24-25\\ml paradigms\\PROJ - ale environment\\wandb\\run-20241130_225313-5duywz9g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marionapla/A2C%20Pacman/runs/5duywz9g' target=\"_blank\">drawn-gorge-44</a></strong> to <a href='https://wandb.ai/marionapla/A2C%20Pacman' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marionapla/A2C%20Pacman' target=\"_blank\">https://wandb.ai/marionapla/A2C%20Pacman</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marionapla/A2C%20Pacman/runs/5duywz9g' target=\"_blank\">https://wandb.ai/marionapla/A2C%20Pacman/runs/5duywz9g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\propietari\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\vec_env\\vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Training...\n",
      "Logging to runs/5duywz9g\\A2C_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\propietari\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x00000146570AA3E0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x0000014657164B20>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4000, episode_reward=12.80 +/- 2.71\n",
      "Episode length: 432.80 +/- 44.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 433      |\n",
      "|    mean_reward        | 12.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.00585  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 15       |\n",
      "|    policy_loss        | -0.328   |\n",
      "|    value_loss         | 1.51     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=18.20 +/- 8.86\n",
      "Episode length: 516.00 +/- 80.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 516      |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.19     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 31       |\n",
      "|    policy_loss        | 0.7      |\n",
      "|    value_loss         | 3.05     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=16.20 +/- 4.07\n",
      "Episode length: 498.80 +/- 87.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 499      |\n",
      "|    mean_reward        | 16.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.575    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 46       |\n",
      "|    policy_loss        | 0.503    |\n",
      "|    value_loss         | 1.02     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=17.20 +/- 3.06\n",
      "Episode length: 548.80 +/- 64.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 549      |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | 0.669    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 62       |\n",
      "|    policy_loss        | 0.177    |\n",
      "|    value_loss         | 0.936    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=20.80 +/- 7.30\n",
      "Episode length: 509.60 +/- 47.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 510      |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.429    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 78       |\n",
      "|    policy_loss        | 0.453    |\n",
      "|    value_loss         | 2.23     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=21.40 +/- 8.87\n",
      "Episode length: 525.80 +/- 132.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 526      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 0.54     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 93       |\n",
      "|    policy_loss        | -0.94    |\n",
      "|    value_loss         | 1.23     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 505      |\n",
      "|    ep_rew_mean        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 71       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 359      |\n",
      "|    total_timesteps    | 25600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.766    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.502   |\n",
      "|    value_loss         | 0.932    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=12.20 +/- 3.66\n",
      "Episode length: 485.60 +/- 65.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 486      |\n",
      "|    mean_reward        | 12.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.0325   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 109      |\n",
      "|    policy_loss        | 0.587    |\n",
      "|    value_loss         | 4.65     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=22.80 +/- 4.71\n",
      "Episode length: 576.80 +/- 73.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 577      |\n",
      "|    mean_reward        | 22.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | -0.183   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 124      |\n",
      "|    policy_loss        | 0.18     |\n",
      "|    value_loss         | 0.825    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=36000, episode_reward=15.40 +/- 4.59\n",
      "Episode length: 482.80 +/- 42.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 483      |\n",
      "|    mean_reward        | 15.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 140      |\n",
      "|    policy_loss        | -0.0159  |\n",
      "|    value_loss         | 0.422    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=19.80 +/- 2.71\n",
      "Episode length: 600.40 +/- 95.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 600      |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.51     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 156      |\n",
      "|    policy_loss        | 0.15     |\n",
      "|    value_loss         | 2.34     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=12.60 +/- 3.77\n",
      "Episode length: 446.80 +/- 36.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 447      |\n",
      "|    mean_reward        | 12.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.391    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 171      |\n",
      "|    policy_loss        | 0.661    |\n",
      "|    value_loss         | 1.93     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=13.40 +/- 2.58\n",
      "Episode length: 480.80 +/- 44.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 481      |\n",
      "|    mean_reward        | 13.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.939   |\n",
      "|    explained_variance | 0.657    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 187      |\n",
      "|    policy_loss        | -1.37    |\n",
      "|    value_loss         | 3.67     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 512      |\n",
      "|    ep_rew_mean        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 992      |\n",
      "|    total_timesteps    | 51200    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.518    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.222   |\n",
      "|    value_loss         | 0.38     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=21.40 +/- 4.45\n",
      "Episode length: 506.00 +/- 101.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 506      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 52000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 203      |\n",
      "|    policy_loss        | -0.259   |\n",
      "|    value_loss         | 0.0949   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=28.40 +/- 19.64\n",
      "Episode length: 776.40 +/- 653.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 776      |\n",
      "|    mean_reward        | 28.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.519    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 218      |\n",
      "|    policy_loss        | 0.376    |\n",
      "|    value_loss         | 0.855    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=17.60 +/- 4.13\n",
      "Episode length: 430.40 +/- 39.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 430      |\n",
      "|    mean_reward        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.709    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 234      |\n",
      "|    policy_loss        | -0.145   |\n",
      "|    value_loss         | 0.383    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=11.60 +/- 4.13\n",
      "Episode length: 382.80 +/- 65.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 383      |\n",
      "|    mean_reward        | 11.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 64000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.381    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 249      |\n",
      "|    policy_loss        | 0.037    |\n",
      "|    value_loss         | 1.01     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=15.00 +/- 2.61\n",
      "Episode length: 398.00 +/- 32.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 398      |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 68000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | -0.0284  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 265      |\n",
      "|    policy_loss        | 0.646    |\n",
      "|    value_loss         | 1.38     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=20.00 +/- 8.92\n",
      "Episode length: 467.20 +/- 111.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 467      |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 72000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0.682    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 281      |\n",
      "|    policy_loss        | -0.0751  |\n",
      "|    value_loss         | 0.965    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=25.20 +/- 18.52\n",
      "Episode length: 510.40 +/- 109.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 510      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 76000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.597    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 296      |\n",
      "|    policy_loss        | -0.292   |\n",
      "|    value_loss         | 0.5      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 471      |\n",
      "|    ep_rew_mean        | 18.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 42       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1810     |\n",
      "|    total_timesteps    | 76800    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0.582    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.0152  |\n",
      "|    value_loss         | 0.515    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=27.80 +/- 29.59\n",
      "Episode length: 454.40 +/- 84.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 454      |\n",
      "|    mean_reward        | 27.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.761    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 312      |\n",
      "|    policy_loss        | -0.297   |\n",
      "|    value_loss         | 0.827    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=22.00 +/- 6.13\n",
      "Episode length: 459.60 +/- 43.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 460      |\n",
      "|    mean_reward        | 22       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 84000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.737    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 328      |\n",
      "|    policy_loss        | 0.0189   |\n",
      "|    value_loss         | 0.491    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=15.80 +/- 4.26\n",
      "Episode length: 425.60 +/- 43.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 426      |\n",
      "|    mean_reward        | 15.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 88000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.836    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 343      |\n",
      "|    policy_loss        | -0.379   |\n",
      "|    value_loss         | 0.519    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=15.20 +/- 5.91\n",
      "Episode length: 451.20 +/- 64.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 451      |\n",
      "|    mean_reward        | 15.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 92000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.845    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 359      |\n",
      "|    policy_loss        | -0.494   |\n",
      "|    value_loss         | 0.551    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=16.20 +/- 14.18\n",
      "Episode length: 365.40 +/- 64.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 365      |\n",
      "|    mean_reward        | 16.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.374    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 374      |\n",
      "|    policy_loss        | -0.551   |\n",
      "|    value_loss         | 0.804    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=11.80 +/- 2.40\n",
      "Episode length: 354.80 +/- 25.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 355      |\n",
      "|    mean_reward        | 11.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 100000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.853    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 390      |\n",
      "|    policy_loss        | 0.637    |\n",
      "|    value_loss         | 0.714    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 427      |\n",
      "|    ep_rew_mean        | 16.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 38       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 2641     |\n",
      "|    total_timesteps    | 102400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.335    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.0698   |\n",
      "|    value_loss         | 0.562    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=10.60 +/- 5.85\n",
      "Episode length: 366.80 +/- 73.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 367      |\n",
      "|    mean_reward        | 10.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 104000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.58     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 406      |\n",
      "|    policy_loss        | -1.3     |\n",
      "|    value_loss         | 3.19     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=25.40 +/- 27.51\n",
      "Episode length: 438.80 +/- 106.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 439      |\n",
      "|    mean_reward        | 25.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 108000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.601    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 421      |\n",
      "|    policy_loss        | -0.238   |\n",
      "|    value_loss         | 0.489    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=12.60 +/- 4.76\n",
      "Episode length: 406.80 +/- 78.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 407      |\n",
      "|    mean_reward        | 12.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 112000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 437      |\n",
      "|    policy_loss        | -0.362   |\n",
      "|    value_loss         | 0.629    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=20.60 +/- 12.14\n",
      "Episode length: 419.20 +/- 60.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 419      |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 116000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.0164   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 453      |\n",
      "|    policy_loss        | 0.113    |\n",
      "|    value_loss         | 0.167    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=44.80 +/- 36.52\n",
      "Episode length: 526.80 +/- 175.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 527      |\n",
      "|    mean_reward        | 44.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 120000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.0967   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 468      |\n",
      "|    policy_loss        | 0.535    |\n",
      "|    value_loss         | 1.77     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=124000, episode_reward=13.80 +/- 7.52\n",
      "Episode length: 385.60 +/- 55.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 386      |\n",
      "|    mean_reward        | 13.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 124000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.895    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 484      |\n",
      "|    policy_loss        | 0.0865   |\n",
      "|    value_loss         | 0.399    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=25.20 +/- 26.00\n",
      "Episode length: 462.00 +/- 201.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 462      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 128000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | -0.654   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.385   |\n",
      "|    value_loss         | 0.489    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 414      |\n",
      "|    ep_rew_mean     | 20.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 500      |\n",
      "|    time_elapsed    | 3474     |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=30.40 +/- 28.46\n",
      "Episode length: 461.20 +/- 85.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 461      |\n",
      "|    mean_reward        | 30.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 132000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 515      |\n",
      "|    policy_loss        | -0.355   |\n",
      "|    value_loss         | 0.351    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=10.00 +/- 3.69\n",
      "Episode length: 361.60 +/- 38.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 362      |\n",
      "|    mean_reward        | 10       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 136000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0.0302   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 531      |\n",
      "|    policy_loss        | 2.43     |\n",
      "|    value_loss         | 44.6     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=16.40 +/- 5.16\n",
      "Episode length: 460.80 +/- 82.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 461      |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 140000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.827    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 546      |\n",
      "|    policy_loss        | 0.126    |\n",
      "|    value_loss         | 0.42     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=15.40 +/- 3.38\n",
      "Episode length: 474.40 +/- 60.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 474      |\n",
      "|    mean_reward        | 15.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 144000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.756    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 562      |\n",
      "|    policy_loss        | -0.681   |\n",
      "|    value_loss         | 0.348    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=23.00 +/- 10.79\n",
      "Episode length: 598.80 +/- 258.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 599      |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 148000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.893    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 578      |\n",
      "|    policy_loss        | -0.202   |\n",
      "|    value_loss         | 0.376    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=16.80 +/- 4.49\n",
      "Episode length: 484.80 +/- 77.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 485      |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 152000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.316    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 593      |\n",
      "|    policy_loss        | 0.79     |\n",
      "|    value_loss         | 1.66     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 430      |\n",
      "|    ep_rew_mean        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 40       |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 3822     |\n",
      "|    total_timesteps    | 153600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0.4      |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.215    |\n",
      "|    value_loss         | 0.262    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=16.60 +/- 1.50\n",
      "Episode length: 454.00 +/- 43.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 454      |\n",
      "|    mean_reward        | 16.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 156000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.483    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 609      |\n",
      "|    policy_loss        | 0.197    |\n",
      "|    value_loss         | 0.583    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=13.80 +/- 3.71\n",
      "Episode length: 432.00 +/- 47.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 432      |\n",
      "|    mean_reward        | 13.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 160000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.573    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 624      |\n",
      "|    policy_loss        | -0.346   |\n",
      "|    value_loss         | 0.778    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=20.80 +/- 11.34\n",
      "Episode length: 493.60 +/- 97.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 494      |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 164000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | -0.247   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 640      |\n",
      "|    policy_loss        | 0.619    |\n",
      "|    value_loss         | 1.14     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=17.20 +/- 5.60\n",
      "Episode length: 442.80 +/- 50.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 443      |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 168000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 656      |\n",
      "|    policy_loss        | -0.189   |\n",
      "|    value_loss         | 0.373    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=18.00 +/- 3.52\n",
      "Episode length: 459.60 +/- 76.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 460      |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 172000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 671      |\n",
      "|    policy_loss        | 0.27     |\n",
      "|    value_loss         | 0.419    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=10.80 +/- 1.72\n",
      "Episode length: 390.00 +/- 40.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 390      |\n",
      "|    mean_reward        | 10.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 176000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 687      |\n",
      "|    policy_loss        | -0.0818  |\n",
      "|    value_loss         | 0.169    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 456      |\n",
      "|    ep_rew_mean        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 43       |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 4160     |\n",
      "|    total_timesteps    | 179200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.0168  |\n",
      "|    value_loss         | 0.204    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=14.60 +/- 2.58\n",
      "Episode length: 445.60 +/- 37.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 446      |\n",
      "|    mean_reward        | 14.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 180000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.482    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 703      |\n",
      "|    policy_loss        | -0.107   |\n",
      "|    value_loss         | 0.596    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=15.80 +/- 5.60\n",
      "Episode length: 411.60 +/- 63.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 412      |\n",
      "|    mean_reward        | 15.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 184000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.324    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 718      |\n",
      "|    policy_loss        | 0.369    |\n",
      "|    value_loss         | 0.408    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=13.80 +/- 5.11\n",
      "Episode length: 426.00 +/- 69.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 426      |\n",
      "|    mean_reward        | 13.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 188000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 734      |\n",
      "|    policy_loss        | -0.841   |\n",
      "|    value_loss         | 0.577    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=18.60 +/- 3.38\n",
      "Episode length: 491.20 +/- 49.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 491      |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 192000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.387    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 749      |\n",
      "|    policy_loss        | -0.304   |\n",
      "|    value_loss         | 0.155    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=20.40 +/- 15.33\n",
      "Episode length: 496.40 +/- 107.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 496      |\n",
      "|    mean_reward        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 196000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.706    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 765      |\n",
      "|    policy_loss        | -0.0939  |\n",
      "|    value_loss         | 1.04     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=18.60 +/- 7.17\n",
      "Episode length: 525.20 +/- 104.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 525      |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 200000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.661    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 781      |\n",
      "|    policy_loss        | 0.378    |\n",
      "|    value_loss         | 1.03     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=20.00 +/- 5.55\n",
      "Episode length: 536.00 +/- 88.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 536      |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 204000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.831    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 796      |\n",
      "|    policy_loss        | -0.541   |\n",
      "|    value_loss         | 0.445    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 462      |\n",
      "|    ep_rew_mean        | 19.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 45       |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 4529     |\n",
      "|    total_timesteps    | 204800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.657    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.143    |\n",
      "|    value_loss         | 0.449    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=14.20 +/- 2.99\n",
      "Episode length: 429.60 +/- 29.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 430      |\n",
      "|    mean_reward        | 14.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 208000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 812      |\n",
      "|    policy_loss        | -0.364   |\n",
      "|    value_loss         | 0.378    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=11.60 +/- 3.26\n",
      "Episode length: 443.20 +/- 23.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 443      |\n",
      "|    mean_reward        | 11.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 212000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.453    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 828      |\n",
      "|    policy_loss        | 0.609    |\n",
      "|    value_loss         | 1.04     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=19.00 +/- 6.54\n",
      "Episode length: 518.80 +/- 47.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 519      |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 216000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 843      |\n",
      "|    policy_loss        | 0.0521   |\n",
      "|    value_loss         | 0.227    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=15.80 +/- 3.54\n",
      "Episode length: 460.40 +/- 50.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 460      |\n",
      "|    mean_reward        | 15.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 220000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 859      |\n",
      "|    policy_loss        | -0.138   |\n",
      "|    value_loss         | 0.706    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=26.40 +/- 18.03\n",
      "Episode length: 512.80 +/- 165.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 513      |\n",
      "|    mean_reward        | 26.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 224000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 874      |\n",
      "|    policy_loss        | -0.0751  |\n",
      "|    value_loss         | 0.266    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=17.00 +/- 4.73\n",
      "Episode length: 600.40 +/- 147.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 600      |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 228000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.0465   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 890      |\n",
      "|    policy_loss        | 2.58     |\n",
      "|    value_loss         | 40.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 467      |\n",
      "|    ep_rew_mean        | 19.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 4869     |\n",
      "|    total_timesteps    | 230400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.695    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.503    |\n",
      "|    value_loss         | 1.05     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=24.20 +/- 11.36\n",
      "Episode length: 620.80 +/- 116.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 621      |\n",
      "|    mean_reward        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 232000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0.128    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 906      |\n",
      "|    policy_loss        | 0.286    |\n",
      "|    value_loss         | 9.13     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=16.40 +/- 2.42\n",
      "Episode length: 424.80 +/- 48.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 425      |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 236000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.796    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 921      |\n",
      "|    policy_loss        | 0.209    |\n",
      "|    value_loss         | 0.893    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=18.60 +/- 7.91\n",
      "Episode length: 499.00 +/- 92.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 499      |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 240000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.792    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 937      |\n",
      "|    policy_loss        | 0.261    |\n",
      "|    value_loss         | 1.15     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=25.00 +/- 9.82\n",
      "Episode length: 616.80 +/- 97.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 617      |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 244000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.0244   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 953      |\n",
      "|    policy_loss        | 0.957    |\n",
      "|    value_loss         | 2.19     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=20.80 +/- 5.67\n",
      "Episode length: 521.00 +/- 92.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 521      |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 248000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.0544   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 968      |\n",
      "|    policy_loss        | 0.146    |\n",
      "|    value_loss         | 2.08     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=40.60 +/- 26.23\n",
      "Episode length: 641.60 +/- 103.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 642      |\n",
      "|    mean_reward        | 40.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 252000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0.256    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 984      |\n",
      "|    policy_loss        | 0.395    |\n",
      "|    value_loss         | 3.53     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=26.20 +/- 4.71\n",
      "Episode length: 761.20 +/- 157.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 761      |\n",
      "|    mean_reward        | 26.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 256000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.12     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.895    |\n",
      "|    value_loss         | 3.03     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 501      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 48       |\n",
      "|    iterations      | 1000     |\n",
      "|    time_elapsed    | 5230     |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=36.80 +/- 26.65\n",
      "Episode length: 662.00 +/- 180.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 662      |\n",
      "|    mean_reward        | 36.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 260000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1015     |\n",
      "|    policy_loss        | -0.716   |\n",
      "|    value_loss         | 0.471    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=21.80 +/- 14.93\n",
      "Episode length: 533.60 +/- 156.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 534      |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 264000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.711    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1031     |\n",
      "|    policy_loss        | -0.344   |\n",
      "|    value_loss         | 0.846    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=25.60 +/- 7.20\n",
      "Episode length: 640.00 +/- 150.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 640      |\n",
      "|    mean_reward        | 25.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 268000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1046     |\n",
      "|    policy_loss        | -0.77    |\n",
      "|    value_loss         | 0.584    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=36.80 +/- 8.13\n",
      "Episode length: 783.60 +/- 85.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 784      |\n",
      "|    mean_reward        | 36.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 272000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.0522   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1062     |\n",
      "|    policy_loss        | -0.103   |\n",
      "|    value_loss         | 4.64     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=18.40 +/- 0.80\n",
      "Episode length: 572.00 +/- 136.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 572      |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 276000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -0.0288  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1078     |\n",
      "|    policy_loss        | 0.459    |\n",
      "|    value_loss         | 3.27     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=33.00 +/- 13.90\n",
      "Episode length: 738.40 +/- 116.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 738      |\n",
      "|    mean_reward        | 33       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 280000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | -0.0853  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1093     |\n",
      "|    policy_loss        | 1.31     |\n",
      "|    value_loss         | 18.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 572      |\n",
      "|    ep_rew_mean        | 28.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 50       |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 5631     |\n",
      "|    total_timesteps    | 281600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.759    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.181    |\n",
      "|    value_loss         | 0.83     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=16.60 +/- 7.42\n",
      "Episode length: 530.80 +/- 130.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 531      |\n",
      "|    mean_reward        | 16.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 284000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.558    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1109     |\n",
      "|    policy_loss        | 0.0879   |\n",
      "|    value_loss         | 0.621    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=29.20 +/- 10.05\n",
      "Episode length: 667.60 +/- 134.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 668      |\n",
      "|    mean_reward        | 29.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 288000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.607   |\n",
      "|    explained_variance | 0.21     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1124     |\n",
      "|    policy_loss        | -0.396   |\n",
      "|    value_loss         | 10.1     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=29.40 +/- 10.11\n",
      "Episode length: 693.60 +/- 138.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 694      |\n",
      "|    mean_reward        | 29.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 292000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.745    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1140     |\n",
      "|    policy_loss        | -0.0946  |\n",
      "|    value_loss         | 1.89     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=31.20 +/- 13.64\n",
      "Episode length: 627.60 +/- 93.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 628      |\n",
      "|    mean_reward        | 31.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 296000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.756    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1156     |\n",
      "|    policy_loss        | -0.182   |\n",
      "|    value_loss         | 0.484    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=26.60 +/- 6.15\n",
      "Episode length: 636.40 +/- 165.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 636      |\n",
      "|    mean_reward        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 300000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.52     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1171     |\n",
      "|    policy_loss        | 0.152    |\n",
      "|    value_loss         | 1.12     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=38.60 +/- 30.73\n",
      "Episode length: 570.40 +/- 144.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 570      |\n",
      "|    mean_reward        | 38.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 304000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.74     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1187     |\n",
      "|    policy_loss        | 0.114    |\n",
      "|    value_loss         | 1.81     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 623      |\n",
      "|    ep_rew_mean        | 31.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 50       |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 6028     |\n",
      "|    total_timesteps    | 307200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.689    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.131   |\n",
      "|    value_loss         | 0.355    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=24.40 +/- 9.48\n",
      "Episode length: 571.20 +/- 140.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 571      |\n",
      "|    mean_reward        | 24.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 308000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1203     |\n",
      "|    policy_loss        | 0.649    |\n",
      "|    value_loss         | 0.863    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=24.20 +/- 3.54\n",
      "Episode length: 481.20 +/- 39.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 481      |\n",
      "|    mean_reward        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 312000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1218     |\n",
      "|    policy_loss        | -0.127   |\n",
      "|    value_loss         | 0.378    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=18.60 +/- 4.80\n",
      "Episode length: 432.40 +/- 50.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 432      |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 316000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.699    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1234     |\n",
      "|    policy_loss        | -0.433   |\n",
      "|    value_loss         | 0.61     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=18.40 +/- 3.01\n",
      "Episode length: 564.00 +/- 46.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 564      |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 320000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1249     |\n",
      "|    policy_loss        | 0.129    |\n",
      "|    value_loss         | 1.11     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=30.80 +/- 8.08\n",
      "Episode length: 549.20 +/- 81.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 549      |\n",
      "|    mean_reward        | 30.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 324000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 0.762    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1265     |\n",
      "|    policy_loss        | 0.0991   |\n",
      "|    value_loss         | 0.502    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=24.00 +/- 8.15\n",
      "Episode length: 598.80 +/- 28.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 599      |\n",
      "|    mean_reward        | 24       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 328000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.19    |\n",
      "|    explained_variance | 0.121    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1281     |\n",
      "|    policy_loss        | 0.239    |\n",
      "|    value_loss         | 8.26     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=32.20 +/- 11.77\n",
      "Episode length: 670.00 +/- 124.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 670      |\n",
      "|    mean_reward        | 32.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 332000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1296     |\n",
      "|    policy_loss        | -0.225   |\n",
      "|    value_loss         | 0.513    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 646      |\n",
      "|    ep_rew_mean        | 34.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 52       |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 6393     |\n",
      "|    total_timesteps    | 332800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.2     |\n",
      "|    explained_variance | 0.254    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.0513  |\n",
      "|    value_loss         | 1.66     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=27.60 +/- 4.27\n",
      "Episode length: 595.60 +/- 166.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 596      |\n",
      "|    mean_reward        | 27.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 336000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.427    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1312     |\n",
      "|    policy_loss        | 0.629    |\n",
      "|    value_loss         | 1.64     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=37.60 +/- 12.77\n",
      "Episode length: 629.40 +/- 136.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 629      |\n",
      "|    mean_reward        | 37.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 340000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.729    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1328     |\n",
      "|    policy_loss        | -0.481   |\n",
      "|    value_loss         | 1.76     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=30.20 +/- 9.87\n",
      "Episode length: 728.80 +/- 219.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 729      |\n",
      "|    mean_reward        | 30.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 344000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.275    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1343     |\n",
      "|    policy_loss        | 0.982    |\n",
      "|    value_loss         | 12.9     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=32.20 +/- 12.89\n",
      "Episode length: 724.80 +/- 253.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 725      |\n",
      "|    mean_reward        | 32.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 348000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.801    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1359     |\n",
      "|    policy_loss        | -0.586   |\n",
      "|    value_loss         | 0.724    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=24.60 +/- 3.56\n",
      "Episode length: 505.60 +/- 113.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 506      |\n",
      "|    mean_reward        | 24.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 352000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.527    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1374     |\n",
      "|    policy_loss        | -0.749   |\n",
      "|    value_loss         | 0.593    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=32.40 +/- 11.83\n",
      "Episode length: 615.20 +/- 105.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 615      |\n",
      "|    mean_reward        | 32.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 356000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1390     |\n",
      "|    policy_loss        | -0.197   |\n",
      "|    value_loss         | 0.41     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 661      |\n",
      "|    ep_rew_mean        | 33.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 52       |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 6789     |\n",
      "|    total_timesteps    | 358400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.292    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.0888  |\n",
      "|    value_loss         | 0.489    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=31.20 +/- 6.18\n",
      "Episode length: 683.60 +/- 153.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 684      |\n",
      "|    mean_reward        | 31.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 360000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.889    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1406     |\n",
      "|    policy_loss        | 0.349    |\n",
      "|    value_loss         | 0.784    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=26.40 +/- 8.11\n",
      "Episode length: 650.40 +/- 88.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 650      |\n",
      "|    mean_reward        | 26.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 364000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1421     |\n",
      "|    policy_loss        | -0.00138 |\n",
      "|    value_loss         | 0.723    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=33.20 +/- 5.11\n",
      "Episode length: 703.80 +/- 177.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 704      |\n",
      "|    mean_reward        | 33.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 368000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.655    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1437     |\n",
      "|    policy_loss        | 0.4      |\n",
      "|    value_loss         | 1.96     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=32.80 +/- 2.79\n",
      "Episode length: 676.80 +/- 92.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 677      |\n",
      "|    mean_reward        | 32.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 372000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.768    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1453     |\n",
      "|    policy_loss        | 0.0316   |\n",
      "|    value_loss         | 1.53     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=24.20 +/- 8.11\n",
      "Episode length: 597.60 +/- 73.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 598      |\n",
      "|    mean_reward        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 376000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.792    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1468     |\n",
      "|    policy_loss        | -0.118   |\n",
      "|    value_loss         | 1.52     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=25.20 +/- 14.55\n",
      "Episode length: 520.40 +/- 111.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 520      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 380000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.744    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1484     |\n",
      "|    policy_loss        | 0.181    |\n",
      "|    value_loss         | 0.914    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=37.40 +/- 8.45\n",
      "Episode length: 728.80 +/- 151.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 729      |\n",
      "|    mean_reward        | 37.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 384000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.84     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.0532   |\n",
      "|    value_loss         | 1.52     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 674      |\n",
      "|    ep_rew_mean     | 33.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 1500     |\n",
      "|    time_elapsed    | 7209     |\n",
      "|    total_timesteps | 384000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=38.60 +/- 14.40\n",
      "Episode length: 755.60 +/- 154.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 756      |\n",
      "|    mean_reward        | 38.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 388000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0.772    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1515     |\n",
      "|    policy_loss        | 0.454    |\n",
      "|    value_loss         | 1.12     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=23.00 +/- 6.16\n",
      "Episode length: 554.00 +/- 88.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 554      |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 392000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1531     |\n",
      "|    policy_loss        | -0.616   |\n",
      "|    value_loss         | 0.411    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=37.20 +/- 19.08\n",
      "Episode length: 582.00 +/- 209.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 582      |\n",
      "|    mean_reward        | 37.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 396000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0.773    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1546     |\n",
      "|    policy_loss        | -0.0622  |\n",
      "|    value_loss         | 1.38     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=42.00 +/- 7.07\n",
      "Episode length: 851.60 +/- 140.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 852      |\n",
      "|    mean_reward        | 42       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 400000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.0275   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1562     |\n",
      "|    policy_loss        | 1.27     |\n",
      "|    value_loss         | 13       |\n",
      "------------------------------------\n",
      "Eval num_timesteps=404000, episode_reward=28.80 +/- 3.66\n",
      "Episode length: 702.60 +/- 65.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 703      |\n",
      "|    mean_reward        | 28.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 404000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.718    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1578     |\n",
      "|    policy_loss        | -0.304   |\n",
      "|    value_loss         | 2.31     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=34.00 +/- 13.40\n",
      "Episode length: 563.00 +/- 113.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 563      |\n",
      "|    mean_reward        | 34       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 408000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1593     |\n",
      "|    policy_loss        | -0.804   |\n",
      "|    value_loss         | 1.21     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 700      |\n",
      "|    ep_rew_mean        | 34.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 7612     |\n",
      "|    total_timesteps    | 409600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.565    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.706    |\n",
      "|    value_loss         | 2.03     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=412000, episode_reward=26.60 +/- 11.93\n",
      "Episode length: 553.20 +/- 146.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 553      |\n",
      "|    mean_reward        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 412000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | -0.3     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1609     |\n",
      "|    policy_loss        | -0.453   |\n",
      "|    value_loss         | 0.41     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=29.80 +/- 3.87\n",
      "Episode length: 664.40 +/- 101.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 664      |\n",
      "|    mean_reward        | 29.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 416000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.732    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1624     |\n",
      "|    policy_loss        | 0.845    |\n",
      "|    value_loss         | 1.76     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=20.40 +/- 5.75\n",
      "Episode length: 455.20 +/- 24.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 455      |\n",
      "|    mean_reward        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 420000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.836    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1640     |\n",
      "|    policy_loss        | 0.55     |\n",
      "|    value_loss         | 0.844    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=424000, episode_reward=33.60 +/- 7.23\n",
      "Episode length: 729.20 +/- 232.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 729      |\n",
      "|    mean_reward        | 33.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 424000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.706    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1656     |\n",
      "|    policy_loss        | 0.246    |\n",
      "|    value_loss         | 1.87     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=428000, episode_reward=30.40 +/- 9.35\n",
      "Episode length: 680.80 +/- 93.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 681      |\n",
      "|    mean_reward        | 30.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 428000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | -0.0227  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1671     |\n",
      "|    policy_loss        | 6.91     |\n",
      "|    value_loss         | 207      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=42.00 +/- 7.48\n",
      "Episode length: 726.00 +/- 115.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 726      |\n",
      "|    mean_reward        | 42       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 432000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1687     |\n",
      "|    policy_loss        | -0.288   |\n",
      "|    value_loss         | 0.353    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 662      |\n",
      "|    ep_rew_mean        | 35       |\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 8009     |\n",
      "|    total_timesteps    | 435200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.175    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 1.22     |\n",
      "|    value_loss         | 14.6     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=436000, episode_reward=35.80 +/- 8.01\n",
      "Episode length: 693.20 +/- 90.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 693      |\n",
      "|    mean_reward        | 35.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 436000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | -0.102   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1703     |\n",
      "|    policy_loss        | 1.56     |\n",
      "|    value_loss         | 29.7     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=41.60 +/- 21.79\n",
      "Episode length: 718.40 +/- 212.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 718      |\n",
      "|    mean_reward        | 41.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 440000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | -0.0576  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1718     |\n",
      "|    policy_loss        | -0.877   |\n",
      "|    value_loss         | 1.26     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=54.40 +/- 58.58\n",
      "Episode length: 484.40 +/- 44.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 484      |\n",
      "|    mean_reward        | 54.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 444000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.657    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1734     |\n",
      "|    policy_loss        | -0.629   |\n",
      "|    value_loss         | 0.676    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=448000, episode_reward=22.60 +/- 6.22\n",
      "Episode length: 505.20 +/- 122.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 505      |\n",
      "|    mean_reward        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 448000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.563    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1749     |\n",
      "|    policy_loss        | 0.0691   |\n",
      "|    value_loss         | 1.4      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=452000, episode_reward=31.40 +/- 10.73\n",
      "Episode length: 532.40 +/- 108.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 532      |\n",
      "|    mean_reward        | 31.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 452000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1765     |\n",
      "|    policy_loss        | -0.0983  |\n",
      "|    value_loss         | 0.694    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=33.40 +/- 9.46\n",
      "Episode length: 586.00 +/- 114.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 586      |\n",
      "|    mean_reward        | 33.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 456000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1781     |\n",
      "|    policy_loss        | -0.405   |\n",
      "|    value_loss         | 0.517    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=31.60 +/- 16.51\n",
      "Episode length: 562.00 +/- 111.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 562      |\n",
      "|    mean_reward        | 31.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 460000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1796     |\n",
      "|    policy_loss        | -0.229   |\n",
      "|    value_loss         | 0.985    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 607      |\n",
      "|    ep_rew_mean        | 33.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 8414     |\n",
      "|    total_timesteps    | 460800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.773    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.334   |\n",
      "|    value_loss         | 0.696    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=35.00 +/- 4.05\n",
      "Episode length: 721.60 +/- 82.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 722      |\n",
      "|    mean_reward        | 35       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 464000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.757    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1812     |\n",
      "|    policy_loss        | 0.353    |\n",
      "|    value_loss         | 1.64     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=468000, episode_reward=29.00 +/- 4.98\n",
      "Episode length: 600.80 +/- 101.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 601      |\n",
      "|    mean_reward        | 29       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 468000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1828     |\n",
      "|    policy_loss        | -0.348   |\n",
      "|    value_loss         | 0.602    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=472000, episode_reward=25.40 +/- 3.61\n",
      "Episode length: 536.40 +/- 122.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 536      |\n",
      "|    mean_reward        | 25.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 472000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1843     |\n",
      "|    policy_loss        | -0.16    |\n",
      "|    value_loss         | 0.445    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=476000, episode_reward=26.00 +/- 13.21\n",
      "Episode length: 497.60 +/- 108.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 498      |\n",
      "|    mean_reward        | 26       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 476000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1859     |\n",
      "|    policy_loss        | -0.835   |\n",
      "|    value_loss         | 0.714    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=29.00 +/- 7.13\n",
      "Episode length: 522.80 +/- 113.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 523      |\n",
      "|    mean_reward        | 29       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 480000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.659    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1874     |\n",
      "|    policy_loss        | 0.63     |\n",
      "|    value_loss         | 3.78     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=484000, episode_reward=24.00 +/- 7.48\n",
      "Episode length: 500.80 +/- 97.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 501      |\n",
      "|    mean_reward        | 24       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 484000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.819    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1890     |\n",
      "|    policy_loss        | 0.254    |\n",
      "|    value_loss         | 1.21     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 595      |\n",
      "|    ep_rew_mean        | 32.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 8799     |\n",
      "|    total_timesteps    | 486400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.575    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.0873  |\n",
      "|    value_loss         | 0.262    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=488000, episode_reward=34.00 +/- 5.06\n",
      "Episode length: 627.60 +/- 81.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 628      |\n",
      "|    mean_reward        | 34       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 488000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.777    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1906     |\n",
      "|    policy_loss        | -0.0509  |\n",
      "|    value_loss         | 1.06     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=492000, episode_reward=38.00 +/- 12.46\n",
      "Episode length: 662.40 +/- 122.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 662      |\n",
      "|    mean_reward        | 38       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 492000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1921     |\n",
      "|    policy_loss        | -0.273   |\n",
      "|    value_loss         | 0.372    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=29.20 +/- 12.87\n",
      "Episode length: 570.80 +/- 155.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 571      |\n",
      "|    mean_reward        | 29.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 496000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.806    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1937     |\n",
      "|    policy_loss        | 0.237    |\n",
      "|    value_loss         | 0.39     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=32.60 +/- 10.97\n",
      "Episode length: 635.60 +/- 120.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 636      |\n",
      "|    mean_reward        | 32.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 500000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.0176   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1953     |\n",
      "|    policy_loss        | 2.67     |\n",
      "|    value_loss         | 37.3     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=32.00 +/- 10.37\n",
      "Episode length: 713.60 +/- 108.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 714      |\n",
      "|    mean_reward        | 32       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 504000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1968     |\n",
      "|    policy_loss        | 0.307    |\n",
      "|    value_loss         | 0.68     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=508000, episode_reward=30.40 +/- 13.51\n",
      "Episode length: 586.80 +/- 91.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 587      |\n",
      "|    mean_reward        | 30.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 508000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.857    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1984     |\n",
      "|    policy_loss        | -0.218   |\n",
      "|    value_loss         | 0.474    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=23.00 +/- 7.87\n",
      "Episode length: 523.60 +/- 76.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 524      |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 512000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.25     |\n",
      "|    value_loss         | 0.612    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 581      |\n",
      "|    ep_rew_mean     | 29.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 55       |\n",
      "|    iterations      | 2000     |\n",
      "|    time_elapsed    | 9213     |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=516000, episode_reward=43.60 +/- 17.48\n",
      "Episode length: 728.80 +/- 204.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 729      |\n",
      "|    mean_reward        | 43.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 516000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2015     |\n",
      "|    policy_loss        | -0.33    |\n",
      "|    value_loss         | 0.218    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=29.80 +/- 11.44\n",
      "Episode length: 852.00 +/- 562.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 852      |\n",
      "|    mean_reward        | 29.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 520000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2031     |\n",
      "|    policy_loss        | 0.484    |\n",
      "|    value_loss         | 0.85     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=524000, episode_reward=29.60 +/- 9.93\n",
      "Episode length: 564.40 +/- 87.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 564      |\n",
      "|    mean_reward        | 29.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 524000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2046     |\n",
      "|    policy_loss        | -0.221   |\n",
      "|    value_loss         | 0.145    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=32.80 +/- 6.43\n",
      "Episode length: 881.20 +/- 244.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 881      |\n",
      "|    mean_reward        | 32.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 528000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.796    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2062     |\n",
      "|    policy_loss        | -0.815   |\n",
      "|    value_loss         | 1.66     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=532000, episode_reward=29.40 +/- 6.62\n",
      "Episode length: 572.40 +/- 128.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 572      |\n",
      "|    mean_reward        | 29.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 532000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2078     |\n",
      "|    policy_loss        | -0.29    |\n",
      "|    value_loss         | 0.764    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=536000, episode_reward=21.60 +/- 6.34\n",
      "Episode length: 466.80 +/- 62.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 467      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 536000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2093     |\n",
      "|    policy_loss        | -0.175   |\n",
      "|    value_loss         | 0.358    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 607      |\n",
      "|    ep_rew_mean        | 29.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 9618     |\n",
      "|    total_timesteps    | 537600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 0.291    |\n",
      "|    value_loss         | 0.724    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=41.80 +/- 32.15\n",
      "Episode length: 605.60 +/- 130.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 606      |\n",
      "|    mean_reward        | 41.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 540000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.76     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2109     |\n",
      "|    policy_loss        | 0.0709   |\n",
      "|    value_loss         | 0.684    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=24.80 +/- 10.48\n",
      "Episode length: 508.40 +/- 75.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 508      |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 544000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.835    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2124     |\n",
      "|    policy_loss        | -0.189   |\n",
      "|    value_loss         | 1.58     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=548000, episode_reward=35.80 +/- 11.96\n",
      "Episode length: 665.20 +/- 108.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 665      |\n",
      "|    mean_reward        | 35.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 548000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.183    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2140     |\n",
      "|    policy_loss        | -0.883   |\n",
      "|    value_loss         | 2.03     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=552000, episode_reward=44.20 +/- 20.17\n",
      "Episode length: 612.80 +/- 95.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 613      |\n",
      "|    mean_reward        | 44.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 552000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.905    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2156     |\n",
      "|    policy_loss        | -0.29    |\n",
      "|    value_loss         | 0.846    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=556000, episode_reward=25.00 +/- 7.04\n",
      "Episode length: 492.00 +/- 68.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 492      |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 556000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 0.884    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2171     |\n",
      "|    policy_loss        | -0.226   |\n",
      "|    value_loss         | 0.712    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=24.20 +/- 3.92\n",
      "Episode length: 490.80 +/- 48.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 491      |\n",
      "|    mean_reward        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 560000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.819    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2187     |\n",
      "|    policy_loss        | 0.686    |\n",
      "|    value_loss         | 1.25     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 605      |\n",
      "|    ep_rew_mean        | 30.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 10003    |\n",
      "|    total_timesteps    | 563200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.831    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | 0.191    |\n",
      "|    value_loss         | 0.53     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=564000, episode_reward=21.80 +/- 2.48\n",
      "Episode length: 526.00 +/- 51.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 526      |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 564000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.717    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2203     |\n",
      "|    policy_loss        | 0.791    |\n",
      "|    value_loss         | 1.24     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=568000, episode_reward=18.60 +/- 8.01\n",
      "Episode length: 468.80 +/- 33.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 469      |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 568000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2218     |\n",
      "|    policy_loss        | -0.609   |\n",
      "|    value_loss         | 0.658    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=572000, episode_reward=22.60 +/- 6.74\n",
      "Episode length: 580.00 +/- 202.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 580      |\n",
      "|    mean_reward        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 572000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0.798    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2234     |\n",
      "|    policy_loss        | 0.427    |\n",
      "|    value_loss         | 0.788    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=18.20 +/- 4.53\n",
      "Episode length: 460.00 +/- 69.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 460      |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 576000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.782    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2249     |\n",
      "|    policy_loss        | -0.187   |\n",
      "|    value_loss         | 0.435    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=18.00 +/- 3.95\n",
      "Episode length: 457.60 +/- 36.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 458      |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 580000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.694    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2265     |\n",
      "|    policy_loss        | -0.0977  |\n",
      "|    value_loss         | 0.922    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=584000, episode_reward=15.00 +/- 4.56\n",
      "Episode length: 489.20 +/- 57.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 489      |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 584000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.246    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2281     |\n",
      "|    policy_loss        | 0.355    |\n",
      "|    value_loss         | 1.66     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=588000, episode_reward=17.20 +/- 7.36\n",
      "Episode length: 444.00 +/- 94.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 444      |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 588000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.692    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2296     |\n",
      "|    policy_loss        | -0.319   |\n",
      "|    value_loss         | 0.22     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 564      |\n",
      "|    ep_rew_mean        | 28.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 10390    |\n",
      "|    total_timesteps    | 588800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.691    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 0.807    |\n",
      "|    value_loss         | 1.37     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=16.60 +/- 4.41\n",
      "Episode length: 449.20 +/- 97.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 449      |\n",
      "|    mean_reward        | 16.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 592000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2312     |\n",
      "|    policy_loss        | 0.374    |\n",
      "|    value_loss         | 0.404    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=596000, episode_reward=25.40 +/- 15.17\n",
      "Episode length: 524.80 +/- 93.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 525      |\n",
      "|    mean_reward        | 25.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 596000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.666    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2328     |\n",
      "|    policy_loss        | 0.405    |\n",
      "|    value_loss         | 0.773    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=17.20 +/- 3.97\n",
      "Episode length: 479.20 +/- 102.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 479      |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 600000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.578    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2343     |\n",
      "|    policy_loss        | 0.494    |\n",
      "|    value_loss         | 1.4      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=604000, episode_reward=12.80 +/- 2.56\n",
      "Episode length: 367.60 +/- 32.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 368      |\n",
      "|    mean_reward        | 12.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 604000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.0242   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2359     |\n",
      "|    policy_loss        | 2.44     |\n",
      "|    value_loss         | 31.1     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=608000, episode_reward=15.80 +/- 6.62\n",
      "Episode length: 405.20 +/- 55.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 405      |\n",
      "|    mean_reward        | 15.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 608000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2374     |\n",
      "|    policy_loss        | 0.319    |\n",
      "|    value_loss         | 0.934    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=612000, episode_reward=23.80 +/- 8.23\n",
      "Episode length: 516.00 +/- 136.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 516      |\n",
      "|    mean_reward        | 23.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 612000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.818    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2390     |\n",
      "|    policy_loss        | 0.483    |\n",
      "|    value_loss         | 1.29     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 549      |\n",
      "|    ep_rew_mean        | 27.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 57       |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 10755    |\n",
      "|    total_timesteps    | 614400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.546    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | -0.647   |\n",
      "|    value_loss         | 1.04     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=616000, episode_reward=28.20 +/- 16.94\n",
      "Episode length: 474.80 +/- 134.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 475      |\n",
      "|    mean_reward        | 28.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 616000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.868    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2406     |\n",
      "|    policy_loss        | 0.0165   |\n",
      "|    value_loss         | 0.513    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=46.60 +/- 30.45\n",
      "Episode length: 586.80 +/- 115.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 587      |\n",
      "|    mean_reward        | 46.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 620000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.738    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2421     |\n",
      "|    policy_loss        | 0.698    |\n",
      "|    value_loss         | 1.04     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=44.20 +/- 34.94\n",
      "Episode length: 553.60 +/- 178.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 554      |\n",
      "|    mean_reward        | 44.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 624000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.662    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2437     |\n",
      "|    policy_loss        | -0.954   |\n",
      "|    value_loss         | 0.8      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=628000, episode_reward=18.20 +/- 6.43\n",
      "Episode length: 494.80 +/- 86.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 495      |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 628000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.559    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2453     |\n",
      "|    policy_loss        | -0.728   |\n",
      "|    value_loss         | 0.43     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=632000, episode_reward=24.60 +/- 5.43\n",
      "Episode length: 522.00 +/- 66.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 522      |\n",
      "|    mean_reward        | 24.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 632000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.8      |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2468     |\n",
      "|    policy_loss        | 0.0838   |\n",
      "|    value_loss         | 0.606    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=636000, episode_reward=30.00 +/- 5.44\n",
      "Episode length: 543.60 +/- 109.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 544      |\n",
      "|    mean_reward        | 30       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 636000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2484     |\n",
      "|    policy_loss        | -0.0603  |\n",
      "|    value_loss         | 0.474    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=21.00 +/- 8.10\n",
      "Episode length: 506.40 +/- 122.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 506      |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 640000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.791    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | -0.6     |\n",
      "|    value_loss         | 0.41     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 519      |\n",
      "|    ep_rew_mean     | 34.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 57       |\n",
      "|    iterations      | 2500     |\n",
      "|    time_elapsed    | 11148    |\n",
      "|    total_timesteps | 640000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=644000, episode_reward=30.00 +/- 17.44\n",
      "Episode length: 552.40 +/- 93.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 552      |\n",
      "|    mean_reward        | 30       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 644000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.861    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2515     |\n",
      "|    policy_loss        | -0.427   |\n",
      "|    value_loss         | 0.487    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=648000, episode_reward=13.20 +/- 4.17\n",
      "Episode length: 418.40 +/- 31.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 418      |\n",
      "|    mean_reward        | 13.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 648000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.46     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2531     |\n",
      "|    policy_loss        | -0.0335  |\n",
      "|    value_loss         | 0.691    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=652000, episode_reward=15.80 +/- 1.83\n",
      "Episode length: 424.40 +/- 31.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 424      |\n",
      "|    mean_reward        | 15.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 652000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.577    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2546     |\n",
      "|    policy_loss        | 0.163    |\n",
      "|    value_loss         | 0.604    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=656000, episode_reward=32.20 +/- 6.73\n",
      "Episode length: 508.00 +/- 42.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 508      |\n",
      "|    mean_reward        | 32.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 656000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2562     |\n",
      "|    policy_loss        | 0.401    |\n",
      "|    value_loss         | 1.44     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=38.40 +/- 11.77\n",
      "Episode length: 564.40 +/- 104.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 564      |\n",
      "|    mean_reward        | 38.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 660000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2578     |\n",
      "|    policy_loss        | -0.576   |\n",
      "|    value_loss         | 0.629    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=664000, episode_reward=24.20 +/- 5.42\n",
      "Episode length: 477.60 +/- 118.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 478      |\n",
      "|    mean_reward        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 664000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | -0.062   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2593     |\n",
      "|    policy_loss        | 12.4     |\n",
      "|    value_loss         | 637      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 482      |\n",
      "|    ep_rew_mean        | 32.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 57       |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 11519    |\n",
      "|    total_timesteps    | 665600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.632    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | -0.0446  |\n",
      "|    value_loss         | 1.57     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=668000, episode_reward=41.60 +/- 27.42\n",
      "Episode length: 579.00 +/- 126.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 579      |\n",
      "|    mean_reward        | 41.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 668000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.891    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2609     |\n",
      "|    policy_loss        | -0.724   |\n",
      "|    value_loss         | 0.659    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=17.80 +/- 8.66\n",
      "Episode length: 426.80 +/- 141.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 427      |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 672000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.521    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2624     |\n",
      "|    policy_loss        | 0.0484   |\n",
      "|    value_loss         | 1.71     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=676000, episode_reward=14.20 +/- 3.31\n",
      "Episode length: 381.60 +/- 51.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 382      |\n",
      "|    mean_reward        | 14.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 676000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.858    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2640     |\n",
      "|    policy_loss        | 0.00325  |\n",
      "|    value_loss         | 0.575    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=17.40 +/- 12.91\n",
      "Episode length: 443.60 +/- 136.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 444      |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 680000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | -0.0457  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2656     |\n",
      "|    policy_loss        | 0.394    |\n",
      "|    value_loss         | 16.7     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=684000, episode_reward=40.60 +/- 25.73\n",
      "Episode length: 496.60 +/- 141.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 497      |\n",
      "|    mean_reward        | 40.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 684000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0.798    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2671     |\n",
      "|    policy_loss        | 0.186    |\n",
      "|    value_loss         | 0.604    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=688000, episode_reward=18.20 +/- 4.71\n",
      "Episode length: 380.40 +/- 40.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 380      |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 688000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.885    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2687     |\n",
      "|    policy_loss        | 0.197    |\n",
      "|    value_loss         | 0.447    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 473      |\n",
      "|    ep_rew_mean        | 36.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 58       |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 11883    |\n",
      "|    total_timesteps    | 691200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | -0.024   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | -0.435   |\n",
      "|    value_loss         | 41.1     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=692000, episode_reward=28.80 +/- 33.68\n",
      "Episode length: 418.80 +/- 107.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 419      |\n",
      "|    mean_reward        | 28.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 692000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.58    |\n",
      "|    explained_variance | 0.127    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2703     |\n",
      "|    policy_loss        | -0.0943  |\n",
      "|    value_loss         | 2.6      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=696000, episode_reward=60.80 +/- 61.08\n",
      "Episode length: 519.60 +/- 162.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 520      |\n",
      "|    mean_reward        | 60.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 696000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.594    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2718     |\n",
      "|    policy_loss        | 0.0792   |\n",
      "|    value_loss         | 4.58     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=700000, episode_reward=14.60 +/- 4.45\n",
      "Episode length: 371.20 +/- 28.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 371      |\n",
      "|    mean_reward        | 14.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 700000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.75     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2734     |\n",
      "|    policy_loss        | -0.224   |\n",
      "|    value_loss         | 0.712    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=704000, episode_reward=41.60 +/- 17.48\n",
      "Episode length: 563.20 +/- 146.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 563      |\n",
      "|    mean_reward        | 41.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 704000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.751    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2749     |\n",
      "|    policy_loss        | 0.457    |\n",
      "|    value_loss         | 1.58     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=708000, episode_reward=32.20 +/- 13.76\n",
      "Episode length: 488.40 +/- 117.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 488      |\n",
      "|    mean_reward        | 32.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 708000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2765     |\n",
      "|    policy_loss        | 0.573    |\n",
      "|    value_loss         | 0.937    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=712000, episode_reward=26.60 +/- 5.89\n",
      "Episode length: 439.00 +/- 33.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 439      |\n",
      "|    mean_reward        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 712000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.753    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2781     |\n",
      "|    policy_loss        | 0.864    |\n",
      "|    value_loss         | 1.41     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=716000, episode_reward=46.40 +/- 35.10\n",
      "Episode length: 571.60 +/- 212.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 572      |\n",
      "|    mean_reward        | 46.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 716000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2796     |\n",
      "|    policy_loss        | -0.22    |\n",
      "|    value_loss         | 0.188    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 465      |\n",
      "|    ep_rew_mean        | 29.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 58       |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 12267    |\n",
      "|    total_timesteps    | 716800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | -0.147   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 2.83     |\n",
      "|    value_loss         | 37.8     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=28.40 +/- 31.17\n",
      "Episode length: 460.40 +/- 110.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 460      |\n",
      "|    mean_reward        | 28.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 720000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0.694    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2812     |\n",
      "|    policy_loss        | -0.19    |\n",
      "|    value_loss         | 0.401    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=724000, episode_reward=27.80 +/- 7.57\n",
      "Episode length: 554.00 +/- 172.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 554      |\n",
      "|    mean_reward        | 27.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 724000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2828     |\n",
      "|    policy_loss        | -0.836   |\n",
      "|    value_loss         | 0.913    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=728000, episode_reward=52.60 +/- 60.33\n",
      "Episode length: 470.80 +/- 143.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 471      |\n",
      "|    mean_reward        | 52.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 728000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.619    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2843     |\n",
      "|    policy_loss        | 0.162    |\n",
      "|    value_loss         | 1.49     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=732000, episode_reward=46.00 +/- 28.23\n",
      "Episode length: 581.80 +/- 168.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 582      |\n",
      "|    mean_reward        | 46       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 732000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | -0.0802  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2859     |\n",
      "|    policy_loss        | 4.54     |\n",
      "|    value_loss         | 111      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=736000, episode_reward=46.00 +/- 34.51\n",
      "Episode length: 535.60 +/- 124.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 536      |\n",
      "|    mean_reward        | 46       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 736000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.695    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2874     |\n",
      "|    policy_loss        | -0.468   |\n",
      "|    value_loss         | 0.47     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=41.00 +/- 25.89\n",
      "Episode length: 525.60 +/- 122.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 526      |\n",
      "|    mean_reward        | 41       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 740000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2890     |\n",
      "|    policy_loss        | -0.395   |\n",
      "|    value_loss         | 0.716    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 487      |\n",
      "|    ep_rew_mean        | 29.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 58       |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 12645    |\n",
      "|    total_timesteps    | 742400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | -0.0189  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 2.21     |\n",
      "|    value_loss         | 135      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=744000, episode_reward=25.60 +/- 11.53\n",
      "Episode length: 494.80 +/- 109.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 495      |\n",
      "|    mean_reward        | 25.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 744000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.354    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2906     |\n",
      "|    policy_loss        | -0.129   |\n",
      "|    value_loss         | 1.58     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=748000, episode_reward=22.40 +/- 9.73\n",
      "Episode length: 439.40 +/- 117.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 439      |\n",
      "|    mean_reward        | 22.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 748000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.19    |\n",
      "|    explained_variance | 0.528    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2921     |\n",
      "|    policy_loss        | -1.64    |\n",
      "|    value_loss         | 3.7      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=752000, episode_reward=23.60 +/- 30.81\n",
      "Episode length: 398.00 +/- 127.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 398      |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 752000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.0245   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2937     |\n",
      "|    policy_loss        | 0.852    |\n",
      "|    value_loss         | 19.3     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=756000, episode_reward=31.80 +/- 7.33\n",
      "Episode length: 585.20 +/- 149.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 585      |\n",
      "|    mean_reward        | 31.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 756000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.873   |\n",
      "|    explained_variance | 0.0624   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2953     |\n",
      "|    policy_loss        | -4.11    |\n",
      "|    value_loss         | 56.7     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=62.60 +/- 35.98\n",
      "Episode length: 529.60 +/- 128.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 530      |\n",
      "|    mean_reward        | 62.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 760000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.739    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2968     |\n",
      "|    policy_loss        | 0.225    |\n",
      "|    value_loss         | 1.71     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=764000, episode_reward=24.00 +/- 11.26\n",
      "Episode length: 446.80 +/- 91.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 447      |\n",
      "|    mean_reward        | 24       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 764000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.715    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2984     |\n",
      "|    policy_loss        | -1.31    |\n",
      "|    value_loss         | 1.2      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=32.20 +/- 15.12\n",
      "Episode length: 552.40 +/- 175.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 552      |\n",
      "|    mean_reward        | 32.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 768000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.61     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | -0.513   |\n",
      "|    value_loss         | 1.93     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 38.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 58       |\n",
      "|    iterations      | 3000     |\n",
      "|    time_elapsed    | 13037    |\n",
      "|    total_timesteps | 768000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=772000, episode_reward=68.40 +/- 64.92\n",
      "Episode length: 573.00 +/- 123.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 573      |\n",
      "|    mean_reward        | 68.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 772000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.444    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3015     |\n",
      "|    policy_loss        | -0.277   |\n",
      "|    value_loss         | 7.15     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=776000, episode_reward=16.80 +/- 11.53\n",
      "Episode length: 410.80 +/- 125.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 411      |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 776000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.799    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3031     |\n",
      "|    policy_loss        | -0.345   |\n",
      "|    value_loss         | 0.262    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=24.00 +/- 14.83\n",
      "Episode length: 481.20 +/- 103.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 481      |\n",
      "|    mean_reward        | 24       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 780000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0.777    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3046     |\n",
      "|    policy_loss        | -0.474   |\n",
      "|    value_loss         | 0.412    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=784000, episode_reward=18.40 +/- 5.75\n",
      "Episode length: 392.80 +/- 45.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 393      |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 784000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.619    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3062     |\n",
      "|    policy_loss        | -0.494   |\n",
      "|    value_loss         | 0.573    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=788000, episode_reward=17.60 +/- 10.05\n",
      "Episode length: 368.40 +/- 78.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 368      |\n",
      "|    mean_reward        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 788000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.706    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3078     |\n",
      "|    policy_loss        | 0.494    |\n",
      "|    value_loss         | 1.48     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=792000, episode_reward=22.80 +/- 11.39\n",
      "Episode length: 484.60 +/- 169.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 485      |\n",
      "|    mean_reward        | 22.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 792000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.369    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3093     |\n",
      "|    policy_loss        | -0.574   |\n",
      "|    value_loss         | 2.79     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 489      |\n",
      "|    ep_rew_mean        | 34.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 13405    |\n",
      "|    total_timesteps    | 793600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.544    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | -0.45    |\n",
      "|    value_loss         | 1.06     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=796000, episode_reward=31.80 +/- 9.91\n",
      "Episode length: 500.00 +/- 120.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 500      |\n",
      "|    mean_reward        | 31.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 796000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0.469    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3109     |\n",
      "|    policy_loss        | 0.716    |\n",
      "|    value_loss         | 3.95     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=31.00 +/- 5.66\n",
      "Episode length: 523.80 +/- 112.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 524      |\n",
      "|    mean_reward        | 31       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 800000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.642    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3124     |\n",
      "|    policy_loss        | 0.78     |\n",
      "|    value_loss         | 1.63     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=804000, episode_reward=29.20 +/- 9.95\n",
      "Episode length: 537.60 +/- 138.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 538      |\n",
      "|    mean_reward        | 29.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 804000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.0645   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3140     |\n",
      "|    policy_loss        | 0.504    |\n",
      "|    value_loss         | 14.2     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=808000, episode_reward=34.60 +/- 26.10\n",
      "Episode length: 480.00 +/- 54.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 480      |\n",
      "|    mean_reward        | 34.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 808000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.439    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3156     |\n",
      "|    policy_loss        | 0.552    |\n",
      "|    value_loss         | 3.28     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=812000, episode_reward=29.80 +/- 11.77\n",
      "Episode length: 576.40 +/- 120.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 576      |\n",
      "|    mean_reward        | 29.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 812000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.45     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3171     |\n",
      "|    policy_loss        | -0.12    |\n",
      "|    value_loss         | 0.937    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=816000, episode_reward=27.80 +/- 8.59\n",
      "Episode length: 507.20 +/- 76.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 507      |\n",
      "|    mean_reward        | 27.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 816000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.499    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3187     |\n",
      "|    policy_loss        | 0.468    |\n",
      "|    value_loss         | 1.91     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 490      |\n",
      "|    ep_rew_mean        | 29.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 13783    |\n",
      "|    total_timesteps    | 819200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.747    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -0.00365 |\n",
      "|    value_loss         | 1.47     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=36.60 +/- 3.38\n",
      "Episode length: 658.00 +/- 88.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 658      |\n",
      "|    mean_reward        | 36.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 820000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3203     |\n",
      "|    policy_loss        | -0.368   |\n",
      "|    value_loss         | 1.98     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=824000, episode_reward=27.60 +/- 27.27\n",
      "Episode length: 371.00 +/- 70.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 371      |\n",
      "|    mean_reward        | 27.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 824000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | -0.167   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3218     |\n",
      "|    policy_loss        | 0.892    |\n",
      "|    value_loss         | 19.7     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=828000, episode_reward=29.80 +/- 10.74\n",
      "Episode length: 472.40 +/- 115.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 472      |\n",
      "|    mean_reward        | 29.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 828000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.498    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3234     |\n",
      "|    policy_loss        | -0.252   |\n",
      "|    value_loss         | 3.65     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=832000, episode_reward=69.00 +/- 53.24\n",
      "Episode length: 677.00 +/- 199.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 677      |\n",
      "|    mean_reward        | 69       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 832000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3249     |\n",
      "|    policy_loss        | -0.404   |\n",
      "|    value_loss         | 1.22     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=836000, episode_reward=37.40 +/- 16.45\n",
      "Episode length: 545.00 +/- 163.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 545      |\n",
      "|    mean_reward        | 37.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 836000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.695    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3265     |\n",
      "|    policy_loss        | -0.676   |\n",
      "|    value_loss         | 1.15     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=30.00 +/- 17.48\n",
      "Episode length: 519.00 +/- 178.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 519      |\n",
      "|    mean_reward        | 30       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 840000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | -0.0445  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3281     |\n",
      "|    policy_loss        | 7.71     |\n",
      "|    value_loss         | 233      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=844000, episode_reward=70.40 +/- 57.62\n",
      "Episode length: 561.80 +/- 122.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 562      |\n",
      "|    mean_reward        | 70.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 844000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | -0.00522 |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3296     |\n",
      "|    policy_loss        | 1.76     |\n",
      "|    value_loss         | 64.3     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 518      |\n",
      "|    ep_rew_mean        | 33.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 14179    |\n",
      "|    total_timesteps    | 844800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.0339   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | 3.21     |\n",
      "|    value_loss         | 33.6     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=848000, episode_reward=68.20 +/- 65.44\n",
      "Episode length: 590.80 +/- 199.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 591      |\n",
      "|    mean_reward        | 68.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 848000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.809    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3312     |\n",
      "|    policy_loss        | 0.0371   |\n",
      "|    value_loss         | 1.02     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=852000, episode_reward=40.20 +/- 9.54\n",
      "Episode length: 601.60 +/- 113.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 602      |\n",
      "|    mean_reward        | 40.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 852000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 0.787    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3328     |\n",
      "|    policy_loss        | -0.0452  |\n",
      "|    value_loss         | 1.69     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=856000, episode_reward=46.80 +/- 26.84\n",
      "Episode length: 610.80 +/- 147.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 611      |\n",
      "|    mean_reward        | 46.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 856000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3343     |\n",
      "|    policy_loss        | -0.665   |\n",
      "|    value_loss         | 1.4      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=112.00 +/- 117.10\n",
      "Episode length: 630.80 +/- 97.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 631      |\n",
      "|    mean_reward        | 112      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 860000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3359     |\n",
      "|    policy_loss        | -1.01    |\n",
      "|    value_loss         | 1.49     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=864000, episode_reward=36.60 +/- 11.22\n",
      "Episode length: 599.20 +/- 164.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 599      |\n",
      "|    mean_reward        | 36.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 864000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 0.608    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3374     |\n",
      "|    policy_loss        | -0.292   |\n",
      "|    value_loss         | 1.47     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=868000, episode_reward=34.60 +/- 11.07\n",
      "Episode length: 572.40 +/- 183.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 572      |\n",
      "|    mean_reward        | 34.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 868000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3390     |\n",
      "|    policy_loss        | -0.317   |\n",
      "|    value_loss         | 0.299    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 588      |\n",
      "|    ep_rew_mean        | 47       |\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 14570    |\n",
      "|    total_timesteps    | 870400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | -0.272   |\n",
      "|    value_loss         | 0.743    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=872000, episode_reward=45.00 +/- 12.12\n",
      "Episode length: 718.40 +/- 209.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 718      |\n",
      "|    mean_reward        | 45       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 872000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.805    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3406     |\n",
      "|    policy_loss        | 0.524    |\n",
      "|    value_loss         | 0.882    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=876000, episode_reward=41.20 +/- 10.96\n",
      "Episode length: 765.60 +/- 195.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 766      |\n",
      "|    mean_reward        | 41.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 876000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.533    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3421     |\n",
      "|    policy_loss        | 0.339    |\n",
      "|    value_loss         | 2.25     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=62.40 +/- 32.77\n",
      "Episode length: 599.20 +/- 88.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 599      |\n",
      "|    mean_reward        | 62.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 880000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.885    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3437     |\n",
      "|    policy_loss        | 0.723    |\n",
      "|    value_loss         | 1.8      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=884000, episode_reward=35.60 +/- 14.49\n",
      "Episode length: 489.00 +/- 114.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 489      |\n",
      "|    mean_reward        | 35.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 884000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.613    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3453     |\n",
      "|    policy_loss        | 0.489    |\n",
      "|    value_loss         | 3.7      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=888000, episode_reward=26.40 +/- 4.13\n",
      "Episode length: 502.40 +/- 34.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 502      |\n",
      "|    mean_reward        | 26.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 888000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0.788    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3468     |\n",
      "|    policy_loss        | 0.871    |\n",
      "|    value_loss         | 3.75     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=892000, episode_reward=30.00 +/- 8.79\n",
      "Episode length: 515.60 +/- 83.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 516      |\n",
      "|    mean_reward        | 30       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 892000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.724    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3484     |\n",
      "|    policy_loss        | -0.209   |\n",
      "|    value_loss         | 1.05     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=896000, episode_reward=53.00 +/- 21.46\n",
      "Episode length: 655.20 +/- 86.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 655      |\n",
      "|    mean_reward        | 53       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 896000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.841    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | 0.226    |\n",
      "|    value_loss         | 0.809    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 620      |\n",
      "|    ep_rew_mean     | 52.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 59       |\n",
      "|    iterations      | 3500     |\n",
      "|    time_elapsed    | 14979    |\n",
      "|    total_timesteps | 896000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=85.20 +/- 65.54\n",
      "Episode length: 666.20 +/- 164.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 666      |\n",
      "|    mean_reward        | 85.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 900000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.847    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3515     |\n",
      "|    policy_loss        | 0.0404   |\n",
      "|    value_loss         | 1.31     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=904000, episode_reward=83.20 +/- 56.32\n",
      "Episode length: 770.00 +/- 221.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 770      |\n",
      "|    mean_reward        | 83.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 904000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3531     |\n",
      "|    policy_loss        | -0.758   |\n",
      "|    value_loss         | 1.28     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=908000, episode_reward=114.20 +/- 120.90\n",
      "Episode length: 611.60 +/- 121.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 612      |\n",
      "|    mean_reward        | 114      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 908000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3546     |\n",
      "|    policy_loss        | -0.603   |\n",
      "|    value_loss         | 0.835    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=912000, episode_reward=54.20 +/- 32.01\n",
      "Episode length: 668.40 +/- 142.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 668      |\n",
      "|    mean_reward        | 54.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 912000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | -0.134   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3562     |\n",
      "|    policy_loss        | 7.55     |\n",
      "|    value_loss         | 209      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=916000, episode_reward=65.40 +/- 58.39\n",
      "Episode length: 573.80 +/- 146.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 574      |\n",
      "|    mean_reward        | 65.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 916000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.733    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3578     |\n",
      "|    policy_loss        | -0.111   |\n",
      "|    value_loss         | 1        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=31.20 +/- 12.01\n",
      "Episode length: 551.20 +/- 141.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 551      |\n",
      "|    mean_reward        | 31.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 920000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3593     |\n",
      "|    policy_loss        | 0.0264   |\n",
      "|    value_loss         | 0.329    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 618       |\n",
      "|    ep_rew_mean        | 54.7      |\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 15377     |\n",
      "|    total_timesteps    | 921600    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.25     |\n",
      "|    explained_variance | 0.765     |\n",
      "|    learning_rate      | 0.0005    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | -0.000791 |\n",
      "|    value_loss         | 1.71      |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=924000, episode_reward=43.00 +/- 16.44\n",
      "Episode length: 620.80 +/- 170.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 621      |\n",
      "|    mean_reward        | 43       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 924000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.272    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3609     |\n",
      "|    policy_loss        | -0.0349  |\n",
      "|    value_loss         | 1        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=928000, episode_reward=59.20 +/- 26.75\n",
      "Episode length: 723.00 +/- 84.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 723      |\n",
      "|    mean_reward        | 59.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 928000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3624     |\n",
      "|    policy_loss        | 0.526    |\n",
      "|    value_loss         | 1.08     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=932000, episode_reward=35.40 +/- 12.16\n",
      "Episode length: 607.20 +/- 108.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 607      |\n",
      "|    mean_reward        | 35.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 932000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0.588    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3640     |\n",
      "|    policy_loss        | 0.757    |\n",
      "|    value_loss         | 1.62     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=936000, episode_reward=43.20 +/- 11.72\n",
      "Episode length: 624.00 +/- 136.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 624      |\n",
      "|    mean_reward        | 43.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 936000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.715    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3656     |\n",
      "|    policy_loss        | -0.141   |\n",
      "|    value_loss         | 1.57     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=47.00 +/- 11.54\n",
      "Episode length: 751.60 +/- 203.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 752      |\n",
      "|    mean_reward        | 47       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 940000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.248    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3671     |\n",
      "|    policy_loss        | 0.947    |\n",
      "|    value_loss         | 23.4     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=944000, episode_reward=43.20 +/- 17.12\n",
      "Episode length: 649.00 +/- 143.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 649      |\n",
      "|    mean_reward        | 43.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 944000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.0299   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3687     |\n",
      "|    policy_loss        | 1.45     |\n",
      "|    value_loss         | 56.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 617      |\n",
      "|    ep_rew_mean        | 50.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 60       |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 15777    |\n",
      "|    total_timesteps    | 947200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.654    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | -0.313   |\n",
      "|    value_loss         | 1.84     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=948000, episode_reward=99.60 +/- 120.05\n",
      "Episode length: 562.80 +/- 105.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 563      |\n",
      "|    mean_reward        | 99.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 948000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3703     |\n",
      "|    policy_loss        | -0.113   |\n",
      "|    value_loss         | 0.844    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=952000, episode_reward=57.80 +/- 28.22\n",
      "Episode length: 645.60 +/- 157.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 646      |\n",
      "|    mean_reward        | 57.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 952000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3718     |\n",
      "|    policy_loss        | 0.374    |\n",
      "|    value_loss         | 2.38     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=956000, episode_reward=58.40 +/- 37.84\n",
      "Episode length: 664.80 +/- 171.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 665      |\n",
      "|    mean_reward        | 58.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 956000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3734     |\n",
      "|    policy_loss        | 0.379    |\n",
      "|    value_loss         | 0.321    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=36.20 +/- 10.53\n",
      "Episode length: 636.40 +/- 122.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 636      |\n",
      "|    mean_reward        | 36.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 960000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.676    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3749     |\n",
      "|    policy_loss        | -0.828   |\n",
      "|    value_loss         | 1.85     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=964000, episode_reward=25.20 +/- 3.06\n",
      "Episode length: 499.20 +/- 72.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 499      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 964000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | -0.108   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3765     |\n",
      "|    policy_loss        | 1.81     |\n",
      "|    value_loss         | 23.4     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=968000, episode_reward=54.40 +/- 31.58\n",
      "Episode length: 754.40 +/- 110.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 754      |\n",
      "|    mean_reward        | 54.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 968000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.26     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3781     |\n",
      "|    policy_loss        | 0.256    |\n",
      "|    value_loss         | 2.61     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=972000, episode_reward=57.80 +/- 28.25\n",
      "Episode length: 691.80 +/- 128.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 692      |\n",
      "|    mean_reward        | 57.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 972000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.718    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3796     |\n",
      "|    policy_loss        | 0.349    |\n",
      "|    value_loss         | 1.43     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 602      |\n",
      "|    ep_rew_mean        | 44.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 60       |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 16193    |\n",
      "|    total_timesteps    | 972800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.715    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | -0.33    |\n",
      "|    value_loss         | 1.46     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=976000, episode_reward=76.40 +/- 44.37\n",
      "Episode length: 671.00 +/- 190.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 671      |\n",
      "|    mean_reward        | 76.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 976000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0.717    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3812     |\n",
      "|    policy_loss        | 0.543    |\n",
      "|    value_loss         | 1.62     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=85.40 +/- 58.27\n",
      "Episode length: 596.00 +/- 146.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 596      |\n",
      "|    mean_reward        | 85.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 980000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.732    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3828     |\n",
      "|    policy_loss        | 0.958    |\n",
      "|    value_loss         | 3.62     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=984000, episode_reward=65.40 +/- 14.50\n",
      "Episode length: 770.60 +/- 152.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 771      |\n",
      "|    mean_reward        | 65.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 984000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3843     |\n",
      "|    policy_loss        | -0.336   |\n",
      "|    value_loss         | 1.15     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=988000, episode_reward=38.20 +/- 13.15\n",
      "Episode length: 638.00 +/- 159.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 638      |\n",
      "|    mean_reward        | 38.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 988000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.656    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3859     |\n",
      "|    policy_loss        | -0.45    |\n",
      "|    value_loss         | 1.12     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=992000, episode_reward=59.60 +/- 36.24\n",
      "Episode length: 628.80 +/- 139.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 629      |\n",
      "|    mean_reward        | 59.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 992000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3874     |\n",
      "|    policy_loss        | -0.46    |\n",
      "|    value_loss         | 0.951    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=996000, episode_reward=57.60 +/- 55.56\n",
      "Episode length: 495.60 +/- 70.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 496      |\n",
      "|    mean_reward        | 57.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 996000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.636    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3890     |\n",
      "|    policy_loss        | 0.132    |\n",
      "|    value_loss         | 2.11     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 644      |\n",
      "|    ep_rew_mean        | 53       |\n",
      "| time/                 |          |\n",
      "|    fps                | 60       |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 16591    |\n",
      "|    total_timesteps    | 998400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.785    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 0.736    |\n",
      "|    value_loss         | 2.4      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=34.20 +/- 7.55\n",
      "Episode length: 560.00 +/- 135.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 560      |\n",
      "|    mean_reward        | 34.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0.766    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3906     |\n",
      "|    policy_loss        | -0.674   |\n",
      "|    value_loss         | 1.59     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1004000, episode_reward=50.40 +/- 21.95\n",
      "Episode length: 574.60 +/- 121.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 575      |\n",
      "|    mean_reward        | 50.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1004000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.663    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3921     |\n",
      "|    policy_loss        | 0.372    |\n",
      "|    value_loss         | 1.99     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=31.20 +/- 2.64\n",
      "Episode length: 498.00 +/- 38.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 498      |\n",
      "|    mean_reward        | 31.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3937     |\n",
      "|    policy_loss        | -0.606   |\n",
      "|    value_loss         | 1.25     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1012000, episode_reward=43.60 +/- 22.75\n",
      "Episode length: 630.80 +/- 249.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 631      |\n",
      "|    mean_reward        | 43.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1012000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3953     |\n",
      "|    policy_loss        | 0.327    |\n",
      "|    value_loss         | 0.929    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1016000, episode_reward=42.60 +/- 15.93\n",
      "Episode length: 555.20 +/- 112.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 555      |\n",
      "|    mean_reward        | 42.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3968     |\n",
      "|    policy_loss        | -0.00504 |\n",
      "|    value_loss         | 0.29     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=32.80 +/- 14.46\n",
      "Episode length: 489.20 +/- 120.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 489      |\n",
      "|    mean_reward        | 32.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3984     |\n",
      "|    policy_loss        | 0.245    |\n",
      "|    value_loss         | 1.12     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1024000, episode_reward=42.60 +/- 10.71\n",
      "Episode length: 555.60 +/- 120.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 556      |\n",
      "|    mean_reward        | 42.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.706    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | -0.0647  |\n",
      "|    value_loss         | 2.69     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 650      |\n",
      "|    ep_rew_mean     | 51.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 60       |\n",
      "|    iterations      | 4000     |\n",
      "|    time_elapsed    | 16992    |\n",
      "|    total_timesteps | 1024000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1028000, episode_reward=117.00 +/- 46.87\n",
      "Episode length: 747.20 +/- 109.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 747      |\n",
      "|    mean_reward        | 117      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1028000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | 0.827    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4015     |\n",
      "|    policy_loss        | -0.998   |\n",
      "|    value_loss         | 2.45     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1032000, episode_reward=34.40 +/- 4.84\n",
      "Episode length: 559.20 +/- 79.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 559      |\n",
      "|    mean_reward        | 34.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4031     |\n",
      "|    policy_loss        | -0.45    |\n",
      "|    value_loss         | 0.447    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1036000, episode_reward=41.40 +/- 27.55\n",
      "Episode length: 471.20 +/- 41.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 471      |\n",
      "|    mean_reward        | 41.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1036000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.46     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4046     |\n",
      "|    policy_loss        | 0.788    |\n",
      "|    value_loss         | 3.52     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=47.40 +/- 33.61\n",
      "Episode length: 537.20 +/- 171.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 537      |\n",
      "|    mean_reward        | 47.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.616    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4062     |\n",
      "|    policy_loss        | 0.0431   |\n",
      "|    value_loss         | 2.19     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1044000, episode_reward=41.80 +/- 19.71\n",
      "Episode length: 553.80 +/- 147.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 554      |\n",
      "|    mean_reward        | 41.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1044000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4078     |\n",
      "|    policy_loss        | -0.13    |\n",
      "|    value_loss         | 0.709    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1048000, episode_reward=86.00 +/- 49.35\n",
      "Episode length: 755.00 +/- 189.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 755      |\n",
      "|    mean_reward        | 86       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4093     |\n",
      "|    policy_loss        | 0.0646   |\n",
      "|    value_loss         | 0.694    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 657      |\n",
      "|    ep_rew_mean        | 50.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 60       |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 17383    |\n",
      "|    total_timesteps    | 1049600  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | -0.423   |\n",
      "|    value_loss         | 1.92     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1052000, episode_reward=89.40 +/- 26.94\n",
      "Episode length: 620.20 +/- 94.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 620      |\n",
      "|    mean_reward        | 89.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1052000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.726    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4109     |\n",
      "|    policy_loss        | -0.296   |\n",
      "|    value_loss         | 1.75     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1056000, episode_reward=47.60 +/- 21.10\n",
      "Episode length: 626.40 +/- 183.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 626      |\n",
      "|    mean_reward        | 47.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.109    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4124     |\n",
      "|    policy_loss        | 2.03     |\n",
      "|    value_loss         | 32       |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=33.40 +/- 12.21\n",
      "Episode length: 551.60 +/- 156.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 552      |\n",
      "|    mean_reward        | 33.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4140     |\n",
      "|    policy_loss        | -0.458   |\n",
      "|    value_loss         | 0.864    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1064000, episode_reward=70.60 +/- 53.97\n",
      "Episode length: 588.80 +/- 104.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 589      |\n",
      "|    mean_reward        | 70.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | -0.0934  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4156     |\n",
      "|    policy_loss        | 3.18     |\n",
      "|    value_loss         | 50.7     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1068000, episode_reward=41.80 +/- 10.32\n",
      "Episode length: 611.60 +/- 150.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 612      |\n",
      "|    mean_reward        | 41.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1068000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.3      |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4171     |\n",
      "|    policy_loss        | 0.473    |\n",
      "|    value_loss         | 7.35     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1072000, episode_reward=61.20 +/- 29.06\n",
      "Episode length: 692.00 +/- 192.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 692      |\n",
      "|    mean_reward        | 61.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.963   |\n",
      "|    explained_variance | 0.254    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4187     |\n",
      "|    policy_loss        | -0.218   |\n",
      "|    value_loss         | 3.89     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 654      |\n",
      "|    ep_rew_mean        | 53       |\n",
      "| time/                 |          |\n",
      "|    fps                | 60       |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 17778    |\n",
      "|    total_timesteps    | 1075200  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.74     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | 0.985    |\n",
      "|    value_loss         | 4.21     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1076000, episode_reward=48.80 +/- 5.04\n",
      "Episode length: 702.40 +/- 66.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 702      |\n",
      "|    mean_reward        | 48.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1076000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4203     |\n",
      "|    policy_loss        | -0.859   |\n",
      "|    value_loss         | 1.64     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=70.40 +/- 21.75\n",
      "Episode length: 886.00 +/- 156.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 886      |\n",
      "|    mean_reward        | 70.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.455    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4218     |\n",
      "|    policy_loss        | -0.0682  |\n",
      "|    value_loss         | 5.55     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1084000, episode_reward=36.80 +/- 10.34\n",
      "Episode length: 639.60 +/- 158.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 640      |\n",
      "|    mean_reward        | 36.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1084000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4234     |\n",
      "|    policy_loss        | 0.0638   |\n",
      "|    value_loss         | 1.15     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1088000, episode_reward=41.60 +/- 9.48\n",
      "Episode length: 595.00 +/- 112.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 595      |\n",
      "|    mean_reward        | 41.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.795    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4249     |\n",
      "|    policy_loss        | 0.175    |\n",
      "|    value_loss         | 0.547    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1092000, episode_reward=52.40 +/- 12.52\n",
      "Episode length: 671.80 +/- 116.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 672      |\n",
      "|    mean_reward        | 52.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1092000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.436    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4265     |\n",
      "|    policy_loss        | 0.274    |\n",
      "|    value_loss         | 1.81     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1096000, episode_reward=37.80 +/- 7.76\n",
      "Episode length: 575.20 +/- 112.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 575      |\n",
      "|    mean_reward        | 37.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.509    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4281     |\n",
      "|    policy_loss        | 0.677    |\n",
      "|    value_loss         | 2.09     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=62.80 +/- 41.18\n",
      "Episode length: 763.00 +/- 114.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 763      |\n",
      "|    mean_reward        | 62.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4296     |\n",
      "|    policy_loss        | -0.978   |\n",
      "|    value_loss         | 1.31     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 629      |\n",
      "|    ep_rew_mean        | 54       |\n",
      "| time/                 |          |\n",
      "|    fps                | 60       |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 18207    |\n",
      "|    total_timesteps    | 1100800  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -0.244   |\n",
      "|    value_loss         | 0.414    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1104000, episode_reward=42.80 +/- 9.37\n",
      "Episode length: 585.80 +/- 106.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 586      |\n",
      "|    mean_reward        | 42.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0.868    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4312     |\n",
      "|    policy_loss        | -0.441   |\n",
      "|    value_loss         | 1.2      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1108000, episode_reward=39.60 +/- 9.73\n",
      "Episode length: 724.80 +/- 166.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 725      |\n",
      "|    mean_reward        | 39.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1108000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.974   |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4328     |\n",
      "|    policy_loss        | -0.773   |\n",
      "|    value_loss         | 2.66     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1112000, episode_reward=105.20 +/- 27.58\n",
      "Episode length: 846.00 +/- 101.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 846      |\n",
      "|    mean_reward        | 105      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | -0.00566 |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4343     |\n",
      "|    policy_loss        | 21.8     |\n",
      "|    value_loss         | 2.67e+03 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1116000, episode_reward=47.60 +/- 11.04\n",
      "Episode length: 712.40 +/- 161.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 712      |\n",
      "|    mean_reward        | 47.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1116000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.63     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4359     |\n",
      "|    policy_loss        | -0.857   |\n",
      "|    value_loss         | 4.93     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=101.60 +/- 93.44\n",
      "Episode length: 641.00 +/- 303.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 641      |\n",
      "|    mean_reward        | 102      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4374     |\n",
      "|    policy_loss        | -0.167   |\n",
      "|    value_loss         | 0.951    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1124000, episode_reward=72.60 +/- 57.93\n",
      "Episode length: 653.20 +/- 31.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 653      |\n",
      "|    mean_reward        | 72.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1124000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.0639   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4390     |\n",
      "|    policy_loss        | 3.34     |\n",
      "|    value_loss         | 268      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 657      |\n",
      "|    ep_rew_mean        | 67.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 60       |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 18618    |\n",
      "|    total_timesteps    | 1126400  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.507    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | 0.378    |\n",
      "|    value_loss         | 5.34     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1128000, episode_reward=46.00 +/- 18.51\n",
      "Episode length: 679.60 +/- 32.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 680      |\n",
      "|    mean_reward        | 46       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.493    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4406     |\n",
      "|    policy_loss        | -0.648   |\n",
      "|    value_loss         | 2.38     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1132000, episode_reward=27.00 +/- 10.35\n",
      "Episode length: 513.20 +/- 149.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 513      |\n",
      "|    mean_reward        | 27       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1132000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.714    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4421     |\n",
      "|    policy_loss        | -0.158   |\n",
      "|    value_loss         | 0.787    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1136000, episode_reward=42.40 +/- 14.22\n",
      "Episode length: 5968.00 +/- 10514.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.97e+03 |\n",
      "|    mean_reward        | 42.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.00543  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4437     |\n",
      "|    policy_loss        | 5.9      |\n",
      "|    value_loss         | 331      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=89.40 +/- 123.11\n",
      "Episode length: 546.60 +/- 143.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 547      |\n",
      "|    mean_reward        | 89.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4453     |\n",
      "|    policy_loss        | -0.38    |\n",
      "|    value_loss         | 0.836    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1144000, episode_reward=28.60 +/- 14.66\n",
      "Episode length: 510.40 +/- 85.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 510      |\n",
      "|    mean_reward        | 28.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.243    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4468     |\n",
      "|    policy_loss        | -0.459   |\n",
      "|    value_loss         | 0.282    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1148000, episode_reward=40.60 +/- 15.78\n",
      "Episode length: 662.00 +/- 118.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 662      |\n",
      "|    mean_reward        | 40.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1148000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4484     |\n",
      "|    policy_loss        | -0.245   |\n",
      "|    value_loss         | 0.578    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1152000, episode_reward=33.20 +/- 11.12\n",
      "Episode length: 544.00 +/- 112.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 544      |\n",
      "|    mean_reward        | 33.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.717    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | -0.112   |\n",
      "|    value_loss         | 1.75     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 654      |\n",
      "|    ep_rew_mean     | 65.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 60       |\n",
      "|    iterations      | 4500     |\n",
      "|    time_elapsed    | 19178    |\n",
      "|    total_timesteps | 1152000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1156000, episode_reward=37.40 +/- 13.79\n",
      "Episode length: 610.40 +/- 139.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 610      |\n",
      "|    mean_reward        | 37.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1156000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0.667    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4515     |\n",
      "|    policy_loss        | -0.0664  |\n",
      "|    value_loss         | 0.642    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=48.20 +/- 16.34\n",
      "Episode length: 730.40 +/- 235.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 730      |\n",
      "|    mean_reward        | 48.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.643    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4531     |\n",
      "|    policy_loss        | 0.817    |\n",
      "|    value_loss         | 1.95     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1164000, episode_reward=49.60 +/- 8.19\n",
      "Episode length: 647.80 +/- 106.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 648      |\n",
      "|    mean_reward        | 49.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1164000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.739    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4546     |\n",
      "|    policy_loss        | 0.551    |\n",
      "|    value_loss         | 1.75     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1168000, episode_reward=44.80 +/- 12.25\n",
      "Episode length: 5973.20 +/- 10511.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.97e+03 |\n",
      "|    mean_reward        | 44.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.136    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4562     |\n",
      "|    policy_loss        | 0.525    |\n",
      "|    value_loss         | 75.3     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1172000, episode_reward=64.20 +/- 10.17\n",
      "Episode length: 794.80 +/- 159.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 795      |\n",
      "|    mean_reward        | 64.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1172000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.26     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4578     |\n",
      "|    policy_loss        | 2.78     |\n",
      "|    value_loss         | 38.3     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1176000, episode_reward=50.40 +/- 21.70\n",
      "Episode length: 714.80 +/- 139.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 715      |\n",
      "|    mean_reward        | 50.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4593     |\n",
      "|    policy_loss        | -0.303   |\n",
      "|    value_loss         | 1.29     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 662      |\n",
      "|    ep_rew_mean        | 68.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 19744    |\n",
      "|    total_timesteps    | 1177600  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.755    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | -0.557   |\n",
      "|    value_loss         | 0.429    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=107.80 +/- 46.93\n",
      "Episode length: 793.20 +/- 110.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 793      |\n",
      "|    mean_reward        | 108      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4609     |\n",
      "|    policy_loss        | -0.162   |\n",
      "|    value_loss         | 1.15     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1184000, episode_reward=67.60 +/- 13.57\n",
      "Episode length: 688.60 +/- 94.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 689      |\n",
      "|    mean_reward        | 67.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.818    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4624     |\n",
      "|    policy_loss        | 0.43     |\n",
      "|    value_loss         | 1.82     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1188000, episode_reward=59.20 +/- 15.77\n",
      "Episode length: 728.60 +/- 56.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 729      |\n",
      "|    mean_reward        | 59.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1188000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4640     |\n",
      "|    policy_loss        | -0.837   |\n",
      "|    value_loss         | 1.88     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1192000, episode_reward=77.60 +/- 39.08\n",
      "Episode length: 833.60 +/- 156.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 834      |\n",
      "|    mean_reward        | 77.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4656     |\n",
      "|    policy_loss        | 1.34     |\n",
      "|    value_loss         | 3.88     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1196000, episode_reward=61.40 +/- 31.07\n",
      "Episode length: 842.40 +/- 224.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 842      |\n",
      "|    mean_reward        | 61.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1196000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.127    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4671     |\n",
      "|    policy_loss        | 3.75     |\n",
      "|    value_loss         | 54.9     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=86.80 +/- 56.61\n",
      "Episode length: 749.40 +/- 196.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 749      |\n",
      "|    mean_reward        | 86.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.823    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4687     |\n",
      "|    policy_loss        | -0.295   |\n",
      "|    value_loss         | 0.551    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 695      |\n",
      "|    ep_rew_mean        | 68.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 20164    |\n",
      "|    total_timesteps    | 1203200  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.51     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | -0.945   |\n",
      "|    value_loss         | 4.69     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1204000, episode_reward=47.40 +/- 7.39\n",
      "Episode length: 668.80 +/- 83.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 669      |\n",
      "|    mean_reward        | 47.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1204000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4703     |\n",
      "|    policy_loss        | 0.331    |\n",
      "|    value_loss         | 1.16     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1208000, episode_reward=54.20 +/- 19.35\n",
      "Episode length: 759.60 +/- 255.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 760      |\n",
      "|    mean_reward        | 54.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.573    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4718     |\n",
      "|    policy_loss        | -0.191   |\n",
      "|    value_loss         | 2.14     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1212000, episode_reward=57.40 +/- 28.00\n",
      "Episode length: 705.20 +/- 115.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 705      |\n",
      "|    mean_reward        | 57.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1212000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4734     |\n",
      "|    policy_loss        | -0.0556  |\n",
      "|    value_loss         | 0.748    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1216000, episode_reward=59.60 +/- 21.15\n",
      "Episode length: 5962.80 +/- 10518.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.96e+03 |\n",
      "|    mean_reward        | 59.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4749     |\n",
      "|    policy_loss        | 0.042    |\n",
      "|    value_loss         | 0.774    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=32.20 +/- 8.38\n",
      "Episode length: 11236.80 +/- 12867.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 32.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.653    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4765     |\n",
      "|    policy_loss        | 0.0623   |\n",
      "|    value_loss         | 1.9      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1224000, episode_reward=42.20 +/- 11.12\n",
      "Episode length: 732.00 +/- 83.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 732      |\n",
      "|    mean_reward        | 42.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4781     |\n",
      "|    policy_loss        | -0.561   |\n",
      "|    value_loss         | 0.447    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1228000, episode_reward=53.80 +/- 28.04\n",
      "Episode length: 736.60 +/- 124.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 737      |\n",
      "|    mean_reward        | 53.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1228000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.202    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4796     |\n",
      "|    policy_loss        | 2.17     |\n",
      "|    value_loss         | 51.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 766      |\n",
      "|    ep_rew_mean        | 70.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 58       |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 21061    |\n",
      "|    total_timesteps    | 1228800  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 0.835    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | -0.116   |\n",
      "|    value_loss         | 0.792    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1232000, episode_reward=68.40 +/- 23.92\n",
      "Episode length: 845.20 +/- 138.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 845      |\n",
      "|    mean_reward        | 68.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0.731    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4812     |\n",
      "|    policy_loss        | -0.499   |\n",
      "|    value_loss         | 2.43     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1236000, episode_reward=80.20 +/- 31.28\n",
      "Episode length: 759.00 +/- 184.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 759      |\n",
      "|    mean_reward        | 80.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1236000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.772    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4828     |\n",
      "|    policy_loss        | 0.659    |\n",
      "|    value_loss         | 1.99     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=109.80 +/- 106.95\n",
      "Episode length: 775.40 +/- 134.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 775      |\n",
      "|    mean_reward        | 110      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.906    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4843     |\n",
      "|    policy_loss        | -0.515   |\n",
      "|    value_loss         | 1.67     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1244000, episode_reward=52.00 +/- 11.63\n",
      "Episode length: 817.60 +/- 131.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 818      |\n",
      "|    mean_reward        | 52       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1244000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4859     |\n",
      "|    policy_loss        | 0.165    |\n",
      "|    value_loss         | 0.936    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1248000, episode_reward=124.20 +/- 65.48\n",
      "Episode length: 708.40 +/- 70.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 708      |\n",
      "|    mean_reward        | 124      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.9      |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4874     |\n",
      "|    policy_loss        | -0.982   |\n",
      "|    value_loss         | 2.32     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1252000, episode_reward=55.60 +/- 12.53\n",
      "Episode length: 706.00 +/- 206.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 706      |\n",
      "|    mean_reward        | 55.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1252000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4890     |\n",
      "|    policy_loss        | -0.0915  |\n",
      "|    value_loss         | 0.705    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 935      |\n",
      "|    ep_rew_mean        | 75.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 58       |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 21482    |\n",
      "|    total_timesteps    | 1254400  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 0.752    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | -0.522   |\n",
      "|    value_loss         | 0.755    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1256000, episode_reward=127.60 +/- 127.91\n",
      "Episode length: 805.00 +/- 317.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 805      |\n",
      "|    mean_reward        | 128      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.853    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4906     |\n",
      "|    policy_loss        | 0.272    |\n",
      "|    value_loss         | 2.01     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1260000, episode_reward=75.40 +/- 19.12\n",
      "Episode length: 767.20 +/- 200.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 767      |\n",
      "|    mean_reward        | 75.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4921     |\n",
      "|    policy_loss        | 0.74     |\n",
      "|    value_loss         | 1.33     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1264000, episode_reward=75.00 +/- 24.27\n",
      "Episode length: 651.20 +/- 53.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 651      |\n",
      "|    mean_reward        | 75       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4937     |\n",
      "|    policy_loss        | -0.223   |\n",
      "|    value_loss         | 0.611    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1268000, episode_reward=82.40 +/- 28.58\n",
      "Episode length: 731.20 +/- 202.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 731      |\n",
      "|    mean_reward        | 82.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1268000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4953     |\n",
      "|    policy_loss        | 0.015    |\n",
      "|    value_loss         | 1.56     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1272000, episode_reward=83.40 +/- 27.07\n",
      "Episode length: 6085.20 +/- 10456.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.09e+03 |\n",
      "|    mean_reward        | 83.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.934   |\n",
      "|    explained_variance | 0.843    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4968     |\n",
      "|    policy_loss        | -0.618   |\n",
      "|    value_loss         | 3.04     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1276000, episode_reward=91.80 +/- 33.84\n",
      "Episode length: 842.20 +/- 185.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 842      |\n",
      "|    mean_reward        | 91.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1276000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.905    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4984     |\n",
      "|    policy_loss        | -0.151   |\n",
      "|    value_loss         | 1.81     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=77.00 +/- 34.01\n",
      "Episode length: 788.00 +/- 90.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 788      |\n",
      "|    mean_reward        | 77       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | -0.0463  |\n",
      "|    value_loss         | 0.564    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 999      |\n",
      "|    ep_rew_mean     | 80.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 57       |\n",
      "|    iterations      | 5000     |\n",
      "|    time_elapsed    | 22080    |\n",
      "|    total_timesteps | 1280000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1284000, episode_reward=46.00 +/- 18.06\n",
      "Episode length: 758.40 +/- 80.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 758      |\n",
      "|    mean_reward        | 46       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1284000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.631    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5015     |\n",
      "|    policy_loss        | -0.0191  |\n",
      "|    value_loss         | 3.41     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1288000, episode_reward=72.40 +/- 29.77\n",
      "Episode length: 885.60 +/- 174.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 886      |\n",
      "|    mean_reward        | 72.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5031     |\n",
      "|    policy_loss        | 0.0514   |\n",
      "|    value_loss         | 2.49     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1292000, episode_reward=61.00 +/- 37.70\n",
      "Episode length: 677.20 +/- 228.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 677      |\n",
      "|    mean_reward        | 61       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1292000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.795    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5046     |\n",
      "|    policy_loss        | 0.48     |\n",
      "|    value_loss         | 1.07     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1296000, episode_reward=59.40 +/- 11.00\n",
      "Episode length: 747.20 +/- 138.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 747      |\n",
      "|    mean_reward        | 59.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.719    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5062     |\n",
      "|    policy_loss        | 0.713    |\n",
      "|    value_loss         | 5.98     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=36.60 +/- 7.14\n",
      "Episode length: 6101.20 +/- 10447.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.1e+03  |\n",
      "|    mean_reward        | 36.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5078     |\n",
      "|    policy_loss        | 0.474    |\n",
      "|    value_loss         | 1.11     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1304000, episode_reward=50.40 +/- 21.51\n",
      "Episode length: 5915.20 +/- 10540.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.92e+03 |\n",
      "|    mean_reward        | 50.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.531    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5093     |\n",
      "|    policy_loss        | 0.931    |\n",
      "|    value_loss         | 1.68     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01e+03 |\n",
      "|    ep_rew_mean        | 77.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 57       |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 22810    |\n",
      "|    total_timesteps    | 1305600  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.802    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | 0.45     |\n",
      "|    value_loss         | 1.62     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1308000, episode_reward=80.40 +/- 59.89\n",
      "Episode length: 668.00 +/- 155.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 668      |\n",
      "|    mean_reward        | 80.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1308000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.154    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5109     |\n",
      "|    policy_loss        | 3.33     |\n",
      "|    value_loss         | 35.4     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1312000, episode_reward=21.00 +/- 4.60\n",
      "Episode length: 451.60 +/- 105.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 452      |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5124     |\n",
      "|    policy_loss        | -0.607   |\n",
      "|    value_loss         | 0.559    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1316000, episode_reward=29.20 +/- 12.80\n",
      "Episode length: 529.60 +/- 98.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 530      |\n",
      "|    mean_reward        | 29.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1316000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5140     |\n",
      "|    policy_loss        | -0.203   |\n",
      "|    value_loss         | 0.422    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=69.80 +/- 16.68\n",
      "Episode length: 4824.40 +/- 6224.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.82e+03 |\n",
      "|    mean_reward        | 69.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.499    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5156     |\n",
      "|    policy_loss        | -0.104   |\n",
      "|    value_loss         | 0.852    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1324000, episode_reward=97.20 +/- 26.29\n",
      "Episode length: 7661.80 +/- 10115.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.66e+03 |\n",
      "|    mean_reward        | 97.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1324000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5171     |\n",
      "|    policy_loss        | 0.0402   |\n",
      "|    value_loss         | 0.811    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1328000, episode_reward=100.60 +/- 26.70\n",
      "Episode length: 954.80 +/- 172.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 955      |\n",
      "|    mean_reward        | 101      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.0125   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5187     |\n",
      "|    policy_loss        | 13.2     |\n",
      "|    value_loss         | 698      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.16e+03 |\n",
      "|    ep_rew_mean        | 70.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 23538    |\n",
      "|    total_timesteps    | 1331200  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | -0.291   |\n",
      "|    value_loss         | 0.716    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1332000, episode_reward=104.20 +/- 41.06\n",
      "Episode length: 5992.00 +/- 10503.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.99e+03 |\n",
      "|    mean_reward        | 104      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1332000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5203     |\n",
      "|    policy_loss        | -0.365   |\n",
      "|    value_loss         | 0.866    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1336000, episode_reward=71.20 +/- 20.60\n",
      "Episode length: 656.40 +/- 137.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 656      |\n",
      "|    mean_reward        | 71.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.381    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5218     |\n",
      "|    policy_loss        | 0.324    |\n",
      "|    value_loss         | 11.6     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=42.20 +/- 19.01\n",
      "Episode length: 572.20 +/- 226.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 572      |\n",
      "|    mean_reward        | 42.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5234     |\n",
      "|    policy_loss        | 0.263    |\n",
      "|    value_loss         | 0.543    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1344000, episode_reward=96.20 +/- 38.51\n",
      "Episode length: 807.00 +/- 218.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 807      |\n",
      "|    mean_reward        | 96.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5249     |\n",
      "|    policy_loss        | 0.315    |\n",
      "|    value_loss         | 0.847    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1348000, episode_reward=29.60 +/- 18.02\n",
      "Episode length: 459.60 +/- 145.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 460      |\n",
      "|    mean_reward        | 29.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1348000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.69     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5265     |\n",
      "|    policy_loss        | -0.00667 |\n",
      "|    value_loss         | 0.86     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1352000, episode_reward=52.20 +/- 3.87\n",
      "Episode length: 692.80 +/- 88.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 693      |\n",
      "|    mean_reward        | 52.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | -0.0214  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5281     |\n",
      "|    policy_loss        | 10.8     |\n",
      "|    value_loss         | 706      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1356000, episode_reward=142.40 +/- 107.30\n",
      "Episode length: 883.00 +/- 171.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 883      |\n",
      "|    mean_reward        | 142      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1356000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0.537    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5296     |\n",
      "|    policy_loss        | 0.559    |\n",
      "|    value_loss         | 3.21     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08e+03 |\n",
      "|    ep_rew_mean        | 71.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 24120    |\n",
      "|    total_timesteps    | 1356800  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | -0.315   |\n",
      "|    value_loss         | 3.32     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=98.20 +/- 116.92\n",
      "Episode length: 625.80 +/- 95.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 626      |\n",
      "|    mean_reward        | 98.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.891    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5312     |\n",
      "|    policy_loss        | -0.911   |\n",
      "|    value_loss         | 1.51     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1364000, episode_reward=96.60 +/- 49.88\n",
      "Episode length: 765.00 +/- 118.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 765      |\n",
      "|    mean_reward        | 96.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1364000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.514    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5328     |\n",
      "|    policy_loss        | 1.12     |\n",
      "|    value_loss         | 3.74     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1368000, episode_reward=76.80 +/- 61.42\n",
      "Episode length: 601.00 +/- 122.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 601      |\n",
      "|    mean_reward        | 76.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | 0.871    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5343     |\n",
      "|    policy_loss        | -0.268   |\n",
      "|    value_loss         | 1.91     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1372000, episode_reward=113.00 +/- 75.75\n",
      "Episode length: 985.60 +/- 51.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 986      |\n",
      "|    mean_reward        | 113      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1372000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.2     |\n",
      "|    explained_variance | 0.62     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5359     |\n",
      "|    policy_loss        | -0.927   |\n",
      "|    value_loss         | 1.46     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1376000, episode_reward=74.40 +/- 23.95\n",
      "Episode length: 782.00 +/- 110.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 782      |\n",
      "|    mean_reward        | 74.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.409    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5374     |\n",
      "|    policy_loss        | 0.581    |\n",
      "|    value_loss         | 19.6     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=67.00 +/- 19.47\n",
      "Episode length: 768.40 +/- 123.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 768      |\n",
      "|    mean_reward        | 67       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.358    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5390     |\n",
      "|    policy_loss        | 2.82     |\n",
      "|    value_loss         | 28.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05e+03 |\n",
      "|    ep_rew_mean        | 77.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 24539    |\n",
      "|    total_timesteps    | 1382400  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.69     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | 0.934    |\n",
      "|    value_loss         | 7.02     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1384000, episode_reward=44.40 +/- 10.78\n",
      "Episode length: 838.80 +/- 77.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 839      |\n",
      "|    mean_reward        | 44.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.743    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5406     |\n",
      "|    policy_loss        | -0.0706  |\n",
      "|    value_loss         | 1.82     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1388000, episode_reward=37.60 +/- 5.95\n",
      "Episode length: 792.00 +/- 132.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 792      |\n",
      "|    mean_reward        | 37.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1388000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.681    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5421     |\n",
      "|    policy_loss        | 0.212    |\n",
      "|    value_loss         | 2.81     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1392000, episode_reward=89.00 +/- 77.59\n",
      "Episode length: 927.20 +/- 163.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 927      |\n",
      "|    mean_reward        | 89       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.841    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5437     |\n",
      "|    policy_loss        | 0.935    |\n",
      "|    value_loss         | 3        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1396000, episode_reward=47.00 +/- 11.68\n",
      "Episode length: 709.20 +/- 135.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 709      |\n",
      "|    mean_reward        | 47       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1396000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.321    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5453     |\n",
      "|    policy_loss        | 0.0556   |\n",
      "|    value_loss         | 1.04     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1400000, episode_reward=119.80 +/- 50.10\n",
      "Episode length: 915.40 +/- 118.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 915      |\n",
      "|    mean_reward        | 120      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5468     |\n",
      "|    policy_loss        | -0.778   |\n",
      "|    value_loss         | 0.698    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1404000, episode_reward=71.40 +/- 23.95\n",
      "Episode length: 902.40 +/- 81.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 902      |\n",
      "|    mean_reward        | 71.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1404000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.861    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5484     |\n",
      "|    policy_loss        | 0.104    |\n",
      "|    value_loss         | 3.57     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1408000, episode_reward=109.60 +/- 55.92\n",
      "Episode length: 7860.40 +/- 10240.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.86e+03 |\n",
      "|    mean_reward        | 110      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | -0.0581  |\n",
      "|    value_loss         | 1.97     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 87.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 55       |\n",
      "|    iterations      | 5500     |\n",
      "|    time_elapsed    | 25199    |\n",
      "|    total_timesteps | 1408000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1412000, episode_reward=92.60 +/- 19.81\n",
      "Episode length: 828.00 +/- 77.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 828      |\n",
      "|    mean_reward        | 92.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1412000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.756    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5515     |\n",
      "|    policy_loss        | -0.777   |\n",
      "|    value_loss         | 1.53     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1416000, episode_reward=158.60 +/- 135.84\n",
      "Episode length: 11447.20 +/- 12697.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+04 |\n",
      "|    mean_reward        | 159      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5531     |\n",
      "|    policy_loss        | -0.361   |\n",
      "|    value_loss         | 1.74     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1420000, episode_reward=22.20 +/- 6.55\n",
      "Episode length: 522.80 +/- 63.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 523      |\n",
      "|    mean_reward        | 22.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5546     |\n",
      "|    policy_loss        | -0.943   |\n",
      "|    value_loss         | 1.57     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1424000, episode_reward=81.60 +/- 27.22\n",
      "Episode length: 9775.00 +/- 11251.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 9.78e+03 |\n",
      "|    mean_reward        | 81.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.819    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5562     |\n",
      "|    policy_loss        | -1.35    |\n",
      "|    value_loss         | 6.35     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1428000, episode_reward=54.60 +/- 47.81\n",
      "Episode length: 690.80 +/- 234.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 691      |\n",
      "|    mean_reward        | 54.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1428000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.405    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5578     |\n",
      "|    policy_loss        | 0.293    |\n",
      "|    value_loss         | 0.595    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1432000, episode_reward=65.80 +/- 21.43\n",
      "Episode length: 919.00 +/- 158.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 919      |\n",
      "|    mean_reward        | 65.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5593     |\n",
      "|    policy_loss        | 0.19     |\n",
      "|    value_loss         | 0.716    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 880      |\n",
      "|    ep_rew_mean        | 87.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 26182    |\n",
      "|    total_timesteps    | 1433600  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | -0.18    |\n",
      "|    value_loss         | 2.79     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1436000, episode_reward=77.20 +/- 35.21\n",
      "Episode length: 720.80 +/- 173.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 721      |\n",
      "|    mean_reward        | 77.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1436000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5609     |\n",
      "|    policy_loss        | 0.589    |\n",
      "|    value_loss         | 2.21     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=96.00 +/- 40.21\n",
      "Episode length: 698.80 +/- 168.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 699      |\n",
      "|    mean_reward        | 96       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.86     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5624     |\n",
      "|    policy_loss        | -0.0418  |\n",
      "|    value_loss         | 1.52     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1444000, episode_reward=63.80 +/- 8.82\n",
      "Episode length: 825.80 +/- 196.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 826      |\n",
      "|    mean_reward        | 63.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1444000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.823    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5640     |\n",
      "|    policy_loss        | -0.278   |\n",
      "|    value_loss         | 1.47     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1448000, episode_reward=56.20 +/- 7.98\n",
      "Episode length: 721.40 +/- 142.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 721      |\n",
      "|    mean_reward        | 56.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 0.714    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5656     |\n",
      "|    policy_loss        | 0.0702   |\n",
      "|    value_loss         | 1.62     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1452000, episode_reward=126.80 +/- 116.60\n",
      "Episode length: 789.00 +/- 161.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 789      |\n",
      "|    mean_reward        | 127      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1452000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5671     |\n",
      "|    policy_loss        | 0.263    |\n",
      "|    value_loss         | 1.78     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1456000, episode_reward=68.60 +/- 29.57\n",
      "Episode length: 767.60 +/- 82.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 768      |\n",
      "|    mean_reward        | 68.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.762    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5687     |\n",
      "|    policy_loss        | 0.304    |\n",
      "|    value_loss         | 1.44     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 914      |\n",
      "|    ep_rew_mean        | 89.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 26601    |\n",
      "|    total_timesteps    | 1459200  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | 0.123    |\n",
      "|    value_loss         | 0.574    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=33.60 +/- 14.11\n",
      "Episode length: 584.60 +/- 119.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 585      |\n",
      "|    mean_reward        | 33.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.791    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5703     |\n",
      "|    policy_loss        | 0.729    |\n",
      "|    value_loss         | 1.06     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1464000, episode_reward=64.40 +/- 15.32\n",
      "Episode length: 810.20 +/- 211.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 810      |\n",
      "|    mean_reward        | 64.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.757    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5718     |\n",
      "|    policy_loss        | 0.464    |\n",
      "|    value_loss         | 5.06     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1468000, episode_reward=118.80 +/- 58.38\n",
      "Episode length: 2720.80 +/- 3770.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.72e+03 |\n",
      "|    mean_reward        | 119      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1468000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.657    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5734     |\n",
      "|    policy_loss        | 0.525    |\n",
      "|    value_loss         | 34.3     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1472000, episode_reward=77.80 +/- 31.76\n",
      "Episode length: 11239.40 +/- 12865.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 77.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5749     |\n",
      "|    policy_loss        | -0.468   |\n",
      "|    value_loss         | 0.606    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1476000, episode_reward=88.40 +/- 52.18\n",
      "Episode length: 951.60 +/- 238.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 952      |\n",
      "|    mean_reward        | 88.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1476000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.2     |\n",
      "|    explained_variance | 0.906    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5765     |\n",
      "|    policy_loss        | -0.195   |\n",
      "|    value_loss         | 1        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=109.00 +/- 31.07\n",
      "Episode length: 962.40 +/- 87.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 962      |\n",
      "|    mean_reward        | 109      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5781     |\n",
      "|    policy_loss        | -0.112   |\n",
      "|    value_loss         | 0.921    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1484000, episode_reward=21.60 +/- 5.89\n",
      "Episode length: 542.80 +/- 107.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 543      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1484000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5796     |\n",
      "|    policy_loss        | -0.0456  |\n",
      "|    value_loss         | 0.505    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 968      |\n",
      "|    ep_rew_mean        | 85.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 27409    |\n",
      "|    total_timesteps    | 1484800  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | -0.215   |\n",
      "|    value_loss         | 0.383    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1488000, episode_reward=142.00 +/- 76.31\n",
      "Episode length: 5609.60 +/- 9228.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.61e+03 |\n",
      "|    mean_reward        | 142      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.821   |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5812     |\n",
      "|    policy_loss        | -0.271   |\n",
      "|    value_loss         | 3.83     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1492000, episode_reward=249.40 +/- 133.86\n",
      "Episode length: 878.00 +/- 52.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 878      |\n",
      "|    mean_reward        | 249      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1492000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5828     |\n",
      "|    policy_loss        | -0.286   |\n",
      "|    value_loss         | 0.675    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1496000, episode_reward=52.60 +/- 35.65\n",
      "Episode length: 613.60 +/- 164.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 614      |\n",
      "|    mean_reward        | 52.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.228    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5843     |\n",
      "|    policy_loss        | 0.861    |\n",
      "|    value_loss         | 49.8     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=78.60 +/- 34.04\n",
      "Episode length: 5882.80 +/- 10556.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.88e+03 |\n",
      "|    mean_reward        | 78.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5859     |\n",
      "|    policy_loss        | -0.537   |\n",
      "|    value_loss         | 0.773    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1504000, episode_reward=94.20 +/- 29.90\n",
      "Episode length: 5478.80 +/- 8038.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.48e+03 |\n",
      "|    mean_reward        | 94.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 0.72     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5874     |\n",
      "|    policy_loss        | -0.473   |\n",
      "|    value_loss         | 1.44     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1508000, episode_reward=66.40 +/- 7.20\n",
      "Episode length: 765.20 +/- 96.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 765      |\n",
      "|    mean_reward        | 66.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1508000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.74     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5890     |\n",
      "|    policy_loss        | 0.71     |\n",
      "|    value_loss         | 4.54     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 963      |\n",
      "|    ep_rew_mean        | 90.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 5900     |\n",
      "|    time_elapsed       | 28253    |\n",
      "|    total_timesteps    | 1510400  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.62     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | -0.291   |\n",
      "|    value_loss         | 1.47     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1512000, episode_reward=141.20 +/- 114.00\n",
      "Episode length: 11229.60 +/- 12873.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 141      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5906     |\n",
      "|    policy_loss        | -0.152   |\n",
      "|    value_loss         | 1.5      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1516000, episode_reward=222.60 +/- 114.82\n",
      "Episode length: 829.20 +/- 53.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 829      |\n",
      "|    mean_reward        | 223      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1516000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5921     |\n",
      "|    policy_loss        | -0.0642  |\n",
      "|    value_loss         | 1.81     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1520000, episode_reward=72.40 +/- 25.83\n",
      "Episode length: 910.00 +/- 217.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 910      |\n",
      "|    mean_reward        | 72.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5937     |\n",
      "|    policy_loss        | -1.17    |\n",
      "|    value_loss         | 4        |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1524000, episode_reward=45.80 +/- 29.35\n",
      "Episode length: 1158.00 +/- 1195.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+03 |\n",
      "|    mean_reward        | 45.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1524000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.673    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5953     |\n",
      "|    policy_loss        | -0.924   |\n",
      "|    value_loss         | 2.09     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1528000, episode_reward=54.00 +/- 10.79\n",
      "Episode length: 6105.80 +/- 10445.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.11e+03 |\n",
      "|    mean_reward        | 54       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5968     |\n",
      "|    policy_loss        | -0.412   |\n",
      "|    value_loss         | 2.17     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1532000, episode_reward=53.20 +/- 56.59\n",
      "Episode length: 546.80 +/- 81.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 547      |\n",
      "|    mean_reward        | 53.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1532000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.863    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5984     |\n",
      "|    policy_loss        | 0.129    |\n",
      "|    value_loss         | 0.561    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1536000, episode_reward=82.60 +/- 55.36\n",
      "Episode length: 4143.60 +/- 6660.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.14e+03 |\n",
      "|    mean_reward        | 82.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.781    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | 0.152    |\n",
      "|    value_loss         | 0.531    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.05e+03 |\n",
      "|    ep_rew_mean     | 89.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 6000     |\n",
      "|    time_elapsed    | 29274    |\n",
      "|    total_timesteps | 1536000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1540000, episode_reward=126.00 +/- 55.79\n",
      "Episode length: 4148.40 +/- 6790.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.15e+03 |\n",
      "|    mean_reward        | 126      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.764    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6015     |\n",
      "|    policy_loss        | -0.343   |\n",
      "|    value_loss         | 3.85     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1544000, episode_reward=127.80 +/- 117.71\n",
      "Episode length: 700.20 +/- 114.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 700      |\n",
      "|    mean_reward        | 128      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6031     |\n",
      "|    policy_loss        | -0.48    |\n",
      "|    value_loss         | 0.483    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1548000, episode_reward=112.80 +/- 47.52\n",
      "Episode length: 735.60 +/- 87.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 736      |\n",
      "|    mean_reward        | 113      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1548000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.545    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6046     |\n",
      "|    policy_loss        | 2.02     |\n",
      "|    value_loss         | 43.9     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1552000, episode_reward=56.20 +/- 19.51\n",
      "Episode length: 822.00 +/- 233.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 822      |\n",
      "|    mean_reward        | 56.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6062     |\n",
      "|    policy_loss        | -0.782   |\n",
      "|    value_loss         | 0.925    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1556000, episode_reward=88.40 +/- 57.27\n",
      "Episode length: 776.00 +/- 114.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 776      |\n",
      "|    mean_reward        | 88.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1556000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6078     |\n",
      "|    policy_loss        | 0.131    |\n",
      "|    value_loss         | 0.432    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1560000, episode_reward=166.40 +/- 108.35\n",
      "Episode length: 1459.60 +/- 796.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.46e+03 |\n",
      "|    mean_reward        | 166      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6093     |\n",
      "|    policy_loss        | 0.699    |\n",
      "|    value_loss         | 2.16     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.16e+03 |\n",
      "|    ep_rew_mean        | 97.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 52       |\n",
      "|    iterations         | 6100     |\n",
      "|    time_elapsed       | 29815    |\n",
      "|    total_timesteps    | 1561600  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.804    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6099     |\n",
      "|    policy_loss        | 1.49     |\n",
      "|    value_loss         | 7.57     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1564000, episode_reward=84.60 +/- 17.23\n",
      "Episode length: 1004.80 +/- 537.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1e+03    |\n",
      "|    mean_reward        | 84.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1564000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6109     |\n",
      "|    policy_loss        | -0.641   |\n",
      "|    value_loss         | 0.961    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1568000, episode_reward=154.40 +/- 52.25\n",
      "Episode length: 3801.20 +/- 3929.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.8e+03  |\n",
      "|    mean_reward        | 154      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.989   |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6124     |\n",
      "|    policy_loss        | -0.444   |\n",
      "|    value_loss         | 2.73     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1572000, episode_reward=201.00 +/- 154.29\n",
      "Episode length: 6116.80 +/- 10440.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.12e+03 |\n",
      "|    mean_reward        | 201      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1572000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.854   |\n",
      "|    explained_variance | 0.9      |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6140     |\n",
      "|    policy_loss        | -0.52    |\n",
      "|    value_loss         | 7.08     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1576000, episode_reward=165.40 +/- 105.27\n",
      "Episode length: 908.20 +/- 129.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 908      |\n",
      "|    mean_reward        | 165      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6156     |\n",
      "|    policy_loss        | 0.882    |\n",
      "|    value_loss         | 4.26     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1580000, episode_reward=93.00 +/- 60.13\n",
      "Episode length: 2814.40 +/- 4173.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.81e+03 |\n",
      "|    mean_reward        | 93       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.818    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6171     |\n",
      "|    policy_loss        | -0.472   |\n",
      "|    value_loss         | 1.58     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1584000, episode_reward=49.20 +/- 16.34\n",
      "Episode length: 4404.80 +/- 4851.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.4e+03  |\n",
      "|    mean_reward        | 49.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6187     |\n",
      "|    policy_loss        | -0.18    |\n",
      "|    value_loss         | 1.34     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+03 |\n",
      "|    ep_rew_mean        | 112      |\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 30661    |\n",
      "|    total_timesteps    | 1587200  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.323    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | 4.6      |\n",
      "|    value_loss         | 127      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1588000, episode_reward=69.40 +/- 12.71\n",
      "Episode length: 712.80 +/- 85.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 713      |\n",
      "|    mean_reward        | 69.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1588000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.974   |\n",
      "|    explained_variance | 0.853    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6203     |\n",
      "|    policy_loss        | -0.336   |\n",
      "|    value_loss         | 2.23     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1592000, episode_reward=144.00 +/- 78.32\n",
      "Episode length: 926.00 +/- 162.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 926      |\n",
      "|    mean_reward        | 144      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1592000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6218     |\n",
      "|    policy_loss        | -0.502   |\n",
      "|    value_loss         | 1.55     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1596000, episode_reward=163.80 +/- 41.18\n",
      "Episode length: 785.80 +/- 101.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 786      |\n",
      "|    mean_reward        | 164      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1596000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.815    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6234     |\n",
      "|    policy_loss        | -0.647   |\n",
      "|    value_loss         | 0.548    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=205.80 +/- 241.32\n",
      "Episode length: 781.60 +/- 105.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 782      |\n",
      "|    mean_reward        | 206      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.788    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6249     |\n",
      "|    policy_loss        | 0.184    |\n",
      "|    value_loss         | 1.46     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1604000, episode_reward=135.00 +/- 86.84\n",
      "Episode length: 1457.20 +/- 1078.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.46e+03 |\n",
      "|    mean_reward        | 135      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1604000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.771    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6265     |\n",
      "|    policy_loss        | -0.901   |\n",
      "|    value_loss         | 5.24     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1608000, episode_reward=57.00 +/- 13.67\n",
      "Episode length: 595.80 +/- 70.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 596      |\n",
      "|    mean_reward        | 57       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6281     |\n",
      "|    policy_loss        | -0.0865  |\n",
      "|    value_loss         | 0.888    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1612000, episode_reward=157.80 +/- 109.33\n",
      "Episode length: 822.40 +/- 168.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 822      |\n",
      "|    mean_reward        | 158      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1612000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.608    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6296     |\n",
      "|    policy_loss        | 1.1      |\n",
      "|    value_loss         | 3.43     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+03 |\n",
      "|    ep_rew_mean        | 108      |\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 6300     |\n",
      "|    time_elapsed       | 31129    |\n",
      "|    total_timesteps    | 1612800  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.201    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | 4.01     |\n",
      "|    value_loss         | 161      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1616000, episode_reward=118.20 +/- 49.61\n",
      "Episode length: 1005.20 +/- 212.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+03 |\n",
      "|    mean_reward        | 118      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6312     |\n",
      "|    policy_loss        | 0.0553   |\n",
      "|    value_loss         | 2.77     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1620000, episode_reward=76.40 +/- 13.48\n",
      "Episode length: 803.00 +/- 179.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 803      |\n",
      "|    mean_reward        | 76.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6328     |\n",
      "|    policy_loss        | -1.21    |\n",
      "|    value_loss         | 2.89     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1624000, episode_reward=147.40 +/- 124.01\n",
      "Episode length: 1089.20 +/- 152.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.09e+03 |\n",
      "|    mean_reward        | 147      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6343     |\n",
      "|    policy_loss        | -1.01    |\n",
      "|    value_loss         | 2.12     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1628000, episode_reward=75.80 +/- 17.15\n",
      "Episode length: 687.60 +/- 158.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 688      |\n",
      "|    mean_reward        | 75.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1628000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.749    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6359     |\n",
      "|    policy_loss        | 1.01     |\n",
      "|    value_loss         | 7.44     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1632000, episode_reward=118.40 +/- 31.40\n",
      "Episode length: 1170.80 +/- 148.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.17e+03 |\n",
      "|    mean_reward        | 118      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1632000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.0844   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6374     |\n",
      "|    policy_loss        | 10.2     |\n",
      "|    value_loss         | 3.64e+03 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1636000, episode_reward=23.80 +/- 4.79\n",
      "Episode length: 557.20 +/- 57.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 557      |\n",
      "|    mean_reward        | 23.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1636000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.547    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6390     |\n",
      "|    policy_loss        | -0.485   |\n",
      "|    value_loss         | 0.57     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.22e+03 |\n",
      "|    ep_rew_mean        | 115      |\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 31574    |\n",
      "|    total_timesteps    | 1638400  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.405    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | 1.02     |\n",
      "|    value_loss         | 46.4     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1640000, episode_reward=131.40 +/- 25.87\n",
      "Episode length: 1048.20 +/- 82.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+03 |\n",
      "|    mean_reward        | 131      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.606    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6406     |\n",
      "|    policy_loss        | 0.153    |\n",
      "|    value_loss         | 5.02     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1644000, episode_reward=110.20 +/- 48.66\n",
      "Episode length: 765.60 +/- 147.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 766      |\n",
      "|    mean_reward        | 110      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1644000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.928   |\n",
      "|    explained_variance | 0.795    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6421     |\n",
      "|    policy_loss        | -0.846   |\n",
      "|    value_loss         | 8.15     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1648000, episode_reward=149.80 +/- 162.31\n",
      "Episode length: 848.80 +/- 200.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 849      |\n",
      "|    mean_reward        | 150      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.753    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6437     |\n",
      "|    policy_loss        | 0.394    |\n",
      "|    value_loss         | 2.89     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1652000, episode_reward=106.40 +/- 12.19\n",
      "Episode length: 1135.20 +/- 307.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+03 |\n",
      "|    mean_reward        | 106      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1652000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.653    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6453     |\n",
      "|    policy_loss        | -0.144   |\n",
      "|    value_loss         | 3.15     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1656000, episode_reward=70.00 +/- 24.40\n",
      "Episode length: 700.00 +/- 67.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 700      |\n",
      "|    mean_reward        | 70       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.825    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6468     |\n",
      "|    policy_loss        | -0.418   |\n",
      "|    value_loss         | 2.18     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1660000, episode_reward=97.00 +/- 43.99\n",
      "Episode length: 846.00 +/- 232.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 846      |\n",
      "|    mean_reward        | 97       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.96    |\n",
      "|    explained_variance | 0.754    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6484     |\n",
      "|    policy_loss        | 0.385    |\n",
      "|    value_loss         | 1.62     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1664000, episode_reward=96.20 +/- 26.54\n",
      "Episode length: 901.20 +/- 172.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 901      |\n",
      "|    mean_reward        | 96.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.822    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | 0.47     |\n",
      "|    value_loss         | 3.24     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.05e+03 |\n",
      "|    ep_rew_mean     | 125      |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 6500     |\n",
      "|    time_elapsed    | 32048    |\n",
      "|    total_timesteps | 1664000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1668000, episode_reward=90.40 +/- 28.02\n",
      "Episode length: 6002.40 +/- 10497.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6e+03    |\n",
      "|    mean_reward        | 90.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1668000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.2     |\n",
      "|    explained_variance | 0.414    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6515     |\n",
      "|    policy_loss        | 0.465    |\n",
      "|    value_loss         | 6.96     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1672000, episode_reward=76.60 +/- 29.38\n",
      "Episode length: 751.40 +/- 222.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 751      |\n",
      "|    mean_reward        | 76.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1672000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.775   |\n",
      "|    explained_variance | 0.749    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6531     |\n",
      "|    policy_loss        | 0.0932   |\n",
      "|    value_loss         | 33.1     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1676000, episode_reward=120.40 +/- 65.13\n",
      "Episode length: 3576.80 +/- 5477.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.58e+03 |\n",
      "|    mean_reward        | 120      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1676000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.835   |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6546     |\n",
      "|    policy_loss        | -0.521   |\n",
      "|    value_loss         | 3.05     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1680000, episode_reward=23.60 +/- 4.18\n",
      "Episode length: 457.40 +/- 49.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 457      |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | -0.0734  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6562     |\n",
      "|    policy_loss        | 0.126    |\n",
      "|    value_loss         | 1.62     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1684000, episode_reward=100.40 +/- 60.30\n",
      "Episode length: 765.00 +/- 150.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 765      |\n",
      "|    mean_reward        | 100      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1684000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0.531    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6578     |\n",
      "|    policy_loss        | 0.975    |\n",
      "|    value_loss         | 2.91     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1688000, episode_reward=161.80 +/- 45.19\n",
      "Episode length: 967.40 +/- 156.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 967      |\n",
      "|    mean_reward        | 162      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.972   |\n",
      "|    explained_variance | 0.76     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6593     |\n",
      "|    policy_loss        | -0.00447 |\n",
      "|    value_loss         | 2.6      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 958      |\n",
      "|    ep_rew_mean        | 114      |\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 6600     |\n",
      "|    time_elapsed       | 32711    |\n",
      "|    total_timesteps    | 1689600  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.457    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | 0.244    |\n",
      "|    value_loss         | 23.7     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1692000, episode_reward=151.40 +/- 51.01\n",
      "Episode length: 972.60 +/- 210.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 973      |\n",
      "|    mean_reward        | 151      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1692000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.712   |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6609     |\n",
      "|    policy_loss        | -0.229   |\n",
      "|    value_loss         | 2.33     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1696000, episode_reward=149.20 +/- 72.24\n",
      "Episode length: 6067.20 +/- 10465.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.07e+03 |\n",
      "|    mean_reward        | 149      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6624     |\n",
      "|    policy_loss        | 0.0681   |\n",
      "|    value_loss         | 0.959    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1700000, episode_reward=117.40 +/- 28.87\n",
      "Episode length: 850.60 +/- 101.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 851      |\n",
      "|    mean_reward        | 117      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.89    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6640     |\n",
      "|    policy_loss        | 0.0679   |\n",
      "|    value_loss         | 2.03     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1704000, episode_reward=100.60 +/- 25.55\n",
      "Episode length: 894.00 +/- 154.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 894      |\n",
      "|    mean_reward        | 101      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1704000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6656     |\n",
      "|    policy_loss        | 0.0999   |\n",
      "|    value_loss         | 1.87     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1708000, episode_reward=53.60 +/- 9.75\n",
      "Episode length: 836.00 +/- 112.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 836      |\n",
      "|    mean_reward        | 53.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1708000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.776    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6671     |\n",
      "|    policy_loss        | -1.35    |\n",
      "|    value_loss         | 3.96     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1712000, episode_reward=28.00 +/- 20.69\n",
      "Episode length: 482.40 +/- 168.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 482      |\n",
      "|    mean_reward        | 28       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.891    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6687     |\n",
      "|    policy_loss        | -0.736   |\n",
      "|    value_loss         | 0.797    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 878      |\n",
      "|    ep_rew_mean        | 112      |\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 33298    |\n",
      "|    total_timesteps    | 1715200  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.751   |\n",
      "|    explained_variance | 0.758    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | -0.484   |\n",
      "|    value_loss         | 3.25     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1716000, episode_reward=98.40 +/- 62.12\n",
      "Episode length: 829.20 +/- 168.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 829      |\n",
      "|    mean_reward        | 98.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1716000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | 0.665    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6703     |\n",
      "|    policy_loss        | 0.419    |\n",
      "|    value_loss         | 3.2      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1720000, episode_reward=34.20 +/- 12.70\n",
      "Episode length: 647.20 +/- 120.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 647      |\n",
      "|    mean_reward        | 34.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.855    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6718     |\n",
      "|    policy_loss        | 0.0742   |\n",
      "|    value_loss         | 1.44     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1724000, episode_reward=126.00 +/- 48.29\n",
      "Episode length: 869.20 +/- 212.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 869      |\n",
      "|    mean_reward        | 126      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1724000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.77     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6734     |\n",
      "|    policy_loss        | -0.166   |\n",
      "|    value_loss         | 3.28     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1728000, episode_reward=60.20 +/- 16.17\n",
      "Episode length: 715.20 +/- 100.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 715      |\n",
      "|    mean_reward        | 60.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.295    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6749     |\n",
      "|    policy_loss        | 1.43     |\n",
      "|    value_loss         | 23       |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1732000, episode_reward=118.20 +/- 28.97\n",
      "Episode length: 971.20 +/- 172.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 971      |\n",
      "|    mean_reward        | 118      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1732000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.909   |\n",
      "|    explained_variance | 0.563    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6765     |\n",
      "|    policy_loss        | 0.336    |\n",
      "|    value_loss         | 35.3     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1736000, episode_reward=137.20 +/- 57.84\n",
      "Episode length: 1003.20 +/- 107.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1e+03    |\n",
      "|    mean_reward        | 137      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6781     |\n",
      "|    policy_loss        | -0.338   |\n",
      "|    value_loss         | 2.17     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1740000, episode_reward=209.00 +/- 88.19\n",
      "Episode length: 959.80 +/- 174.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 960      |\n",
      "|    mean_reward        | 209      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6796     |\n",
      "|    policy_loss        | -0.393   |\n",
      "|    value_loss         | 1.28     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 873      |\n",
      "|    ep_rew_mean        | 108      |\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 6800     |\n",
      "|    time_elapsed       | 33762    |\n",
      "|    total_timesteps    | 1740800  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.836    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6799     |\n",
      "|    policy_loss        | 0.887    |\n",
      "|    value_loss         | 7.53     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1744000, episode_reward=245.60 +/- 60.85\n",
      "Episode length: 1247.60 +/- 247.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.25e+03 |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6812     |\n",
      "|    policy_loss        | -0.354   |\n",
      "|    value_loss         | 2.06     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1748000, episode_reward=153.60 +/- 92.35\n",
      "Episode length: 4387.20 +/- 6960.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.39e+03 |\n",
      "|    mean_reward        | 154      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1748000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.731    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6828     |\n",
      "|    policy_loss        | -0.434   |\n",
      "|    value_loss         | 2.38     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1752000, episode_reward=168.40 +/- 61.20\n",
      "Episode length: 2303.60 +/- 1491.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.3e+03  |\n",
      "|    mean_reward        | 168      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6843     |\n",
      "|    policy_loss        | 0.0995   |\n",
      "|    value_loss         | 0.363    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1756000, episode_reward=136.40 +/- 44.18\n",
      "Episode length: 6230.80 +/- 10384.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.23e+03 |\n",
      "|    mean_reward        | 136      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1756000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.71     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6859     |\n",
      "|    policy_loss        | 0.666    |\n",
      "|    value_loss         | 15.2     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=114.60 +/- 24.78\n",
      "Episode length: 1030.00 +/- 183.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+03 |\n",
      "|    mean_reward        | 115      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.255    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6874     |\n",
      "|    policy_loss        | 2.63     |\n",
      "|    value_loss         | 318      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1764000, episode_reward=192.60 +/- 160.74\n",
      "Episode length: 1058.40 +/- 185.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+03 |\n",
      "|    mean_reward        | 193      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1764000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6890     |\n",
      "|    policy_loss        | 0.174    |\n",
      "|    value_loss         | 1.31     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 916      |\n",
      "|    ep_rew_mean        | 113      |\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 6900     |\n",
      "|    time_elapsed       | 34528    |\n",
      "|    total_timesteps    | 1766400  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.986   |\n",
      "|    explained_variance | 0.328    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | 0.722    |\n",
      "|    value_loss         | 194      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1768000, episode_reward=124.60 +/- 30.12\n",
      "Episode length: 925.00 +/- 196.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 925      |\n",
      "|    mean_reward        | 125      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6906     |\n",
      "|    policy_loss        | -0.123   |\n",
      "|    value_loss         | 3.77     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1772000, episode_reward=238.00 +/- 164.28\n",
      "Episode length: 1130.00 +/- 224.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+03 |\n",
      "|    mean_reward        | 238      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1772000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6921     |\n",
      "|    policy_loss        | -0.116   |\n",
      "|    value_loss         | 0.484    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1776000, episode_reward=116.00 +/- 19.36\n",
      "Episode length: 1756.80 +/- 1433.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.76e+03 |\n",
      "|    mean_reward        | 116      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6937     |\n",
      "|    policy_loss        | 0.442    |\n",
      "|    value_loss         | 0.536    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1780000, episode_reward=75.60 +/- 8.91\n",
      "Episode length: 823.60 +/- 158.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 824      |\n",
      "|    mean_reward        | 75.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.207    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6953     |\n",
      "|    policy_loss        | 1.25     |\n",
      "|    value_loss         | 3.91     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1784000, episode_reward=142.80 +/- 117.06\n",
      "Episode length: 713.40 +/- 191.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 713      |\n",
      "|    mean_reward        | 143      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6968     |\n",
      "|    policy_loss        | -0.241   |\n",
      "|    value_loss         | 1.07     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1788000, episode_reward=118.20 +/- 29.31\n",
      "Episode length: 924.00 +/- 88.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 924      |\n",
      "|    mean_reward        | 118      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1788000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6984     |\n",
      "|    policy_loss        | -0.177   |\n",
      "|    value_loss         | 0.561    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1792000, episode_reward=239.80 +/- 151.37\n",
      "Episode length: 1043.60 +/- 205.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+03 |\n",
      "|    mean_reward        | 240      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | -0.174   |\n",
      "|    value_loss         | 0.148    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 925      |\n",
      "|    ep_rew_mean     | 121      |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 7000     |\n",
      "|    time_elapsed    | 35032    |\n",
      "|    total_timesteps | 1792000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1796000, episode_reward=151.80 +/- 57.41\n",
      "Episode length: 1082.80 +/- 233.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+03 |\n",
      "|    mean_reward        | 152      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1796000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7015     |\n",
      "|    policy_loss        | 0.257    |\n",
      "|    value_loss         | 1.81     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1800000, episode_reward=125.60 +/- 28.30\n",
      "Episode length: 1193.00 +/- 443.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.19e+03 |\n",
      "|    mean_reward        | 126      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.357    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7031     |\n",
      "|    policy_loss        | 2.38     |\n",
      "|    value_loss         | 25.4     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1804000, episode_reward=166.00 +/- 64.21\n",
      "Episode length: 1076.20 +/- 98.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+03 |\n",
      "|    mean_reward        | 166      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1804000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.974   |\n",
      "|    explained_variance | 0.845    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7046     |\n",
      "|    policy_loss        | -1.68    |\n",
      "|    value_loss         | 13.2     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1808000, episode_reward=114.80 +/- 18.15\n",
      "Episode length: 1756.00 +/- 1720.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.76e+03 |\n",
      "|    mean_reward        | 115      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1808000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.2     |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7062     |\n",
      "|    policy_loss        | 0.141    |\n",
      "|    value_loss         | 1.85     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1812000, episode_reward=40.40 +/- 17.32\n",
      "Episode length: 645.20 +/- 246.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 645      |\n",
      "|    mean_reward        | 40.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1812000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7078     |\n",
      "|    policy_loss        | -0.174   |\n",
      "|    value_loss         | 0.27     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1816000, episode_reward=124.80 +/- 62.69\n",
      "Episode length: 939.40 +/- 313.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 939      |\n",
      "|    mean_reward        | 125      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.785   |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7093     |\n",
      "|    policy_loss        | -0.887   |\n",
      "|    value_loss         | 6.21     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.21e+03 |\n",
      "|    ep_rew_mean        | 118      |\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 7100     |\n",
      "|    time_elapsed       | 35516    |\n",
      "|    total_timesteps    | 1817600  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.762   |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7099     |\n",
      "|    policy_loss        | -0.595   |\n",
      "|    value_loss         | 2.05     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1820000, episode_reward=106.40 +/- 35.81\n",
      "Episode length: 831.80 +/- 154.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 832      |\n",
      "|    mean_reward        | 106      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7109     |\n",
      "|    policy_loss        | -0.141   |\n",
      "|    value_loss         | 2.36     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1824000, episode_reward=81.20 +/- 29.55\n",
      "Episode length: 804.00 +/- 137.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 804      |\n",
      "|    mean_reward        | 81.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7124     |\n",
      "|    policy_loss        | -0.374   |\n",
      "|    value_loss         | 2.59     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1828000, episode_reward=112.20 +/- 69.52\n",
      "Episode length: 751.80 +/- 155.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 752      |\n",
      "|    mean_reward        | 112      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1828000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7140     |\n",
      "|    policy_loss        | -1       |\n",
      "|    value_loss         | 3.54     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1832000, episode_reward=105.40 +/- 34.91\n",
      "Episode length: 937.40 +/- 326.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 937      |\n",
      "|    mean_reward        | 105      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.916   |\n",
      "|    explained_variance | 0.3      |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7156     |\n",
      "|    policy_loss        | 1.78     |\n",
      "|    value_loss         | 35.9     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1836000, episode_reward=207.20 +/- 170.26\n",
      "Episode length: 6265.60 +/- 10365.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.27e+03 |\n",
      "|    mean_reward        | 207      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1836000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.0568   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7171     |\n",
      "|    policy_loss        | 0.889    |\n",
      "|    value_loss         | 284      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1840000, episode_reward=159.60 +/- 71.38\n",
      "Episode length: 1042.40 +/- 120.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+03 |\n",
      "|    mean_reward        | 160      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7187     |\n",
      "|    policy_loss        | 0.385    |\n",
      "|    value_loss         | 2.44     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+03  |\n",
      "|    ep_rew_mean        | 137      |\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 7200     |\n",
      "|    time_elapsed       | 36115    |\n",
      "|    total_timesteps    | 1843200  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.975   |\n",
      "|    explained_variance | 0.903    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | -0.0237  |\n",
      "|    value_loss         | 4.28     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1844000, episode_reward=295.80 +/- 260.36\n",
      "Episode length: 1135.20 +/- 87.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+03 |\n",
      "|    mean_reward        | 296      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1844000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.821   |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7203     |\n",
      "|    policy_loss        | -0.493   |\n",
      "|    value_loss         | 3.36     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1848000, episode_reward=285.80 +/- 62.80\n",
      "Episode length: 1056.00 +/- 86.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+03 |\n",
      "|    mean_reward        | 286      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7218     |\n",
      "|    policy_loss        | -0.38    |\n",
      "|    value_loss         | 1.54     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1852000, episode_reward=129.40 +/- 32.81\n",
      "Episode length: 971.60 +/- 196.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 972      |\n",
      "|    mean_reward        | 129      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1852000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7234     |\n",
      "|    policy_loss        | 0.593    |\n",
      "|    value_loss         | 4.83     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1856000, episode_reward=146.20 +/- 45.11\n",
      "Episode length: 922.00 +/- 121.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 922      |\n",
      "|    mean_reward        | 146      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7249     |\n",
      "|    policy_loss        | 0.583    |\n",
      "|    value_loss         | 3.48     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1860000, episode_reward=368.60 +/- 66.13\n",
      "Episode length: 1068.00 +/- 142.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.07e+03 |\n",
      "|    mean_reward        | 369      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1860000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7265     |\n",
      "|    policy_loss        | -0.345   |\n",
      "|    value_loss         | 3.86     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1864000, episode_reward=249.60 +/- 134.19\n",
      "Episode length: 1070.00 +/- 107.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.07e+03 |\n",
      "|    mean_reward        | 250      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.602    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7281     |\n",
      "|    policy_loss        | -0.0235  |\n",
      "|    value_loss         | 87.1     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1868000, episode_reward=242.00 +/- 110.39\n",
      "Episode length: 3678.80 +/- 5229.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.68e+03 |\n",
      "|    mean_reward        | 242      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1868000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.788    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7296     |\n",
      "|    policy_loss        | 0.733    |\n",
      "|    value_loss         | 33.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.37e+03 |\n",
      "|    ep_rew_mean        | 152      |\n",
      "| time/                 |          |\n",
      "|    fps                | 50       |\n",
      "|    iterations         | 7300     |\n",
      "|    time_elapsed       | 36691    |\n",
      "|    total_timesteps    | 1868800  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.855    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7299     |\n",
      "|    policy_loss        | 0.176    |\n",
      "|    value_loss         | 1.78     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1872000, episode_reward=167.40 +/- 59.88\n",
      "Episode length: 1003.60 +/- 122.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1e+03    |\n",
      "|    mean_reward        | 167      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7312     |\n",
      "|    policy_loss        | -0.467   |\n",
      "|    value_loss         | 3.76     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1876000, episode_reward=184.20 +/- 107.28\n",
      "Episode length: 791.80 +/- 231.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 792      |\n",
      "|    mean_reward        | 184      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1876000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7328     |\n",
      "|    policy_loss        | -0.607   |\n",
      "|    value_loss         | 1.93     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1880000, episode_reward=237.40 +/- 109.03\n",
      "Episode length: 895.20 +/- 174.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 895      |\n",
      "|    mean_reward        | 237      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7343     |\n",
      "|    policy_loss        | 0.308    |\n",
      "|    value_loss         | 1.83     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1884000, episode_reward=131.40 +/- 56.01\n",
      "Episode length: 957.60 +/- 159.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 958      |\n",
      "|    mean_reward        | 131      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1884000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7359     |\n",
      "|    policy_loss        | -0.471   |\n",
      "|    value_loss         | 1.96     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1888000, episode_reward=54.60 +/- 32.80\n",
      "Episode length: 615.80 +/- 158.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 616      |\n",
      "|    mean_reward        | 54.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.16    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7374     |\n",
      "|    policy_loss        | -0.124   |\n",
      "|    value_loss         | 2.24     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1892000, episode_reward=234.80 +/- 105.59\n",
      "Episode length: 1044.00 +/- 140.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+03 |\n",
      "|    mean_reward        | 235      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1892000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7390     |\n",
      "|    policy_loss        | 0.371    |\n",
      "|    value_loss         | 3.48     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.42e+03 |\n",
      "|    ep_rew_mean        | 159      |\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 37134    |\n",
      "|    total_timesteps    | 1894400  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | -0.491   |\n",
      "|    value_loss         | 0.658    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1896000, episode_reward=326.60 +/- 123.37\n",
      "Episode length: 875.00 +/- 120.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 875      |\n",
      "|    mean_reward        | 327      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.19    |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7406     |\n",
      "|    policy_loss        | -0.0351  |\n",
      "|    value_loss         | 0.233    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1900000, episode_reward=229.00 +/- 185.89\n",
      "Episode length: 894.80 +/- 71.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 895      |\n",
      "|    mean_reward        | 229      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7421     |\n",
      "|    policy_loss        | -0.144   |\n",
      "|    value_loss         | 0.911    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1904000, episode_reward=106.20 +/- 22.85\n",
      "Episode length: 880.20 +/- 167.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 880      |\n",
      "|    mean_reward        | 106      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.775    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7437     |\n",
      "|    policy_loss        | -0.254   |\n",
      "|    value_loss         | 3.33     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1908000, episode_reward=128.00 +/- 63.03\n",
      "Episode length: 859.20 +/- 137.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 859      |\n",
      "|    mean_reward        | 128      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1908000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7453     |\n",
      "|    policy_loss        | -0.668   |\n",
      "|    value_loss         | 1.54     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1912000, episode_reward=176.40 +/- 58.98\n",
      "Episode length: 1911.00 +/- 2181.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.91e+03 |\n",
      "|    mean_reward        | 176      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7468     |\n",
      "|    policy_loss        | 0.729    |\n",
      "|    value_loss         | 3.98     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1916000, episode_reward=96.40 +/- 3.56\n",
      "Episode length: 842.20 +/- 81.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 842      |\n",
      "|    mean_reward        | 96.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1916000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.489    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7484     |\n",
      "|    policy_loss        | 0.984    |\n",
      "|    value_loss         | 226      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=177.40 +/- 77.86\n",
      "Episode length: 827.20 +/- 166.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 827      |\n",
      "|    mean_reward        | 177      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.819    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7499     |\n",
      "|    policy_loss        | 0.863    |\n",
      "|    value_loss         | 24       |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.39e+03 |\n",
      "|    ep_rew_mean     | 149      |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 7500     |\n",
      "|    time_elapsed    | 37627    |\n",
      "|    total_timesteps | 1920000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1924000, episode_reward=203.00 +/- 52.36\n",
      "Episode length: 918.40 +/- 163.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 918      |\n",
      "|    mean_reward        | 203      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1924000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.864    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7515     |\n",
      "|    policy_loss        | 0.373    |\n",
      "|    value_loss         | 8.39     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1928000, episode_reward=164.40 +/- 37.82\n",
      "Episode length: 1015.00 +/- 102.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+03 |\n",
      "|    mean_reward        | 164      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.917   |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7531     |\n",
      "|    policy_loss        | 0.291    |\n",
      "|    value_loss         | 8.16     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1932000, episode_reward=181.20 +/- 72.91\n",
      "Episode length: 1078.40 +/- 171.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+03 |\n",
      "|    mean_reward        | 181      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1932000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7546     |\n",
      "|    policy_loss        | -0.291   |\n",
      "|    value_loss         | 3.2      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1936000, episode_reward=156.80 +/- 62.02\n",
      "Episode length: 1156.40 +/- 566.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+03 |\n",
      "|    mean_reward        | 157      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7562     |\n",
      "|    policy_loss        | -0.311   |\n",
      "|    value_loss         | 1.34     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1940000, episode_reward=219.80 +/- 67.57\n",
      "Episode length: 1010.80 +/- 57.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+03 |\n",
      "|    mean_reward        | 220      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.354    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7578     |\n",
      "|    policy_loss        | 3.55     |\n",
      "|    value_loss         | 150      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1944000, episode_reward=138.80 +/- 29.71\n",
      "Episode length: 1060.80 +/- 106.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+03 |\n",
      "|    mean_reward        | 139      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7593     |\n",
      "|    policy_loss        | 0.156    |\n",
      "|    value_loss         | 5.52     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.27e+03 |\n",
      "|    ep_rew_mean        | 159      |\n",
      "| time/                 |          |\n",
      "|    fps                | 50       |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 38662    |\n",
      "|    total_timesteps    | 1945600  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.0409   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | 7.69     |\n",
      "|    value_loss         | 737      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1948000, episode_reward=245.20 +/- 157.86\n",
      "Episode length: 947.00 +/- 188.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 947      |\n",
      "|    mean_reward        | 245      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1948000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.803   |\n",
      "|    explained_variance | 0.391    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7609     |\n",
      "|    policy_loss        | 0.496    |\n",
      "|    value_loss         | 362      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1952000, episode_reward=196.60 +/- 86.38\n",
      "Episode length: 1118.00 +/- 102.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+03 |\n",
      "|    mean_reward        | 197      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.844   |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7624     |\n",
      "|    policy_loss        | -0.892   |\n",
      "|    value_loss         | 7.34     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1956000, episode_reward=119.80 +/- 14.82\n",
      "Episode length: 1185.20 +/- 411.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.19e+03 |\n",
      "|    mean_reward        | 120      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1956000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7640     |\n",
      "|    policy_loss        | -0.574   |\n",
      "|    value_loss         | 5.14     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1960000, episode_reward=248.80 +/- 82.32\n",
      "Episode length: 1046.40 +/- 66.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+03 |\n",
      "|    mean_reward        | 249      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7656     |\n",
      "|    policy_loss        | -0.162   |\n",
      "|    value_loss         | 5.02     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1964000, episode_reward=127.20 +/- 52.79\n",
      "Episode length: 6052.80 +/- 10471.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.05e+03 |\n",
      "|    mean_reward        | 127      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1964000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.752   |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7671     |\n",
      "|    policy_loss        | -0.531   |\n",
      "|    value_loss         | 3.86     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1968000, episode_reward=57.40 +/- 27.84\n",
      "Episode length: 739.20 +/- 133.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 739      |\n",
      "|    mean_reward        | 57.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.301    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7687     |\n",
      "|    policy_loss        | 0.785    |\n",
      "|    value_loss         | 1.83     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.19e+03 |\n",
      "|    ep_rew_mean        | 164      |\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 40106    |\n",
      "|    total_timesteps    | 1971200  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.922   |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | -0.424   |\n",
      "|    value_loss         | 0.414    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1972000, episode_reward=21.60 +/- 2.06\n",
      "Episode length: 447.60 +/- 68.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 448      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1972000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7703     |\n",
      "|    policy_loss        | -0.433   |\n",
      "|    value_loss         | 0.154    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1976000, episode_reward=222.00 +/- 126.02\n",
      "Episode length: 922.40 +/- 173.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 922      |\n",
      "|    mean_reward        | 222      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.824   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7718     |\n",
      "|    policy_loss        | 0.9      |\n",
      "|    value_loss         | 5.18     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1980000, episode_reward=114.00 +/- 39.42\n",
      "Episode length: 858.00 +/- 97.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 858      |\n",
      "|    mean_reward        | 114      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7734     |\n",
      "|    policy_loss        | -0.384   |\n",
      "|    value_loss         | 0.501    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1984000, episode_reward=148.80 +/- 58.96\n",
      "Episode length: 947.20 +/- 141.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 947      |\n",
      "|    mean_reward        | 149      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1984000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.727   |\n",
      "|    explained_variance | 0.354    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7749     |\n",
      "|    policy_loss        | 1.25     |\n",
      "|    value_loss         | 80.7     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1988000, episode_reward=273.40 +/- 153.06\n",
      "Episode length: 7727.60 +/- 9957.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 7.73e+03 |\n",
      "|    mean_reward        | 273      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1988000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.862   |\n",
      "|    explained_variance | 0.885    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7765     |\n",
      "|    policy_loss        | -0.251   |\n",
      "|    value_loss         | 1.05     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1992000, episode_reward=42.80 +/- 16.36\n",
      "Episode length: 701.60 +/- 229.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 702      |\n",
      "|    mean_reward        | 42.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.634    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7781     |\n",
      "|    policy_loss        | 0.302    |\n",
      "|    value_loss         | 0.777    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1996000, episode_reward=73.60 +/- 34.35\n",
      "Episode length: 750.80 +/- 84.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 751      |\n",
      "|    mean_reward        | 73.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1996000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7796     |\n",
      "|    policy_loss        | 0.123    |\n",
      "|    value_loss         | 0.181    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.2e+03  |\n",
      "|    ep_rew_mean        | 148      |\n",
      "| time/                 |          |\n",
      "|    fps                | 48       |\n",
      "|    iterations         | 7800     |\n",
      "|    time_elapsed       | 40960    |\n",
      "|    total_timesteps    | 1996800  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.706    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7799     |\n",
      "|    policy_loss        | 1.33     |\n",
      "|    value_loss         | 10.3     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2000000, episode_reward=224.80 +/- 100.44\n",
      "Episode length: 1030.80 +/- 70.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+03 |\n",
      "|    mean_reward        | 225      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.37     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7812     |\n",
      "|    policy_loss        | 1.26     |\n",
      "|    value_loss         | 68.5     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2004000, episode_reward=100.60 +/- 23.37\n",
      "Episode length: 1559.60 +/- 1349.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.56e+03 |\n",
      "|    mean_reward        | 101      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2004000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.912   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7828     |\n",
      "|    policy_loss        | 0.0732   |\n",
      "|    value_loss         | 0.896    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2008000, episode_reward=218.20 +/- 132.47\n",
      "Episode length: 1062.00 +/- 57.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+03 |\n",
      "|    mean_reward        | 218      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.07    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7843     |\n",
      "|    policy_loss        | -0.0746  |\n",
      "|    value_loss         | 0.298    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2012000, episode_reward=131.40 +/- 26.23\n",
      "Episode length: 983.60 +/- 98.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 984      |\n",
      "|    mean_reward        | 131      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2012000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.945   |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7859     |\n",
      "|    policy_loss        | 0.0134   |\n",
      "|    value_loss         | 0.285    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2016000, episode_reward=167.80 +/- 61.07\n",
      "Episode length: 4708.00 +/- 7489.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.71e+03 |\n",
      "|    mean_reward        | 168      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2016000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.996   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7874     |\n",
      "|    policy_loss        | -0.221   |\n",
      "|    value_loss         | 0.847    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2020000, episode_reward=311.40 +/- 189.71\n",
      "Episode length: 1058.80 +/- 104.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+03 |\n",
      "|    mean_reward        | 311      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7890     |\n",
      "|    policy_loss        | -0.615   |\n",
      "|    value_loss         | 1.82     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.17e+03 |\n",
      "|    ep_rew_mean        | 148      |\n",
      "| time/                 |          |\n",
      "|    fps                | 48       |\n",
      "|    iterations         | 7900     |\n",
      "|    time_elapsed       | 41547    |\n",
      "|    total_timesteps    | 2022400  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7899     |\n",
      "|    policy_loss        | -0.683   |\n",
      "|    value_loss         | 1.98     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2024000, episode_reward=167.60 +/- 39.90\n",
      "Episode length: 967.60 +/- 159.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 968      |\n",
      "|    mean_reward        | 168      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2024000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7906     |\n",
      "|    policy_loss        | -0.0195  |\n",
      "|    value_loss         | 0.969    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2028000, episode_reward=256.80 +/- 99.53\n",
      "Episode length: 1050.80 +/- 89.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+03 |\n",
      "|    mean_reward        | 257      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2028000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.931   |\n",
      "|    explained_variance | 0.796    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7921     |\n",
      "|    policy_loss        | 0.507    |\n",
      "|    value_loss         | 25.7     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2032000, episode_reward=25.60 +/- 3.72\n",
      "Episode length: 548.00 +/- 33.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 548      |\n",
      "|    mean_reward        | 25.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7937     |\n",
      "|    policy_loss        | 0.313    |\n",
      "|    value_loss         | 1.19     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2036000, episode_reward=279.20 +/- 112.08\n",
      "Episode length: 1108.80 +/- 80.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+03 |\n",
      "|    mean_reward        | 279      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2036000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.841   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7953     |\n",
      "|    policy_loss        | 0.25     |\n",
      "|    value_loss         | 3.87     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2040000, episode_reward=219.00 +/- 105.62\n",
      "Episode length: 1136.00 +/- 183.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+03 |\n",
      "|    mean_reward        | 219      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.575   |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7968     |\n",
      "|    policy_loss        | -0.388   |\n",
      "|    value_loss         | 14.2     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2044000, episode_reward=204.00 +/- 109.39\n",
      "Episode length: 959.80 +/- 161.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 960      |\n",
      "|    mean_reward        | 204      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2044000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7984     |\n",
      "|    policy_loss        | -0.671   |\n",
      "|    value_loss         | 6.08     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2048000, episode_reward=217.60 +/- 137.77\n",
      "Episode length: 927.60 +/- 175.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 928      |\n",
      "|    mean_reward        | 218      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.726    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | 0.94     |\n",
      "|    value_loss         | 17.4     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.25e+03 |\n",
      "|    ep_rew_mean     | 170      |\n",
      "| time/              |          |\n",
      "|    fps             | 48       |\n",
      "|    iterations      | 8000     |\n",
      "|    time_elapsed    | 42010    |\n",
      "|    total_timesteps | 2048000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2052000, episode_reward=140.40 +/- 36.96\n",
      "Episode length: 2789.60 +/- 3567.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.79e+03 |\n",
      "|    mean_reward        | 140      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2052000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.826    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8015     |\n",
      "|    policy_loss        | 1.43     |\n",
      "|    value_loss         | 21.4     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2056000, episode_reward=143.80 +/- 61.01\n",
      "Episode length: 936.80 +/- 214.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 937      |\n",
      "|    mean_reward        | 144      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2056000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8031     |\n",
      "|    policy_loss        | 0.536    |\n",
      "|    value_loss         | 15.2     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2060000, episode_reward=152.60 +/- 67.30\n",
      "Episode length: 812.40 +/- 95.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 812      |\n",
      "|    mean_reward        | 153      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.46    |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8046     |\n",
      "|    policy_loss        | 0.0798   |\n",
      "|    value_loss         | 0.705    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2064000, episode_reward=184.40 +/- 48.79\n",
      "Episode length: 895.60 +/- 178.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 896      |\n",
      "|    mean_reward        | 184      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.853    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8062     |\n",
      "|    policy_loss        | 1.22     |\n",
      "|    value_loss         | 34.3     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2068000, episode_reward=207.00 +/- 69.64\n",
      "Episode length: 896.00 +/- 171.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 896      |\n",
      "|    mean_reward        | 207      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2068000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8078     |\n",
      "|    policy_loss        | -0.0438  |\n",
      "|    value_loss         | 0.399    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2072000, episode_reward=259.20 +/- 36.16\n",
      "Episode length: 1038.80 +/- 134.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+03 |\n",
      "|    mean_reward        | 259      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.421    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8093     |\n",
      "|    policy_loss        | -0.642   |\n",
      "|    value_loss         | 505      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.21e+03 |\n",
      "|    ep_rew_mean        | 171      |\n",
      "| time/                 |          |\n",
      "|    fps                | 48       |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 42501    |\n",
      "|    total_timesteps    | 2073600  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -0.424   |\n",
      "|    value_loss         | 1.01     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2076000, episode_reward=235.40 +/- 87.04\n",
      "Episode length: 868.00 +/- 163.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 868      |\n",
      "|    mean_reward        | 235      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2076000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.84     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8109     |\n",
      "|    policy_loss        | -0.185   |\n",
      "|    value_loss         | 0.728    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2080000, episode_reward=225.00 +/- 58.19\n",
      "Episode length: 1050.40 +/- 78.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+03 |\n",
      "|    mean_reward        | 225      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.767   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8124     |\n",
      "|    policy_loss        | 0.182    |\n",
      "|    value_loss         | 2.54     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2084000, episode_reward=323.00 +/- 101.80\n",
      "Episode length: 1003.80 +/- 186.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1e+03    |\n",
      "|    mean_reward        | 323      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2084000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8140     |\n",
      "|    policy_loss        | -0.275   |\n",
      "|    value_loss         | 0.403    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2088000, episode_reward=326.40 +/- 108.34\n",
      "Episode length: 957.60 +/- 63.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 958      |\n",
      "|    mean_reward        | 326      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.495   |\n",
      "|    explained_variance | 0.86     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8156     |\n",
      "|    policy_loss        | 0.193    |\n",
      "|    value_loss         | 46.9     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2092000, episode_reward=264.80 +/- 148.33\n",
      "Episode length: 1006.20 +/- 139.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+03 |\n",
      "|    mean_reward        | 265      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2092000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.951   |\n",
      "|    explained_variance | 0.402    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8171     |\n",
      "|    policy_loss        | 1.53     |\n",
      "|    value_loss         | 350      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2096000, episode_reward=426.80 +/- 216.05\n",
      "Episode length: 961.20 +/- 88.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 961      |\n",
      "|    mean_reward        | 427      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.737   |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8187     |\n",
      "|    policy_loss        | -0.42    |\n",
      "|    value_loss         | 44.4     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.3e+03  |\n",
      "|    ep_rew_mean        | 185      |\n",
      "| time/                 |          |\n",
      "|    fps                | 48       |\n",
      "|    iterations         | 8200     |\n",
      "|    time_elapsed       | 42902    |\n",
      "|    total_timesteps    | 2099200  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.626   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8199     |\n",
      "|    policy_loss        | -0.669   |\n",
      "|    value_loss         | 7.36     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2100000, episode_reward=105.00 +/- 38.87\n",
      "Episode length: 745.40 +/- 122.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 745      |\n",
      "|    mean_reward        | 105      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.863   |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8203     |\n",
      "|    policy_loss        | -0.172   |\n",
      "|    value_loss         | 17.7     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2104000, episode_reward=25.60 +/- 3.32\n",
      "Episode length: 781.20 +/- 29.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 781      |\n",
      "|    mean_reward        | 25.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.995   |\n",
      "|    explained_variance | 0.589    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8218     |\n",
      "|    policy_loss        | 0.418    |\n",
      "|    value_loss         | 4.21     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2108000, episode_reward=37.00 +/- 31.61\n",
      "Episode length: 540.40 +/- 182.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 540      |\n",
      "|    mean_reward        | 37       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2108000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | -0.027   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8234     |\n",
      "|    policy_loss        | 3.38     |\n",
      "|    value_loss         | 185      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2112000, episode_reward=45.20 +/- 7.76\n",
      "Episode length: 782.80 +/- 110.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 783      |\n",
      "|    mean_reward        | 45.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.78     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8249     |\n",
      "|    policy_loss        | 0.446    |\n",
      "|    value_loss         | 1.56     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2116000, episode_reward=92.00 +/- 22.25\n",
      "Episode length: 854.20 +/- 116.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 854      |\n",
      "|    mean_reward        | 92       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2116000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.92    |\n",
      "|    explained_variance | 0.519    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8265     |\n",
      "|    policy_loss        | 0.464    |\n",
      "|    value_loss         | 7.13     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2120000, episode_reward=169.80 +/- 111.80\n",
      "Episode length: 914.40 +/- 154.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 914      |\n",
      "|    mean_reward        | 170      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.861   |\n",
      "|    explained_variance | 0.891    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8281     |\n",
      "|    policy_loss        | -1.32    |\n",
      "|    value_loss         | 5.74     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2124000, episode_reward=26.60 +/- 6.15\n",
      "Episode length: 458.80 +/- 65.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 459      |\n",
      "|    mean_reward        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2124000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8296     |\n",
      "|    policy_loss        | 0.0705   |\n",
      "|    value_loss         | 0.366    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.4e+03  |\n",
      "|    ep_rew_mean        | 193      |\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 8300     |\n",
      "|    time_elapsed       | 43274    |\n",
      "|    total_timesteps    | 2124800  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8299     |\n",
      "|    policy_loss        | 0.434    |\n",
      "|    value_loss         | 2.03     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2128000, episode_reward=148.40 +/- 37.23\n",
      "Episode length: 885.20 +/- 100.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 885      |\n",
      "|    mean_reward        | 148      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2128000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.845    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8312     |\n",
      "|    policy_loss        | -0.555   |\n",
      "|    value_loss         | 2.75     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2132000, episode_reward=218.40 +/- 108.65\n",
      "Episode length: 1098.00 +/- 110.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.1e+03  |\n",
      "|    mean_reward        | 218      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2132000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.589    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8328     |\n",
      "|    policy_loss        | 3.49     |\n",
      "|    value_loss         | 83.6     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2136000, episode_reward=225.40 +/- 110.04\n",
      "Episode length: 1085.60 +/- 135.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.09e+03 |\n",
      "|    mean_reward        | 225      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.671   |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8343     |\n",
      "|    policy_loss        | 0.233    |\n",
      "|    value_loss         | 6.16     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2140000, episode_reward=52.00 +/- 24.75\n",
      "Episode length: 838.40 +/- 65.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 838      |\n",
      "|    mean_reward        | 52       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8359     |\n",
      "|    policy_loss        | -0.48    |\n",
      "|    value_loss         | 0.43     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2144000, episode_reward=84.80 +/- 59.89\n",
      "Episode length: 768.80 +/- 194.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 769      |\n",
      "|    mean_reward        | 84.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.796    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8374     |\n",
      "|    policy_loss        | 0.0735   |\n",
      "|    value_loss         | 0.716    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2148000, episode_reward=133.80 +/- 63.62\n",
      "Episode length: 880.80 +/- 110.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 881      |\n",
      "|    mean_reward        | 134      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2148000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.676   |\n",
      "|    explained_variance | 0.638    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8390     |\n",
      "|    policy_loss        | 0.206    |\n",
      "|    value_loss         | 5.62     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.29e+03 |\n",
      "|    ep_rew_mean        | 189      |\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 8400     |\n",
      "|    time_elapsed       | 43674    |\n",
      "|    total_timesteps    | 2150400  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.887   |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8399     |\n",
      "|    policy_loss        | -1.24    |\n",
      "|    value_loss         | 5.61     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2152000, episode_reward=129.60 +/- 43.73\n",
      "Episode length: 945.40 +/- 199.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 945      |\n",
      "|    mean_reward        | 130      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.911   |\n",
      "|    explained_variance | 0.795    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8406     |\n",
      "|    policy_loss        | -0.62    |\n",
      "|    value_loss         | 5.83     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2156000, episode_reward=243.60 +/- 113.50\n",
      "Episode length: 888.00 +/- 90.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 888      |\n",
      "|    mean_reward        | 244      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2156000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.514    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8421     |\n",
      "|    policy_loss        | 0.456    |\n",
      "|    value_loss         | 21.7     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2160000, episode_reward=267.20 +/- 101.40\n",
      "Episode length: 1060.00 +/- 108.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+03 |\n",
      "|    mean_reward        | 267      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.976   |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8437     |\n",
      "|    policy_loss        | -0.703   |\n",
      "|    value_loss         | 2.26     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2164000, episode_reward=286.80 +/- 154.01\n",
      "Episode length: 1055.60 +/- 93.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+03 |\n",
      "|    mean_reward        | 287      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2164000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.895   |\n",
      "|    explained_variance | 0.738    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8453     |\n",
      "|    policy_loss        | 0.667    |\n",
      "|    value_loss         | 20       |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2168000, episode_reward=241.20 +/- 103.62\n",
      "Episode length: 1091.20 +/- 66.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.09e+03 |\n",
      "|    mean_reward        | 241      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.763   |\n",
      "|    explained_variance | 0.173    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8468     |\n",
      "|    policy_loss        | 4.55     |\n",
      "|    value_loss         | 193      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2172000, episode_reward=172.00 +/- 65.41\n",
      "Episode length: 1066.80 +/- 95.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.07e+03 |\n",
      "|    mean_reward        | 172      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2172000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.825   |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8484     |\n",
      "|    policy_loss        | -0.845   |\n",
      "|    value_loss         | 3.66     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2176000, episode_reward=135.60 +/- 83.50\n",
      "Episode length: 840.20 +/- 192.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 840      |\n",
      "|    mean_reward        | 136      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.631   |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8499     |\n",
      "|    policy_loss        | -0.515   |\n",
      "|    value_loss         | 6.36     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.18e+03 |\n",
      "|    ep_rew_mean     | 175      |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 8500     |\n",
      "|    time_elapsed    | 44116    |\n",
      "|    total_timesteps | 2176000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2180000, episode_reward=150.00 +/- 111.52\n",
      "Episode length: 780.40 +/- 183.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 780      |\n",
      "|    mean_reward        | 150      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.767   |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8515     |\n",
      "|    policy_loss        | -0.143   |\n",
      "|    value_loss         | 5.89     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2184000, episode_reward=161.60 +/- 89.95\n",
      "Episode length: 861.60 +/- 127.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 862      |\n",
      "|    mean_reward        | 162      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | 0.724    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8531     |\n",
      "|    policy_loss        | -0.426   |\n",
      "|    value_loss         | 3.15     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2188000, episode_reward=121.60 +/- 52.91\n",
      "Episode length: 1062.00 +/- 148.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+03 |\n",
      "|    mean_reward        | 122      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2188000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.686   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8546     |\n",
      "|    policy_loss        | -1.14    |\n",
      "|    value_loss         | 5.89     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2192000, episode_reward=157.40 +/- 110.48\n",
      "Episode length: 755.60 +/- 122.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 756      |\n",
      "|    mean_reward        | 157      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.934   |\n",
      "|    explained_variance | -0.0648  |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8562     |\n",
      "|    policy_loss        | -0.394   |\n",
      "|    value_loss         | 7.51     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2196000, episode_reward=52.60 +/- 16.80\n",
      "Episode length: 728.00 +/- 93.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 728      |\n",
      "|    mean_reward        | 52.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2196000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.28    |\n",
      "|    explained_variance | 0.242    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8578     |\n",
      "|    policy_loss        | 1.5      |\n",
      "|    value_loss         | 7.34     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2200000, episode_reward=139.60 +/- 81.22\n",
      "Episode length: 832.00 +/- 86.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 832      |\n",
      "|    mean_reward        | 140      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.763   |\n",
      "|    explained_variance | 0.832    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8593     |\n",
      "|    policy_loss        | -1.18    |\n",
      "|    value_loss         | 4.99     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 922      |\n",
      "|    ep_rew_mean        | 140      |\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 8600     |\n",
      "|    time_elapsed       | 44481    |\n",
      "|    total_timesteps    | 2201600  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.8     |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | -0.737   |\n",
      "|    value_loss         | 2.74     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2204000, episode_reward=243.80 +/- 116.14\n",
      "Episode length: 786.40 +/- 36.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 786      |\n",
      "|    mean_reward        | 244      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2204000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.94    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8609     |\n",
      "|    policy_loss        | -0.424   |\n",
      "|    value_loss         | 3.22     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2208000, episode_reward=126.40 +/- 53.52\n",
      "Episode length: 834.00 +/- 85.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 834      |\n",
      "|    mean_reward        | 126      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.944   |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8624     |\n",
      "|    policy_loss        | -0.521   |\n",
      "|    value_loss         | 4.17     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2212000, episode_reward=169.60 +/- 48.90\n",
      "Episode length: 837.20 +/- 87.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 837      |\n",
      "|    mean_reward        | 170      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2212000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.532   |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8640     |\n",
      "|    policy_loss        | -0.407   |\n",
      "|    value_loss         | 6.09     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2216000, episode_reward=117.00 +/- 55.02\n",
      "Episode length: 873.60 +/- 206.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 874      |\n",
      "|    mean_reward        | 117      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8656     |\n",
      "|    policy_loss        | -0.332   |\n",
      "|    value_loss         | 5.07     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2220000, episode_reward=259.80 +/- 103.50\n",
      "Episode length: 888.80 +/- 103.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 889      |\n",
      "|    mean_reward        | 260      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.53    |\n",
      "|    explained_variance | 0.346    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8671     |\n",
      "|    policy_loss        | -0.121   |\n",
      "|    value_loss         | 88.1     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2224000, episode_reward=171.60 +/- 111.64\n",
      "Episode length: 906.00 +/- 100.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 906      |\n",
      "|    mean_reward        | 172      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.97    |\n",
      "|    explained_variance | 0.861    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8687     |\n",
      "|    policy_loss        | -1.51    |\n",
      "|    value_loss         | 50.5     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 908      |\n",
      "|    ep_rew_mean        | 158      |\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 8700     |\n",
      "|    time_elapsed       | 44857    |\n",
      "|    total_timesteps    | 2227200  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.59     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | -0.613   |\n",
      "|    value_loss         | 2.48     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2228000, episode_reward=180.00 +/- 96.28\n",
      "Episode length: 903.20 +/- 113.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 903      |\n",
      "|    mean_reward        | 180      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2228000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.577   |\n",
      "|    explained_variance | 0.346    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8703     |\n",
      "|    policy_loss        | 3.2      |\n",
      "|    value_loss         | 156      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2232000, episode_reward=312.40 +/- 112.33\n",
      "Episode length: 1000.00 +/- 104.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1e+03    |\n",
      "|    mean_reward        | 312      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2232000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.802   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8718     |\n",
      "|    policy_loss        | -0.302   |\n",
      "|    value_loss         | 3.26     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2236000, episode_reward=193.80 +/- 75.09\n",
      "Episode length: 946.40 +/- 248.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 946      |\n",
      "|    mean_reward        | 194      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2236000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.869   |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8734     |\n",
      "|    policy_loss        | -0.095   |\n",
      "|    value_loss         | 3.62     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2240000, episode_reward=330.40 +/- 207.62\n",
      "Episode length: 1024.00 +/- 102.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+03 |\n",
      "|    mean_reward        | 330      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.809   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8749     |\n",
      "|    policy_loss        | 0.364    |\n",
      "|    value_loss         | 7.51     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2244000, episode_reward=333.20 +/- 117.81\n",
      "Episode length: 968.40 +/- 127.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 968      |\n",
      "|    mean_reward        | 333      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2244000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.488   |\n",
      "|    explained_variance | 0.0123   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8765     |\n",
      "|    policy_loss        | 4.29     |\n",
      "|    value_loss         | 1.85e+03 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2248000, episode_reward=312.60 +/- 213.89\n",
      "Episode length: 819.20 +/- 155.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 819      |\n",
      "|    mean_reward        | 313      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.731   |\n",
      "|    explained_variance | 0.17     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8781     |\n",
      "|    policy_loss        | -0.523   |\n",
      "|    value_loss         | 3.28e+03 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2252000, episode_reward=218.60 +/- 85.44\n",
      "Episode length: 906.00 +/- 125.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 906      |\n",
      "|    mean_reward        | 219      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2252000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.783   |\n",
      "|    explained_variance | 0.792    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8796     |\n",
      "|    policy_loss        | -0.677   |\n",
      "|    value_loss         | 11.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 878      |\n",
      "|    ep_rew_mean        | 179      |\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 8800     |\n",
      "|    time_elapsed       | 45272    |\n",
      "|    total_timesteps    | 2252800  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.62    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8799     |\n",
      "|    policy_loss        | -0.191   |\n",
      "|    value_loss         | 6.01     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2256000, episode_reward=283.40 +/- 110.38\n",
      "Episode length: 1082.40 +/- 131.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+03 |\n",
      "|    mean_reward        | 283      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.489   |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8812     |\n",
      "|    policy_loss        | 0.701    |\n",
      "|    value_loss         | 85.8     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2260000, episode_reward=270.20 +/- 141.07\n",
      "Episode length: 1006.20 +/- 203.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+03 |\n",
      "|    mean_reward        | 270      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.441   |\n",
      "|    explained_variance | 0.419    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8828     |\n",
      "|    policy_loss        | 0.696    |\n",
      "|    value_loss         | 277      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2264000, episode_reward=276.20 +/- 154.76\n",
      "Episode length: 974.80 +/- 18.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 975      |\n",
      "|    mean_reward        | 276      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.632   |\n",
      "|    explained_variance | 0.692    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8843     |\n",
      "|    policy_loss        | 0.14     |\n",
      "|    value_loss         | 354      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2268000, episode_reward=159.80 +/- 66.02\n",
      "Episode length: 980.00 +/- 227.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 980      |\n",
      "|    mean_reward        | 160      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2268000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.666   |\n",
      "|    explained_variance | 0.811    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8859     |\n",
      "|    policy_loss        | -0.63    |\n",
      "|    value_loss         | 28.6     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2272000, episode_reward=290.40 +/- 133.83\n",
      "Episode length: 1057.20 +/- 143.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+03 |\n",
      "|    mean_reward        | 290      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.428   |\n",
      "|    explained_variance | 0.78     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8874     |\n",
      "|    policy_loss        | -0.611   |\n",
      "|    value_loss         | 48.3     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2276000, episode_reward=76.40 +/- 45.87\n",
      "Episode length: 781.20 +/- 222.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 781      |\n",
      "|    mean_reward        | 76.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2276000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0.889    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8890     |\n",
      "|    policy_loss        | 0.214    |\n",
      "|    value_loss         | 0.831    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 905      |\n",
      "|    ep_rew_mean        | 220      |\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 8900     |\n",
      "|    time_elapsed       | 45693    |\n",
      "|    total_timesteps    | 2278400  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.634   |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8899     |\n",
      "|    policy_loss        | 0.184    |\n",
      "|    value_loss         | 0.89     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2280000, episode_reward=23.20 +/- 7.73\n",
      "Episode length: 5732.80 +/- 10631.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.73e+03 |\n",
      "|    mean_reward        | 23.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.305    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8906     |\n",
      "|    policy_loss        | -0.121   |\n",
      "|    value_loss         | 1.68     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2284000, episode_reward=23.60 +/- 12.24\n",
      "Episode length: 5793.20 +/- 10602.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.79e+03 |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2284000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.746   |\n",
      "|    explained_variance | -0.762   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8921     |\n",
      "|    policy_loss        | -0.368   |\n",
      "|    value_loss         | 3.08     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2288000, episode_reward=52.20 +/- 29.69\n",
      "Episode length: 1225.20 +/- 1006.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.23e+03 |\n",
      "|    mean_reward        | 52.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.723   |\n",
      "|    explained_variance | 0.137    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8937     |\n",
      "|    policy_loss        | 1.96     |\n",
      "|    value_loss         | 56.2     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2292000, episode_reward=233.40 +/- 87.38\n",
      "Episode length: 1059.40 +/- 227.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+03 |\n",
      "|    mean_reward        | 233      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2292000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.232   |\n",
      "|    explained_variance | 0.826    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8953     |\n",
      "|    policy_loss        | 0.297    |\n",
      "|    value_loss         | 5.71     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2296000, episode_reward=229.80 +/- 151.62\n",
      "Episode length: 2061.20 +/- 2357.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.06e+03 |\n",
      "|    mean_reward        | 230      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.646   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8968     |\n",
      "|    policy_loss        | -1.41    |\n",
      "|    value_loss         | 7.42     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2300000, episode_reward=83.80 +/- 40.62\n",
      "Episode length: 801.40 +/- 107.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 801      |\n",
      "|    mean_reward        | 83.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.206    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8984     |\n",
      "|    policy_loss        | 0.589    |\n",
      "|    value_loss         | 25.9     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2304000, episode_reward=58.60 +/- 17.70\n",
      "Episode length: 775.80 +/- 128.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 776      |\n",
      "|    mean_reward        | 58.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2304000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.931   |\n",
      "|    explained_variance | 0.783    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | -0.157   |\n",
      "|    value_loss         | 1.97     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 219      |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 9000     |\n",
      "|    time_elapsed    | 46438    |\n",
      "|    total_timesteps | 2304000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2308000, episode_reward=268.20 +/- 109.68\n",
      "Episode length: 1052.00 +/- 149.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+03 |\n",
      "|    mean_reward        | 268      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2308000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.538   |\n",
      "|    explained_variance | 0.18     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9015     |\n",
      "|    policy_loss        | 4.68     |\n",
      "|    value_loss         | 322      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2312000, episode_reward=271.80 +/- 119.60\n",
      "Episode length: 1156.40 +/- 161.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.16e+03 |\n",
      "|    mean_reward        | 272      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.994   |\n",
      "|    explained_variance | 0.893    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9031     |\n",
      "|    policy_loss        | -0.165   |\n",
      "|    value_loss         | 5.86     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2316000, episode_reward=159.40 +/- 50.72\n",
      "Episode length: 1064.80 +/- 277.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.06e+03 |\n",
      "|    mean_reward        | 159      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2316000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.784   |\n",
      "|    explained_variance | 0.817    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9046     |\n",
      "|    policy_loss        | -0.346   |\n",
      "|    value_loss         | 8.09     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2320000, episode_reward=116.40 +/- 33.71\n",
      "Episode length: 4846.80 +/- 7663.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.85e+03 |\n",
      "|    mean_reward        | 116      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.969   |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9062     |\n",
      "|    policy_loss        | 1.12     |\n",
      "|    value_loss         | 6.59     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2324000, episode_reward=190.00 +/- 51.34\n",
      "Episode length: 978.40 +/- 76.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 978      |\n",
      "|    mean_reward        | 190      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2324000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.949   |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9078     |\n",
      "|    policy_loss        | -0.153   |\n",
      "|    value_loss         | 1.45     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2328000, episode_reward=177.60 +/- 114.93\n",
      "Episode length: 1007.20 +/- 122.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+03 |\n",
      "|    mean_reward        | 178      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9093     |\n",
      "|    policy_loss        | -0.199   |\n",
      "|    value_loss         | 1.03     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 911      |\n",
      "|    ep_rew_mean        | 208      |\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 46980    |\n",
      "|    total_timesteps    | 2329600  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.967   |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | -0.116   |\n",
      "|    value_loss         | 8.98     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2332000, episode_reward=208.80 +/- 110.48\n",
      "Episode length: 1942.80 +/- 1966.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.94e+03 |\n",
      "|    mean_reward        | 209      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2332000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.977   |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9109     |\n",
      "|    policy_loss        | -0.194   |\n",
      "|    value_loss         | 5.36     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2336000, episode_reward=270.60 +/- 106.52\n",
      "Episode length: 818.80 +/- 90.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 819      |\n",
      "|    mean_reward        | 271      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9124     |\n",
      "|    policy_loss        | 0.0988   |\n",
      "|    value_loss         | 1.9      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2340000, episode_reward=323.40 +/- 225.61\n",
      "Episode length: 1191.60 +/- 69.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.19e+03 |\n",
      "|    mean_reward        | 323      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.559   |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9140     |\n",
      "|    policy_loss        | -0.437   |\n",
      "|    value_loss         | 10.7     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2344000, episode_reward=274.40 +/- 224.18\n",
      "Episode length: 1075.60 +/- 160.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.08e+03 |\n",
      "|    mean_reward        | 274      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9156     |\n",
      "|    policy_loss        | -0.409   |\n",
      "|    value_loss         | 1.87     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2348000, episode_reward=129.60 +/- 20.75\n",
      "Episode length: 992.80 +/- 62.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 993      |\n",
      "|    mean_reward        | 130      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2348000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.731   |\n",
      "|    explained_variance | 0.778    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9171     |\n",
      "|    policy_loss        | -0.12    |\n",
      "|    value_loss         | 35.2     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2352000, episode_reward=315.00 +/- 244.54\n",
      "Episode length: 1073.80 +/- 94.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.07e+03 |\n",
      "|    mean_reward        | 315      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.18    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9187     |\n",
      "|    policy_loss        | 0.0772   |\n",
      "|    value_loss         | 2.11     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 929      |\n",
      "|    ep_rew_mean        | 193      |\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 9200     |\n",
      "|    time_elapsed       | 47443    |\n",
      "|    total_timesteps    | 2355200  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.623   |\n",
      "|    explained_variance | 0.527    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | -0.186   |\n",
      "|    value_loss         | 253      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2356000, episode_reward=246.40 +/- 128.01\n",
      "Episode length: 959.40 +/- 153.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 959      |\n",
      "|    mean_reward        | 246      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2356000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.279   |\n",
      "|    explained_variance | 0.459    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9203     |\n",
      "|    policy_loss        | -0.0985  |\n",
      "|    value_loss         | 517      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2360000, episode_reward=188.20 +/- 64.17\n",
      "Episode length: 913.20 +/- 54.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 913      |\n",
      "|    mean_reward        | 188      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.897   |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9218     |\n",
      "|    policy_loss        | -0.57    |\n",
      "|    value_loss         | 3.99     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2364000, episode_reward=351.00 +/- 190.04\n",
      "Episode length: 986.00 +/- 69.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 986      |\n",
      "|    mean_reward        | 351      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2364000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.782   |\n",
      "|    explained_variance | 0.793    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9234     |\n",
      "|    policy_loss        | -0.22    |\n",
      "|    value_loss         | 11.2     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2368000, episode_reward=293.60 +/- 132.56\n",
      "Episode length: 1010.40 +/- 68.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+03 |\n",
      "|    mean_reward        | 294      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.768   |\n",
      "|    explained_variance | 0.319    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9249     |\n",
      "|    policy_loss        | 4.38     |\n",
      "|    value_loss         | 468      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2372000, episode_reward=269.40 +/- 85.67\n",
      "Episode length: 1141.20 +/- 89.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+03 |\n",
      "|    mean_reward        | 269      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2372000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.687   |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9265     |\n",
      "|    policy_loss        | -0.456   |\n",
      "|    value_loss         | 8.55     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2376000, episode_reward=264.40 +/- 74.91\n",
      "Episode length: 1014.40 +/- 143.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+03 |\n",
      "|    mean_reward        | 264      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9281     |\n",
      "|    policy_loss        | -0.0824  |\n",
      "|    value_loss         | 4.28     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2380000, episode_reward=360.60 +/- 98.43\n",
      "Episode length: 1111.20 +/- 64.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+03 |\n",
      "|    mean_reward        | 361      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.983   |\n",
      "|    explained_variance | 0.75     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9296     |\n",
      "|    policy_loss        | 0.44     |\n",
      "|    value_loss         | 82.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 964      |\n",
      "|    ep_rew_mean        | 183      |\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 9300     |\n",
      "|    time_elapsed       | 47901    |\n",
      "|    total_timesteps    | 2380800  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.849   |\n",
      "|    explained_variance | 0.863    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9299     |\n",
      "|    policy_loss        | 0.398    |\n",
      "|    value_loss         | 33.8     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2384000, episode_reward=186.40 +/- 44.07\n",
      "Episode length: 1099.60 +/- 58.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.1e+03  |\n",
      "|    mean_reward        | 186      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2384000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.985   |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9312     |\n",
      "|    policy_loss        | 0.562    |\n",
      "|    value_loss         | 10.3     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2388000, episode_reward=180.40 +/- 65.98\n",
      "Episode length: 974.40 +/- 121.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 974      |\n",
      "|    mean_reward        | 180      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2388000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.897   |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9328     |\n",
      "|    policy_loss        | -0.028   |\n",
      "|    value_loss         | 24.1     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2392000, episode_reward=221.60 +/- 103.31\n",
      "Episode length: 1018.80 +/- 123.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+03 |\n",
      "|    mean_reward        | 222      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2392000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.945   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9343     |\n",
      "|    policy_loss        | -0.215   |\n",
      "|    value_loss         | 3.41     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2396000, episode_reward=207.80 +/- 29.73\n",
      "Episode length: 983.20 +/- 65.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 983      |\n",
      "|    mean_reward        | 208      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2396000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.61    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9359     |\n",
      "|    policy_loss        | -1.15    |\n",
      "|    value_loss         | 9.42     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2400000, episode_reward=411.20 +/- 159.25\n",
      "Episode length: 991.60 +/- 61.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 992      |\n",
      "|    mean_reward        | 411      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.819   |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9374     |\n",
      "|    policy_loss        | -0.478   |\n",
      "|    value_loss         | 14.1     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2404000, episode_reward=333.00 +/- 135.76\n",
      "Episode length: 1042.40 +/- 52.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.04e+03 |\n",
      "|    mean_reward        | 333      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2404000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.451   |\n",
      "|    explained_variance | 0.217    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9390     |\n",
      "|    policy_loss        | 4.53     |\n",
      "|    value_loss         | 2.55e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+03 |\n",
      "|    ep_rew_mean        | 218      |\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 9400     |\n",
      "|    time_elapsed       | 48329    |\n",
      "|    total_timesteps    | 2406400  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.649   |\n",
      "|    explained_variance | 0.551    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | -0.141   |\n",
      "|    value_loss         | 71.3     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2408000, episode_reward=109.40 +/- 63.43\n",
      "Episode length: 15301.60 +/- 12065.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.53e+04 |\n",
      "|    mean_reward        | 109      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.977   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9406     |\n",
      "|    policy_loss        | -0.419   |\n",
      "|    value_loss         | 0.971    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2412000, episode_reward=252.80 +/- 144.14\n",
      "Episode length: 964.00 +/- 74.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 964      |\n",
      "|    mean_reward        | 253      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2412000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.49    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9421     |\n",
      "|    policy_loss        | 0.244    |\n",
      "|    value_loss         | 31.4     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2416000, episode_reward=335.80 +/- 133.16\n",
      "Episode length: 1031.60 +/- 106.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.03e+03 |\n",
      "|    mean_reward        | 336      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.914   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9437     |\n",
      "|    policy_loss        | -0.49    |\n",
      "|    value_loss         | 3.19     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2420000, episode_reward=242.20 +/- 89.57\n",
      "Episode length: 1008.80 +/- 86.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+03 |\n",
      "|    mean_reward        | 242      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.825   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9453     |\n",
      "|    policy_loss        | -0.141   |\n",
      "|    value_loss         | 1.25     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2424000, episode_reward=206.20 +/- 63.50\n",
      "Episode length: 974.40 +/- 68.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 974      |\n",
      "|    mean_reward        | 206      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2424000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9468     |\n",
      "|    policy_loss        | -0.0258  |\n",
      "|    value_loss         | 2.01     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2428000, episode_reward=151.00 +/- 21.84\n",
      "Episode length: 982.00 +/- 86.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 982      |\n",
      "|    mean_reward        | 151      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2428000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.489    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9484     |\n",
      "|    policy_loss        | 3.01     |\n",
      "|    value_loss         | 160      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2432000, episode_reward=165.00 +/- 120.97\n",
      "Episode length: 1074.80 +/- 393.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.07e+03 |\n",
      "|    mean_reward        | 165      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2432000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | -0.0569  |\n",
      "|    value_loss         | 3.44     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.23e+03 |\n",
      "|    ep_rew_mean     | 252      |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 9500     |\n",
      "|    time_elapsed    | 49177    |\n",
      "|    total_timesteps | 2432000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2436000, episode_reward=218.80 +/- 191.97\n",
      "Episode length: 963.20 +/- 214.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 963      |\n",
      "|    mean_reward        | 219      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2436000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.552    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9515     |\n",
      "|    policy_loss        | 0.0425   |\n",
      "|    value_loss         | 106      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2440000, episode_reward=147.40 +/- 28.77\n",
      "Episode length: 1015.60 +/- 119.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+03 |\n",
      "|    mean_reward        | 147      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.905   |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9531     |\n",
      "|    policy_loss        | -0.198   |\n",
      "|    value_loss         | 2.54     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2444000, episode_reward=254.40 +/- 158.19\n",
      "Episode length: 933.20 +/- 108.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 933      |\n",
      "|    mean_reward        | 254      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2444000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.634   |\n",
      "|    explained_variance | 0.191    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9546     |\n",
      "|    policy_loss        | 0.4      |\n",
      "|    value_loss         | 437      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2448000, episode_reward=369.40 +/- 83.61\n",
      "Episode length: 1854.40 +/- 1668.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.85e+03 |\n",
      "|    mean_reward        | 369      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.717   |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9562     |\n",
      "|    policy_loss        | 0.174    |\n",
      "|    value_loss         | 15       |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2452000, episode_reward=38.80 +/- 17.35\n",
      "Episode length: 622.40 +/- 92.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 622      |\n",
      "|    mean_reward        | 38.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2452000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9578     |\n",
      "|    policy_loss        | -0.41    |\n",
      "|    value_loss         | 0.503    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2456000, episode_reward=35.60 +/- 20.75\n",
      "Episode length: 2056.00 +/- 2945.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 2.06e+03 |\n",
      "|    mean_reward        | 35.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.0927   |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9593     |\n",
      "|    policy_loss        | -0.00932 |\n",
      "|    value_loss         | 0.943    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.18e+03 |\n",
      "|    ep_rew_mean        | 245      |\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 9600     |\n",
      "|    time_elapsed       | 49643    |\n",
      "|    total_timesteps    | 2457600  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.746    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9599     |\n",
      "|    policy_loss        | -0.155   |\n",
      "|    value_loss         | 0.369    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2460000, episode_reward=17.80 +/- 1.94\n",
      "Episode length: 474.00 +/- 35.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 474      |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 0.906    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9609     |\n",
      "|    policy_loss        | -0.135   |\n",
      "|    value_loss         | 0.196    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2464000, episode_reward=142.00 +/- 62.21\n",
      "Episode length: 854.40 +/- 173.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 854      |\n",
      "|    mean_reward        | 142      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.773    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9624     |\n",
      "|    policy_loss        | 0.77     |\n",
      "|    value_loss         | 8.51     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2468000, episode_reward=225.40 +/- 104.66\n",
      "Episode length: 1009.80 +/- 249.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+03 |\n",
      "|    mean_reward        | 225      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2468000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.71    |\n",
      "|    explained_variance | 0.321    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9640     |\n",
      "|    policy_loss        | -1.18    |\n",
      "|    value_loss         | 827      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2472000, episode_reward=294.40 +/- 95.82\n",
      "Episode length: 1004.00 +/- 54.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1e+03    |\n",
      "|    mean_reward        | 294      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.789   |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9656     |\n",
      "|    policy_loss        | -0.15    |\n",
      "|    value_loss         | 20.5     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2476000, episode_reward=340.40 +/- 85.25\n",
      "Episode length: 1138.40 +/- 111.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.14e+03 |\n",
      "|    mean_reward        | 340      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2476000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.305   |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9671     |\n",
      "|    policy_loss        | 0.134    |\n",
      "|    value_loss         | 50.5     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2480000, episode_reward=143.20 +/- 25.51\n",
      "Episode length: 5904.40 +/- 9898.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.9e+03  |\n",
      "|    mean_reward        | 143      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.822   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9687     |\n",
      "|    policy_loss        | 0.437    |\n",
      "|    value_loss         | 20.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13e+03 |\n",
      "|    ep_rew_mean        | 222      |\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 9700     |\n",
      "|    time_elapsed       | 50187    |\n",
      "|    total_timesteps    | 2483200  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.506   |\n",
      "|    explained_variance | 0.703    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | 0.535    |\n",
      "|    value_loss         | 59.9     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2484000, episode_reward=327.00 +/- 75.51\n",
      "Episode length: 955.60 +/- 70.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 956      |\n",
      "|    mean_reward        | 327      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2484000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.518   |\n",
      "|    explained_variance | 0.725    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9703     |\n",
      "|    policy_loss        | 0.159    |\n",
      "|    value_loss         | 19.8     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2488000, episode_reward=294.20 +/- 101.35\n",
      "Episode length: 910.80 +/- 79.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 911      |\n",
      "|    mean_reward        | 294      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.746   |\n",
      "|    explained_variance | 0.768    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9718     |\n",
      "|    policy_loss        | 0.832    |\n",
      "|    value_loss         | 229      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2492000, episode_reward=396.60 +/- 218.90\n",
      "Episode length: 993.60 +/- 125.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 994      |\n",
      "|    mean_reward        | 397      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2492000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.849   |\n",
      "|    explained_variance | 0.688    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9734     |\n",
      "|    policy_loss        | -0.207   |\n",
      "|    value_loss         | 17.7     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2496000, episode_reward=143.40 +/- 52.66\n",
      "Episode length: 897.20 +/- 102.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 897      |\n",
      "|    mean_reward        | 143      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2496000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.512   |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9749     |\n",
      "|    policy_loss        | -0.337   |\n",
      "|    value_loss         | 9.85     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2500000, episode_reward=69.80 +/- 26.38\n",
      "Episode length: 4310.40 +/- 5255.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4.31e+03 |\n",
      "|    mean_reward        | 69.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9765     |\n",
      "|    policy_loss        | 0.209    |\n",
      "|    value_loss         | 0.648    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2504000, episode_reward=24.00 +/- 17.17\n",
      "Episode length: 395.80 +/- 74.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 396      |\n",
      "|    mean_reward        | 24       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.552    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9781     |\n",
      "|    policy_loss        | 0.0083   |\n",
      "|    value_loss         | 0.946    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2508000, episode_reward=20.20 +/- 20.54\n",
      "Episode length: 435.60 +/- 155.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 436      |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2508000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0.633    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9796     |\n",
      "|    policy_loss        | 1.19     |\n",
      "|    value_loss         | 2.61     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04e+03 |\n",
      "|    ep_rew_mean        | 174      |\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 50693    |\n",
      "|    total_timesteps    | 2508800  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0.574    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | -0.28    |\n",
      "|    value_loss         | 0.301    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2512000, episode_reward=34.00 +/- 13.21\n",
      "Episode length: 717.60 +/- 97.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 718      |\n",
      "|    mean_reward        | 34       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2512000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.583    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9812     |\n",
      "|    policy_loss        | 0.207    |\n",
      "|    value_loss         | 3.46     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2516000, episode_reward=37.40 +/- 9.73\n",
      "Episode length: 748.00 +/- 175.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 748      |\n",
      "|    mean_reward        | 37.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2516000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0.117    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9828     |\n",
      "|    policy_loss        | 0.363    |\n",
      "|    value_loss         | 3.96     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=385.00 +/- 95.16\n",
      "Episode length: 916.80 +/- 77.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 917      |\n",
      "|    mean_reward        | 385      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.867   |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9843     |\n",
      "|    policy_loss        | 0.0842   |\n",
      "|    value_loss         | 5.16     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2524000, episode_reward=281.20 +/- 162.97\n",
      "Episode length: 937.60 +/- 43.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 938      |\n",
      "|    mean_reward        | 281      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2524000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.542    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9859     |\n",
      "|    policy_loss        | 0.162    |\n",
      "|    value_loss         | 87.2     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2528000, episode_reward=271.00 +/- 121.76\n",
      "Episode length: 998.00 +/- 76.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 998      |\n",
      "|    mean_reward        | 271      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9874     |\n",
      "|    policy_loss        | -0.0469  |\n",
      "|    value_loss         | 3.98     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2532000, episode_reward=157.60 +/- 24.04\n",
      "Episode length: 1074.40 +/- 63.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.07e+03 |\n",
      "|    mean_reward        | 158      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2532000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.893   |\n",
      "|    explained_variance | 0.453    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9890     |\n",
      "|    policy_loss        | 1.3      |\n",
      "|    value_loss         | 147      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 837      |\n",
      "|    ep_rew_mean        | 159      |\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 9900     |\n",
      "|    time_elapsed       | 51108    |\n",
      "|    total_timesteps    | 2534400  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9899     |\n",
      "|    policy_loss        | 0.277    |\n",
      "|    value_loss         | 2.51     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2536000, episode_reward=240.00 +/- 87.40\n",
      "Episode length: 952.80 +/- 138.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 953      |\n",
      "|    mean_reward        | 240      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9906     |\n",
      "|    policy_loss        | -0.151   |\n",
      "|    value_loss         | 0.405    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2540000, episode_reward=229.00 +/- 121.78\n",
      "Episode length: 3064.80 +/- 4423.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 3.06e+03 |\n",
      "|    mean_reward        | 229      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.757   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9921     |\n",
      "|    policy_loss        | 0.705    |\n",
      "|    value_loss         | 4.68     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2544000, episode_reward=210.60 +/- 87.51\n",
      "Episode length: 1762.40 +/- 1411.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.76e+03 |\n",
      "|    mean_reward        | 211      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9937     |\n",
      "|    policy_loss        | -1.17    |\n",
      "|    value_loss         | 4.87     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2548000, episode_reward=196.60 +/- 107.78\n",
      "Episode length: 888.40 +/- 53.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 888      |\n",
      "|    mean_reward        | 197      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2548000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.859   |\n",
      "|    explained_variance | 0.727    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9953     |\n",
      "|    policy_loss        | -0.512   |\n",
      "|    value_loss         | 27.8     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2552000, episode_reward=153.20 +/- 30.10\n",
      "Episode length: 859.20 +/- 118.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 859      |\n",
      "|    mean_reward        | 153      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.81     |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9968     |\n",
      "|    policy_loss        | -0.0825  |\n",
      "|    value_loss         | 18.7     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2556000, episode_reward=155.60 +/- 124.84\n",
      "Episode length: 777.20 +/- 76.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 777      |\n",
      "|    mean_reward        | 156      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2556000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.991   |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9984     |\n",
      "|    policy_loss        | -0.258   |\n",
      "|    value_loss         | 3.5      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2560000, episode_reward=286.60 +/- 103.67\n",
      "Episode length: 932.40 +/- 86.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 932      |\n",
      "|    mean_reward        | 287      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.836   |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 9999     |\n",
      "|    policy_loss        | 1.45     |\n",
      "|    value_loss         | 21.7     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.02e+03 |\n",
      "|    ep_rew_mean     | 188      |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 10000    |\n",
      "|    time_elapsed    | 51621    |\n",
      "|    total_timesteps | 2560000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2564000, episode_reward=121.00 +/- 18.51\n",
      "Episode length: 951.60 +/- 105.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 952      |\n",
      "|    mean_reward        | 121      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2564000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.999   |\n",
      "|    explained_variance | 0.717    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 10015    |\n",
      "|    policy_loss        | -0.42    |\n",
      "|    value_loss         | 37.3     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2568000, episode_reward=20.00 +/- 4.98\n",
      "Episode length: 456.40 +/- 66.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 456      |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 0.839    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 10031    |\n",
      "|    policy_loss        | -0.133   |\n",
      "|    value_loss         | 1.01     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2572000, episode_reward=21.80 +/- 8.75\n",
      "Episode length: 512.80 +/- 52.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 513      |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2572000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | 0.768    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 10046    |\n",
      "|    policy_loss        | -0.344   |\n",
      "|    value_loss         | 0.451    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2576000, episode_reward=220.80 +/- 80.61\n",
      "Episode length: 958.00 +/- 84.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 958      |\n",
      "|    mean_reward        | 221      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.251   |\n",
      "|    explained_variance | 0.457    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 10062    |\n",
      "|    policy_loss        | 0.194    |\n",
      "|    value_loss         | 32       |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2580000, episode_reward=118.60 +/- 5.75\n",
      "Episode length: 836.40 +/- 105.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 836      |\n",
      "|    mean_reward        | 119      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 10078    |\n",
      "|    policy_loss        | 0.223    |\n",
      "|    value_loss         | 2.62     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2584000, episode_reward=144.60 +/- 75.67\n",
      "Episode length: 756.00 +/- 203.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 756      |\n",
      "|    mean_reward        | 145      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.876   |\n",
      "|    explained_variance | 0.778    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 10093    |\n",
      "|    policy_loss        | -0.747   |\n",
      "|    value_loss         | 9.04     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 890      |\n",
      "|    ep_rew_mean        | 143      |\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 10100    |\n",
      "|    time_elapsed       | 51985    |\n",
      "|    total_timesteps    | 2585600  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0005   |\n",
      "|    n_updates          | 10099    |\n",
      "|    policy_loss        | 0.351    |\n",
      "|    value_loss         | 0.368    |\n",
      "------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m     59\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(wandb\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport_path\u001b[39m\u001b[38;5;124m\"\u001b[39m], wandb\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\a2c\\a2c.py:201\u001b[0m, in \u001b[0;36mA2C.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfA2C,\n\u001b[0;32m    194\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfA2C:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:224\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[0;32m    223\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[1;32m--> 224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_info_buffer(infos, dones)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\callbacks.py:223\u001b[0m, in \u001b[0;36mCallbackList._on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    220\u001b[0m continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# Return False (stop training) if at least one callback returns False\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m continue_training\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m continue_training\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\callbacks.py:464\u001b[0m, in \u001b[0;36mEvalCallback._on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;66;03m# Reset success rate buffer\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_success_buffer \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 464\u001b[0m episode_rewards, episode_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_episode_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_success_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(episode_rewards, \u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\evaluation.py:88\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     86\u001b[0m episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((env\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m---> 88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[0;32m     95\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\policies.py:368\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 368\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[0;32m    370\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc, assignment]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\policies.py:717\u001b[0m, in \u001b[0;36mActorCriticPolicy._predict\u001b[1;34m(self, observation, deterministic)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    710\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;124;03m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;124;03m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\policies.py:750\u001b[0m, in \u001b[0;36mActorCriticPolicy.get_distribution\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_distribution\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: PyTorchObs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Distribution:\n\u001b[0;32m    744\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;124;03m    Get the current policy distribution given the observations.\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \n\u001b[0;32m    747\u001b[0m \u001b[38;5;124;03m    :param obs:\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;124;03m    :return: the action distribution.\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi_features_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m     latent_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_actor(features)\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\policies.py:131\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m:return: The extracted features\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m preprocessed_obs \u001b[38;5;241m=\u001b[39m preprocess_obs(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, normalize_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_images)\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeatures_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_obs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stable_baselines3\\common\\torch_layers.py:107\u001b[0m, in \u001b[0;36mNatureCNN.forward\u001b[1;34m(self, observations)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, observations: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"A2C Pacman\",\n",
    "    config={\n",
    "        \"env_id\": constants.env_id,\n",
    "        \"algorithm\": constants.algorithm,\n",
    "        \"learning_rate\": constants.learning_rate,\n",
    "        \"gamma\": constants.gamma,\n",
    "        \"n_steps\": constants.n_steps,\n",
    "        \"vf_coef\": constants.vf_coef,\n",
    "        \"ent_coef\": constants.ent_coef,   \n",
    "        \"max_grad_norm\": constants.max_grad_norm,\n",
    "        \"total_timesteps\": constants.total_timesteps, \n",
    "        \"model_name\": constants.model_name,\n",
    "        \"export_path\": constants.export_path,\n",
    "        \"videos_path\": constants.videos_path,\n",
    "    },\n",
    "    sync_tensorboard=True,\n",
    "    save_code=True,  \n",
    ")\n",
    "\n",
    "\n",
    "env_id = \"ALE/Pacman-v5\"  # Pac-Man environment ID\n",
    "env = DummyVecEnv([make_env(env_id) for i in range(8)])  # Create 8 parallel environments\n",
    "env = VecMonitor(env) # Log rollout metrics\n",
    "\n",
    "# Create and configure the A2C model\n",
    "model = A2C(\n",
    "    policy = constants.policy,  # Use a convolutional neural network . Pacman is represented in images (better to use CNN rather than MLP)\n",
    "    env = env,\n",
    "    learning_rate = constants.learning_rate,  \n",
    "    gamma = constants.gamma,  \n",
    "    n_steps = constants.n_steps,  \n",
    "    vf_coef = constants.vf_coef,  \n",
    "    ent_coef = constants.ent_coef,  \n",
    "    max_grad_norm = constants.max_grad_norm,  \n",
    "    verbose = 2,  # Enable verbose output                 \n",
    "    tensorboard_log = f\"runs/{run.id}\",\n",
    ")\n",
    "\n",
    "eval_env = DummyVecEnv([make_env(env_id) for i in range(1)])  \n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=500,\n",
    "                             deterministic=False, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=1000,  \n",
    "    save_path='./checkpoints/',  \n",
    "    name_prefix=\"a2c_pacman\", \n",
    ")\n",
    "\n",
    "callback_list = CallbackList([WandbCallback(verbose=2, gradient_save_freq=10), eval_callback, checkpoint_callback])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training...\")\n",
    "model.learn(total_timesteps=constants.total_timesteps, log_interval=100, callback=callback_list)\n",
    "\n",
    "# Save the trained model\n",
    "model_path = os.path.join(constants.export_path, constants.model_name)\n",
    "model.save(model_path)\n",
    "wandb.save(model_path + \".zip\")\n",
    "wandb.finish()\n",
    "\n",
    "# Close the training environment\n",
    "env.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
